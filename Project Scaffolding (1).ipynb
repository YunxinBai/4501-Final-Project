{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "import os\n",
    "\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import re\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TLC_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "WEATHER_CSV_DIR = \"\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b98651bc-39b5-4da2-971d-3f09256a3285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-01.parquet ...\n",
      "File saved to: data/yellow_taxi/2009-01.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-02.parquet ...\n",
      "File saved to: data/yellow_taxi/2009-02.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-03.parquet ...\n",
      "File saved to: data/yellow_taxi/2009-03.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-04.parquet ...\n",
      "File saved to: data/yellow_taxi/2009-04.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-05.parquet ...\n",
      "File saved to: data/yellow_taxi/2009-05.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-06.parquet ...\n",
      "File saved to: data/yellow_taxi/2009-06.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-07.parquet ...\n",
      "File saved to: data/yellow_taxi/2009-07.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-08.parquet ...\n",
      "File saved to: data/yellow_taxi/2009-08.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-09.parquet ...\n",
      "File saved to: data/yellow_taxi/2009-09.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-10.parquet ...\n",
      "File saved to: data/yellow_taxi/2009-10.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-11.parquet ...\n",
      "File saved to: data/yellow_taxi/2009-11.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-12.parquet ...\n",
      "File saved to: data/yellow_taxi/2009-12.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-01.parquet ...\n",
      "File saved to: data/yellow_taxi/2010-01.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-02.parquet ...\n",
      "File saved to: data/yellow_taxi/2010-02.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-03.parquet ...\n",
      "File saved to: data/yellow_taxi/2010-03.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-04.parquet ...\n",
      "File saved to: data/yellow_taxi/2010-04.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-05.parquet ...\n",
      "File saved to: data/yellow_taxi/2010-05.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-06.parquet ...\n",
      "File saved to: data/yellow_taxi/2010-06.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-07.parquet ...\n",
      "File saved to: data/yellow_taxi/2010-07.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-08.parquet ...\n",
      "File saved to: data/yellow_taxi/2010-08.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-09.parquet ...\n",
      "File saved to: data/yellow_taxi/2010-09.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-10.parquet ...\n",
      "File saved to: data/yellow_taxi/2010-10.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-11.parquet ...\n",
      "File saved to: data/yellow_taxi/2010-11.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-12.parquet ...\n",
      "File saved to: data/yellow_taxi/2010-12.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-01.parquet ...\n",
      "File saved to: data/yellow_taxi/2011-01.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-02.parquet ...\n",
      "File saved to: data/yellow_taxi/2011-02.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-03.parquet ...\n",
      "File saved to: data/yellow_taxi/2011-03.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-04.parquet ...\n",
      "File saved to: data/yellow_taxi/2011-04.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-05.parquet ...\n",
      "File saved to: data/yellow_taxi/2011-05.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-06.parquet ...\n",
      "File saved to: data/yellow_taxi/2011-06.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-07.parquet ...\n",
      "File saved to: data/yellow_taxi/2011-07.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-08.parquet ...\n",
      "File saved to: data/yellow_taxi/2011-08.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-09.parquet ...\n",
      "File saved to: data/yellow_taxi/2011-09.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-10.parquet ...\n",
      "File saved to: data/yellow_taxi/2011-10.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-11.parquet ...\n",
      "File saved to: data/yellow_taxi/2011-11.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-12.parquet ...\n",
      "File saved to: data/yellow_taxi/2011-12.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-01.parquet ...\n",
      "File saved to: data/yellow_taxi/2012-01.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-02.parquet ...\n",
      "File saved to: data/yellow_taxi/2012-02.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-03.parquet ...\n",
      "File saved to: data/yellow_taxi/2012-03.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-04.parquet ...\n",
      "File saved to: data/yellow_taxi/2012-04.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-05.parquet ...\n",
      "File saved to: data/yellow_taxi/2012-05.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-06.parquet ...\n",
      "File saved to: data/yellow_taxi/2012-06.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-07.parquet ...\n",
      "File saved to: data/yellow_taxi/2012-07.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-08.parquet ...\n",
      "File saved to: data/yellow_taxi/2012-08.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-09.parquet ...\n",
      "File saved to: data/yellow_taxi/2012-09.parquet\n",
      "Downloading Yellow Taxi file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-10.parquet ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(local_file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m):\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:\n\u001b[1;32m     39\u001b[0m             f\u001b[38;5;241m.\u001b[39mwrite(chunk)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp):\n\u001b[0;32m--> 628\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(amt\u001b[38;5;241m=\u001b[39mamt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m    631\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    564\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 567\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_read(amt) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m         flush_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1308\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1309\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1310\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate the date range for Yellow Taxi and HVFHV datasets\n",
    "start_date = \"2009-01\"\n",
    "end_date = \"2024-08\"\n",
    "\n",
    "dates = []\n",
    "current_date = datetime.strptime(start_date, \"%Y-%m\")\n",
    "end_date_obj = datetime.strptime(end_date, \"%Y-%m\")\n",
    "\n",
    "while current_date <= end_date_obj:\n",
    "    dates.append(current_date.strftime(\"%Y-%m\"))\n",
    "    current_date += timedelta(days=31)  # Move to the next month\n",
    "    current_date = current_date.replace(day=1)\n",
    "\n",
    "# Base URLs for Yellow Taxi and HVFHV datasets\n",
    "yellow_taxi_base_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_\"\n",
    "hvhf_base_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_\"\n",
    "\n",
    "# Save directories for Yellow Taxi and HVFHV data\n",
    "yellow_taxi_save_dir = \"data/yellow_taxi\"\n",
    "hvhf_save_dir = \"data/hvhf\"\n",
    "\n",
    "# Ensure the directories exist\n",
    "os.makedirs(yellow_taxi_save_dir, exist_ok=True)\n",
    "os.makedirs(hvhf_save_dir, exist_ok=True)\n",
    "\n",
    "# Download Yellow Taxi data\n",
    "for date in dates:\n",
    "    file_url = f\"{yellow_taxi_base_url}{date}.parquet\"\n",
    "    file_name = f\"{date}.parquet\"\n",
    "    local_file_path = os.path.join(yellow_taxi_save_dir, file_name)\n",
    "\n",
    "    print(f\"Downloading Yellow Taxi file: {file_url} ...\")\n",
    "    try:\n",
    "        response = requests.get(file_url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(local_file_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        print(f\"File saved to: {local_file_path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to download {file_url}: {e}\")\n",
    "\n",
    "# Download HVFHV data\n",
    "for date in dates:\n",
    "    file_url = f\"{hvhf_base_url}{date}.parquet\"\n",
    "    file_name = f\"{date}.parquet\"\n",
    "    local_file_path = os.path.join(hvhf_save_dir, file_name)\n",
    "\n",
    "    print(f\"Downloading HVFHV file: {file_url} ...\")\n",
    "    try:\n",
    "        response = requests.get(file_url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(local_file_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        print(f\"File saved to: {local_file_path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to download {file_url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dff9f30-f5e6-47ee-a435-4cf68fd4a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_uber_rides(hvhf_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters out non-Uber rides from the HVFHV dataset.\n",
    "\n",
    "    Args:\n",
    "        hvhf_data (pd.DataFrame): The raw HVFHV dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered dataset containing only Uber rides.\n",
    "    \"\"\"\n",
    "    uber_only_data = hvhf_data[hvhf_data['dispatching_base_num'].str.startswith(\"B\")].copy()  # Adjust as needed\n",
    "    print(f\"Filtered Uber rides: {len(uber_only_data)} out of {len(hvhf_data)} total rides.\")\n",
    "    return uber_only_data\n",
    "\n",
    "\n",
    "def cochran_sample_size(population_size: int, confidence_level: float = 0.95, margin_of_error: float = 0.05, p: float = 0.5) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the sample size using Cochran's formula.\n",
    "\n",
    "    Args:\n",
    "        population_size (int): The total number of data points in the population.\n",
    "        confidence_level (float): The confidence level (default is 0.95).\n",
    "        margin_of_error (float): The margin of error (default is 0.05).\n",
    "        p (float): The estimated proportion of the population (default is 0.5).\n",
    "\n",
    "    Returns:\n",
    "        int: The calculated sample size.\n",
    "    \"\"\"\n",
    "    z = {0.90: 1.645, 0.95: 1.96, 0.99: 2.576}.get(confidence_level, 1.96)\n",
    "    numerator = (z**2) * p * (1 - p)\n",
    "    denominator = margin_of_error**2\n",
    "    sample_size = numerator / denominator\n",
    "    if population_size > 0:\n",
    "        adjusted_sample_size = sample_size / (1 + (sample_size - 1) / population_size)\n",
    "    else:\n",
    "        adjusted_sample_size = sample_size\n",
    "    return int(np.ceil(adjusted_sample_size))\n",
    "\n",
    "\n",
    "def sample_dataset(data: pd.DataFrame, sample_size: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Randomly samples a dataset.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The dataset to sample from.\n",
    "        sample_size (int): The size of the sample to draw.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A randomly sampled dataset.\n",
    "    \"\"\"\n",
    "    if sample_size >= len(data):\n",
    "        print(\"Sample size is larger than or equal to the dataset size. Returning the entire dataset.\")\n",
    "        return data\n",
    "    return data.sample(n=sample_size, random_state=42)\n",
    "\n",
    "\n",
    "def process_dataset(file_path: str, output_dir: str, filter_uber: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Processes a dataset: loads, filters, samples, and saves the result.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the dataset file.\n",
    "        output_dir (str): Directory to save the processed file.\n",
    "        filter_uber (bool): Whether to filter for Uber rides (default is False).\n",
    "    \"\"\"\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    data = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Apply filtering for Uber rides if needed\n",
    "    if filter_uber:\n",
    "        data = filter_uber_rides(data)\n",
    "    \n",
    "    # Determine population size\n",
    "    population_size = len(data)\n",
    "    \n",
    "    # Calculate sample size\n",
    "    sample_size = cochran_sample_size(population_size, confidence_level=0.95, margin_of_error=0.05)\n",
    "    print(f\"Calculated sample size: {sample_size}\")\n",
    "    \n",
    "    # Sample the dataset\n",
    "    sampled_data = sample_dataset(data, sample_size)\n",
    "    \n",
    "    # Save the sampled dataset\n",
    "    output_file = os.path.join(output_dir, os.path.basename(file_path))\n",
    "    sampled_data.to_parquet(output_file)\n",
    "    print(f\"Processed file saved to: {output_file}\")\n",
    "\n",
    "\n",
    "# Example Workflow\n",
    "\n",
    "# Input directories (where raw datasets are stored)\n",
    "yellow_taxi_dir = \"data/yellow_taxi\"\n",
    "hvhf_dir = \"data/hvhf\"\n",
    "\n",
    "# Output directories (where processed datasets will be saved)\n",
    "processed_yellow_taxi_dir = \"processed_data/yellow_taxi\"\n",
    "processed_hvhf_dir = \"processed_data/hvhf\"\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(processed_yellow_taxi_dir, exist_ok=True)\n",
    "os.makedirs(processed_hvhf_dir, exist_ok=True)\n",
    "\n",
    "# Process Yellow Taxi datasets\n",
    "for file in os.listdir(yellow_taxi_dir):\n",
    "    if file.endswith(\".parquet\"):\n",
    "        process_dataset(\n",
    "            file_path=os.path.join(yellow_taxi_dir, file),\n",
    "            output_dir=processed_yellow_taxi_dir\n",
    "        )\n",
    "\n",
    "# Process HVFHV datasets (filter for Uber rides)\n",
    "for file in os.listdir(hvhf_dir):\n",
    "    if file.endswith(\".parquet\"):\n",
    "        process_dataset(\n",
    "            file_path=os.path.join(hvhf_dir, file),\n",
    "            output_dir=processed_hvhf_dir,\n",
    "            filter_uber=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "### Load Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58708809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones(shapefile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d04c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_coords_for_taxi_zone_id(zone_loc_id, loaded_taxi_zones):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sample_size(population):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc33eed4-b2e9-4ab3-94a8-59f04e464c98",
   "metadata": {},
   "source": [
    "### Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_tlc_page(taxi_page):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_parquet_urls(all_urls):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Process Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_month(url):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(parquet_urls):\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    for parquet_url in parquet_urls:\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_month(parquet_url)\n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        \n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.contact(all_taxi_dataframes)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200776ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data():\n",
    "    all_urls = get_all_urls_from_taxi_page(TLC_URL)\n",
    "    all_parquet_urls = find_taxi_parquet_urls(all_urls)\n",
    "    taxi_data = get_and_clean_taxi_data(all_parquet_urls)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876bd645",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data = get_taxi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebd75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da7089-3f6b-4f93-a22e-76bf554daca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c85e25-6416-4c16-b98c-09596cdc6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07574983-f41d-4cd6-8f70-489493089b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_uber_month(url):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3d85ff-313c-41a2-9a46-261a9a2bb10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_uber_data(parquet_urls):\n",
    "    all_uber_dataframes = []\n",
    "    \n",
    "    for parquet_url in parquet_urls:\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_uber_month(parquet_url)\n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        \n",
    "        all_uber_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    uber_data = pd.contact(all_uber_dataframes)\n",
    "    return uber_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data():\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    all_urls = get_all_urls_from_tlc_page(TLC_URL)\n",
    "    all_parquet_urls = find_parquet_urls(all_urls)\n",
    "    taxi_data = get_and_clean_uber_data(all_parquet_urls)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339997e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d783db-e527-4847-bf70-2d7428ea3897",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fddeb14-cd70-4e83-8f93-974642c3bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    weather_csv_files = get_all_weather_csvs(WEATHER_CSV_DIR)\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "        \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48216557",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935261b7-ae23-427c-97ff-ea31aa4e44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dcb502-d1d1-447d-aa68-11bff0dc53b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f090eb94-a5b0-4d93-bf82-a596d2521b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c074aa3-a5f2-4586-8748-411e1e6c11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query either via sqlalchemy\n",
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_1)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "pd.read_sql(QUERY_1, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
