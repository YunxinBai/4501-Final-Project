{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "854d1cd4-4133-4b31-a288-b51de769f9e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgpd\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdb\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "import os\n",
    "\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import fiona\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TLC_URL = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "WEATHER_CSV_DIR = \"\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "### Load Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58708809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones(file_path):\n",
    "    geofile = gpd.read_file(file_path)\n",
    "    return geofile\n",
    "    \n",
    "taxi_zones = load_taxi_zones(\"taxi_zones.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d04c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_coords_for_taxi_zone_id(zone_loc_id, loaded_taxi_zones):\n",
    "    if loaded_taxi_zones.crs is None:\n",
    "        loaded_taxi_zones = loaded_taxi_zones.set_crs(epsg=2263)\n",
    "\n",
    "    # Find the zone with the matching LocationID\n",
    "    zone = loaded_taxi_zones[loaded_taxi_zones['LocationID'] == zone_loc_id]\n",
    "\n",
    "    # If no match is found, return None\n",
    "    if zone.empty:\n",
    "        return None\n",
    "\n",
    "    # Temporarily reproject to a projected CRS for accurate centroid calculation\n",
    "    projected_zone = zone.to_crs(epsg=2263)\n",
    "    centroid = projected_zone.geometry.centroid.iloc[0]\n",
    "\n",
    "    # Transform the centroid back to geographic CRS (latitude/longitude)\n",
    "    centroid_geo = gpd.GeoSeries([centroid], crs=2263).to_crs(epsg=4326)\n",
    "\n",
    "    # Return the latitude and longitude as a tuple\n",
    "    return (centroid_geo.geometry.iloc[0].y, centroid_geo.geometry.iloc[0].x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sample_size(population, p = 0.5) -> int:\n",
    "    \"\"\"\n",
    "    Calculates the required sample size using Cochran's formula.\n",
    "\n",
    "    Args:\n",
    "        population (int): The total population size.\n",
    "        confidence_level (float): Confidence level as a proportion (default is 0.95 for 95% confidence).\n",
    "        margin_of_error (float): Desired margin of error as a proportion (default is 0.05 for 5%).\n",
    "\n",
    "    Returns:\n",
    "        int: Calculated sample size.\n",
    "    \"\"\"\n",
    "    # Z-value for confidence level (default: 1.96 for 95%)\n",
    "    z = 1.96\n",
    "    margin_of_error = 0.05\n",
    "    q = 1 - p  # Complementary proportion\n",
    "    \n",
    "    # Cochran's sample size formula for infinite population\n",
    "    n_0 = (z**2 * p * q) / (margin_of_error**2)\n",
    "    \n",
    "    # Adjust for finite population size\n",
    "    sample_size = n_0 / (1 + (n_0 - 1) / population)\n",
    "    \n",
    "    return math.ceil(sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc33eed4-b2e9-4ab3-94a8-59f04e464c98",
   "metadata": {},
   "source": [
    "### Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_page(page_url):\n",
    "    \"\"\"\n",
    "    Fetches all URLs from a given webpage.\n",
    "\n",
    "    Args:\n",
    "        page_url (str): URL of the webpage to scrape.\n",
    "\n",
    "    Returns:\n",
    "        list: List of all URLs found on the webpage.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Send a GET request to the page\n",
    "        response = requests.get(page_url)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise Exception(f\"Failed to access the URL: {page_url}. Error: {e}\")\n",
    "    \n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Find all anchor tags with href attributes\n",
    "    links = soup.find_all(\"a\", href=True)\n",
    "    \n",
    "    # Extract and return all href attributes\n",
    "    all_urls = [link[\"href\"] for link in links]\n",
    "    \n",
    "    return all_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_parquet_urls(links):\n",
    "    parquet_urls = []\n",
    "    for url in links:\n",
    "        # Normalize the URL (strip whitespace, handle cases like trailing slashes)\n",
    "        url = url.strip()\n",
    "        # Use regex to ensure matching even with query parameters\n",
    "        if re.search(r\"\\.parquet(\\?.*)?$\", url):\n",
    "            parquet_urls.append(url)\n",
    "    return parquet_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Process Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_month(parquet_url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downloads, processes, and saves Yellow Taxi dataset for a given month.\n",
    "\n",
    "    Args:\n",
    "        parquet_url (str): URL of the Yellow Taxi Parquet file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Sampled and processed DataFrame.\n",
    "    \"\"\"\n",
    "    # Default directory for processed Yellow Taxi data\n",
    "    save_dir = \"processed_data/yellow_taxi\"\n",
    "\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Extract file name and define local path\n",
    "    file_name = parquet_url.split(\"/\")[-1]\n",
    "    local_file_path = os.path.join(save_dir, file_name)\n",
    "\n",
    "    # Download the file if not already downloaded\n",
    "    if not os.path.exists(local_file_path):\n",
    "        response = requests.get(parquet_url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(local_file_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024 * 1024):  # 1MB chunks\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "    # Load the dataset\n",
    "    data = pd.read_parquet(local_file_path)\n",
    "\n",
    "    # Determine population size\n",
    "    population = len(data)\n",
    "\n",
    "    # Calculate sample size (using p = 0.5 for Yellow Taxi data)\n",
    "    sample_size = calculate_sample_size(population, p = 0.5)\n",
    "\n",
    "    # Sample the dataset\n",
    "    sampled_data = data.sample(n=sample_size, random_state=42) if population > sample_size else data\n",
    "\n",
    "    # Save the sampled dataset\n",
    "    processed_file_path = os.path.join(save_dir, f\"sampled_{file_name}\")\n",
    "    sampled_data.to_parquet(processed_file_path)\n",
    "    return sampled_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(parquet_urls):\n",
    "    all_taxi_dataframes = []\n",
    "    yellow_taxi_pattern = re.compile(r\"yellow_tripdata_(2020-(0[1-9]|1[0-2])|202[1-3]-(0[1-9]|1[0-2])|2024-(0[1-8]))\\.parquet\")\n",
    "\n",
    "    # Filter URLs matching the pattern\n",
    "    yellow_taxi_urls = [url for url in parquet_urls if yellow_taxi_pattern.search(url)]\n",
    "    \n",
    "    for url in yellow_taxi_urls:\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_taxi_month(url)\n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        \n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    \n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "830ef27d-90a7-4ed8-aaa2-da3a7e5c8403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_taxi_data(taxi_data):\n",
    "    \"\"\"\n",
    "    Cleans the taxi data by retaining specified columns, normalizing column names,\n",
    "    converting column types, and removing invalid trips.\n",
    "\n",
    "    Args:\n",
    "        taxi_data (pd.DataFrame): The input taxi data DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned and filtered taxi data.\n",
    "    \"\"\"\n",
    "    # Add latitude and longitude for PULocationID and DOLocationID\n",
    "    taxi_data[\"PU_coords\"] = taxi_data[\"PULocationID\"].apply(lambda x: lookup_coords_for_taxi_zone_id(x, taxi_zones))\n",
    "    taxi_data[\"DO_coords\"] = taxi_data[\"DOLocationID\"].apply(lambda x: lookup_coords_for_taxi_zone_id(x, taxi_zones))\n",
    "\n",
    "    # Remove trips with invalid location IDs (where coordinates could not be found)\n",
    "    taxi_data = taxi_data.dropna(subset=[\"PU_coords\", \"DO_coords\"]).reset_index(drop=True)\n",
    "\n",
    "    # Split coordinates into latitude and longitude for pickups and dropoffs\n",
    "    taxi_data[[\"PU_lat\", \"PU_lon\"]] = pd.DataFrame(taxi_data[\"PU_coords\"].tolist(), index=taxi_data.index)\n",
    "    taxi_data[[\"DO_lat\", \"DO_lon\"]] = pd.DataFrame(taxi_data[\"DO_coords\"].tolist(), index=taxi_data.index)\n",
    "\n",
    "    # Drop temporary coordinate columns\n",
    "    taxi_data = taxi_data.drop(columns=[\"PU_coords\", \"DO_coords\"])\n",
    "    \n",
    "    # Step 1: Retain only the required columns\n",
    "    required_columns = [\n",
    "        'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_distance',\n",
    "        'PU_lat', 'PU_lon', 'DO_lat', 'DO_lon',\n",
    "        'fare_amount', 'extra', 'mta_tax', 'tip_amount',\n",
    "        'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge'\n",
    "    ]\n",
    "    taxi_data = taxi_data[required_columns]\n",
    "\n",
    "    # Removing Invalid Data Points\n",
    "    # Remove rows where trip distance is less than or equal to 0\n",
    "    taxi_data = taxi_data[taxi_data[\"trip_distance\"] > 0]\n",
    "\n",
    "    # Remove rows where fare amount or total amount is less than or equal to 0\n",
    "    taxi_data = taxi_data[taxi_data[\"fare_amount\"] > 0]\n",
    "    taxi_data = taxi_data[taxi_data[\"total_amount\"] > 0]\n",
    "\n",
    "    # Remove rows where pickup time is after dropoff time\n",
    "    taxi_data = taxi_data[taxi_data[\"tpep_pickup_datetime\"] < taxi_data[\"tpep_dropoff_datetime\"]]\n",
    "\n",
    "    # Reset index after filtering\n",
    "    taxi_data = taxi_data.reset_index(drop=True)\n",
    "\n",
    "    #Normalize column names\n",
    "    taxi_data.columns = [col.lower() for col in taxi_data.columns]\n",
    "\n",
    "    # Normalizing and Using Appropriate Column Types\n",
    "    taxi_data[\"tpep_pickup_datetime\"] = pd.to_datetime(taxi_data[\"tpep_pickup_datetime\"], errors=\"coerce\")\n",
    "    taxi_data[\"tpep_dropoff_datetime\"] = pd.to_datetime(taxi_data[\"tpep_dropoff_datetime\"], errors=\"coerce\")\n",
    "\n",
    "    # Ensure numeric columns are float or int\n",
    "    numeric_columns = [\n",
    "        \"trip_distance\", \"pu_lat\", \"pu_lon\", \"do_lat\", \"do_lon\",\n",
    "        \"fare_amount\", \"extra\", \"mta_tax\", \"tip_amount\",\n",
    "        \"tolls_amount\", \"improvement_surcharge\", \"total_amount\", \"congestion_surcharge\"\n",
    "    ]\n",
    "    taxi_data[numeric_columns] = taxi_data[numeric_columns].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    # Drop rows with invalid datetime or numeric values\n",
    "    taxi_data = taxi_data.dropna(subset=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"] + numeric_columns)\n",
    "\n",
    "    # Reset index after dropping invalid rows\n",
    "    taxi_data = taxi_data.reset_index(drop=True)\n",
    "\n",
    "    # Removing Trips Outside the Latitude/Longitude Bounding Box\n",
    "    # Latitude and longitude bounding box\n",
    "    lat_min, lon_min = 40.560445, -74.242330\n",
    "    lat_max, lon_max = 40.908524, -73.717047\n",
    "\n",
    "    # Filter rows where pickup and dropoff locations are within the bounding box\n",
    "    taxi_data = taxi_data[\n",
    "        (taxi_data[\"pu_lat\"] >= lat_min) & (taxi_data[\"pu_lat\"] <= lat_max) &\n",
    "        (taxi_data[\"pu_lon\"] >= lon_min) & (taxi_data[\"pu_lon\"] <= lon_max) &\n",
    "        (taxi_data[\"do_lat\"] >= lat_min) & (taxi_data[\"do_lat\"] <= lat_max) &\n",
    "        (taxi_data[\"do_lon\"] >= lon_min) & (taxi_data[\"do_lon\"] <= lon_max)\n",
    "    ]\n",
    "\n",
    "    # Reset index after filtering\n",
    "    taxi_data = taxi_data.reset_index(drop=True)\n",
    "\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "200776ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data():\n",
    "    all_urls = get_all_urls_from_page(TLC_URL)\n",
    "    all_parquet_urls = filter_parquet_urls(all_urls)\n",
    "    taxi_data = get_and_clean_taxi_data(all_parquet_urls)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "876bd645",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data = get_taxi_data()\n",
    "taxi_data = clean_taxi_data(taxi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "10ebd75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pu_lat</th>\n",
       "      <th>pu_lon</th>\n",
       "      <th>do_lat</th>\n",
       "      <th>do_lon</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-20 13:31:30</td>\n",
       "      <td>2024-01-20 14:03:25</td>\n",
       "      <td>17.14</td>\n",
       "      <td>40.646985</td>\n",
       "      <td>-73.786530</td>\n",
       "      <td>40.749914</td>\n",
       "      <td>-73.970443</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8.27</td>\n",
       "      <td>6.94</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90.96</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-18 21:52:46</td>\n",
       "      <td>2024-01-18 22:03:21</td>\n",
       "      <td>2.49</td>\n",
       "      <td>40.764421</td>\n",
       "      <td>-73.977569</td>\n",
       "      <td>40.790011</td>\n",
       "      <td>-73.945750</td>\n",
       "      <td>13.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.50</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01 03:43:58</td>\n",
       "      <td>2024-01-01 03:50:47</td>\n",
       "      <td>1.84</td>\n",
       "      <td>40.866075</td>\n",
       "      <td>-73.919308</td>\n",
       "      <td>40.857779</td>\n",
       "      <td>-73.885867</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.50</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-19 22:20:12</td>\n",
       "      <td>2024-01-19 22:50:12</td>\n",
       "      <td>3.60</td>\n",
       "      <td>40.748497</td>\n",
       "      <td>-73.992438</td>\n",
       "      <td>40.778766</td>\n",
       "      <td>-73.951010</td>\n",
       "      <td>23.3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.95</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-06 22:41:50</td>\n",
       "      <td>2024-01-06 22:43:24</td>\n",
       "      <td>0.04</td>\n",
       "      <td>40.791705</td>\n",
       "      <td>-73.973049</td>\n",
       "      <td>40.791705</td>\n",
       "      <td>-73.973049</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tpep_pickup_datetime tpep_dropoff_datetime  trip_distance     pu_lat  \\\n",
       "0  2024-01-20 13:31:30   2024-01-20 14:03:25          17.14  40.646985   \n",
       "1  2024-01-18 21:52:46   2024-01-18 22:03:21           2.49  40.764421   \n",
       "2  2024-01-01 03:43:58   2024-01-01 03:50:47           1.84  40.866075   \n",
       "3  2024-01-19 22:20:12   2024-01-19 22:50:12           3.60  40.748497   \n",
       "4  2024-01-06 22:41:50   2024-01-06 22:43:24           0.04  40.791705   \n",
       "\n",
       "      pu_lon     do_lat     do_lon  fare_amount  extra  mta_tax  tip_amount  \\\n",
       "0 -73.786530  40.749914 -73.970443         70.0    0.0      0.5        8.27   \n",
       "1 -73.977569  40.790011 -73.945750         13.5    1.0      0.5        4.00   \n",
       "2 -73.919308  40.857779 -73.885867         10.0    1.0      0.5        0.00   \n",
       "3 -73.992438  40.778766 -73.951010         23.3    3.5      0.5        5.65   \n",
       "4 -73.973049  40.791705 -73.973049          3.7    1.0      0.5        0.00   \n",
       "\n",
       "   tolls_amount  improvement_surcharge  total_amount  congestion_surcharge  \n",
       "0          6.94                    1.0         90.96                   2.5  \n",
       "1          0.00                    1.0         22.50                   2.5  \n",
       "2          0.00                    1.0         12.50                   0.0  \n",
       "3          0.00                    1.0         33.95                   2.5  \n",
       "4          0.00                    1.0          6.20                   0.0  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c9da7089-3f6b-4f93-a22e-76bf554daca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19669 entries, 0 to 19668\n",
      "Data columns (total 15 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   tpep_pickup_datetime   19669 non-null  datetime64[ns]\n",
      " 1   tpep_dropoff_datetime  19669 non-null  datetime64[ns]\n",
      " 2   trip_distance          19669 non-null  float64       \n",
      " 3   pu_lat                 19669 non-null  float64       \n",
      " 4   pu_lon                 19669 non-null  float64       \n",
      " 5   do_lat                 19669 non-null  float64       \n",
      " 6   do_lon                 19669 non-null  float64       \n",
      " 7   fare_amount            19669 non-null  float64       \n",
      " 8   extra                  19669 non-null  float64       \n",
      " 9   mta_tax                19669 non-null  float64       \n",
      " 10  tip_amount             19669 non-null  float64       \n",
      " 11  tolls_amount           19669 non-null  float64       \n",
      " 12  improvement_surcharge  19669 non-null  float64       \n",
      " 13  total_amount           19669 non-null  float64       \n",
      " 14  congestion_surcharge   19669 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(13)\n",
      "memory usage: 2.3 MB\n"
     ]
    }
   ],
   "source": [
    "taxi_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "50c85e25-6416-4c16-b98c-09596cdc6865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pu_lat</th>\n",
       "      <th>pu_lon</th>\n",
       "      <th>do_lat</th>\n",
       "      <th>do_lon</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19669</td>\n",
       "      <td>19669</td>\n",
       "      <td>19669.000000</td>\n",
       "      <td>19669.000000</td>\n",
       "      <td>19669.000000</td>\n",
       "      <td>19669.000000</td>\n",
       "      <td>19669.000000</td>\n",
       "      <td>19669.000000</td>\n",
       "      <td>19669.000000</td>\n",
       "      <td>19669.000000</td>\n",
       "      <td>19669.000000</td>\n",
       "      <td>19669.000000</td>\n",
       "      <td>19669.000000</td>\n",
       "      <td>19669.000000</td>\n",
       "      <td>19669.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-04-29 05:40:59.003304704</td>\n",
       "      <td>2022-04-29 05:56:50.285322240</td>\n",
       "      <td>3.166626</td>\n",
       "      <td>40.753658</td>\n",
       "      <td>-73.967372</td>\n",
       "      <td>40.756144</td>\n",
       "      <td>-73.971694</td>\n",
       "      <td>14.955250</td>\n",
       "      <td>1.270525</td>\n",
       "      <td>0.497916</td>\n",
       "      <td>2.735081</td>\n",
       "      <td>0.416819</td>\n",
       "      <td>0.550704</td>\n",
       "      <td>22.142688</td>\n",
       "      <td>2.329935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-01-01 00:11:06</td>\n",
       "      <td>2020-01-01 00:30:50</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>40.576961</td>\n",
       "      <td>-74.029893</td>\n",
       "      <td>40.576961</td>\n",
       "      <td>-74.174002</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2021-03-08 15:59:21</td>\n",
       "      <td>2021-03-08 16:22:45</td>\n",
       "      <td>1.060000</td>\n",
       "      <td>40.740439</td>\n",
       "      <td>-73.989845</td>\n",
       "      <td>40.740337</td>\n",
       "      <td>-73.989845</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-04-30 09:25:30</td>\n",
       "      <td>2022-04-30 12:04:02</td>\n",
       "      <td>1.770000</td>\n",
       "      <td>40.758028</td>\n",
       "      <td>-73.977698</td>\n",
       "      <td>40.758028</td>\n",
       "      <td>-73.977698</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.160000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>16.630000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-06-18 22:51:55</td>\n",
       "      <td>2023-06-18 23:00:48</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>40.773633</td>\n",
       "      <td>-73.965146</td>\n",
       "      <td>40.775932</td>\n",
       "      <td>-73.959635</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.800000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-08-31 22:43:47</td>\n",
       "      <td>2024-08-31 23:26:23</td>\n",
       "      <td>67.900000</td>\n",
       "      <td>40.899528</td>\n",
       "      <td>-73.739337</td>\n",
       "      <td>40.899528</td>\n",
       "      <td>-73.726655</td>\n",
       "      <td>209.500000</td>\n",
       "      <td>11.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>262.700000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.037265</td>\n",
       "      <td>0.030998</td>\n",
       "      <td>0.044858</td>\n",
       "      <td>0.031462</td>\n",
       "      <td>0.034564</td>\n",
       "      <td>13.469103</td>\n",
       "      <td>1.519187</td>\n",
       "      <td>0.032217</td>\n",
       "      <td>3.180876</td>\n",
       "      <td>1.756269</td>\n",
       "      <td>0.336082</td>\n",
       "      <td>17.338749</td>\n",
       "      <td>0.629492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tpep_pickup_datetime          tpep_dropoff_datetime  \\\n",
       "count                          19669                          19669   \n",
       "mean   2022-04-29 05:40:59.003304704  2022-04-29 05:56:50.285322240   \n",
       "min              2020-01-01 00:11:06            2020-01-01 00:30:50   \n",
       "25%              2021-03-08 15:59:21            2021-03-08 16:22:45   \n",
       "50%              2022-04-30 09:25:30            2022-04-30 12:04:02   \n",
       "75%              2023-06-18 22:51:55            2023-06-18 23:00:48   \n",
       "max              2024-08-31 22:43:47            2024-08-31 23:26:23   \n",
       "std                              NaN                            NaN   \n",
       "\n",
       "       trip_distance        pu_lat        pu_lon        do_lat        do_lon  \\\n",
       "count   19669.000000  19669.000000  19669.000000  19669.000000  19669.000000   \n",
       "mean        3.166626     40.753658    -73.967372     40.756144    -73.971694   \n",
       "min         0.010000     40.576961    -74.029893     40.576961    -74.174002   \n",
       "25%         1.060000     40.740439    -73.989845     40.740337    -73.989845   \n",
       "50%         1.770000     40.758028    -73.977698     40.758028    -73.977698   \n",
       "75%         3.200000     40.773633    -73.965146     40.775932    -73.959635   \n",
       "max        67.900000     40.899528    -73.739337     40.899528    -73.726655   \n",
       "std         4.037265      0.030998      0.044858      0.031462      0.034564   \n",
       "\n",
       "        fare_amount         extra       mta_tax    tip_amount  tolls_amount  \\\n",
       "count  19669.000000  19669.000000  19669.000000  19669.000000  19669.000000   \n",
       "mean      14.955250      1.270525      0.497916      2.735081      0.416819   \n",
       "min        1.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        7.200000      0.000000      0.500000      0.000000      0.000000   \n",
       "50%       10.500000      0.500000      0.500000      2.160000      0.000000   \n",
       "75%       16.500000      2.500000      0.500000      3.460000      0.000000   \n",
       "max      209.500000     11.750000      0.500000     50.000000     40.000000   \n",
       "std       13.469103      1.519187      0.032217      3.180876      1.756269   \n",
       "\n",
       "       improvement_surcharge  total_amount  congestion_surcharge  \n",
       "count           19669.000000  19669.000000          19669.000000  \n",
       "mean                0.550704     22.142688              2.329935  \n",
       "min                 0.000000      1.300000              0.000000  \n",
       "25%                 0.300000     12.600000              2.500000  \n",
       "50%                 0.300000     16.630000              2.500000  \n",
       "75%                 1.000000     23.800000              2.500000  \n",
       "max                 1.000000    262.700000              2.500000  \n",
       "std                 0.336082     17.338749              0.629492  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07574983-f41d-4cd6-8f70-489493089b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_uber_month(parquet_url):\n",
    "    save_dir = \"processed_data/hvhf\"\n",
    "\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Extract file name and define local path\n",
    "    file_name = parquet_url.split(\"/\")[-1]\n",
    "    local_file_path = os.path.join(save_dir, file_name)\n",
    "\n",
    "    # Download the file if not already downloaded\n",
    "    if not os.path.exists(local_file_path):\n",
    "        response = requests.get(parquet_url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(local_file_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024 * 1024):  # 1MB chunks\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "    data = pd.read_parquet(local_file_path)\n",
    "\n",
    "    # Determine population size\n",
    "    population = len(data)\n",
    "\n",
    "    # Calculate sample size (using p = 0.5 for Yellow Taxi data)\n",
    "    sample_size = calculate_sample_size(population, p = 0.4)\n",
    "\n",
    "    # Sample the dataset\n",
    "    sampled_data = data.sample(n=sample_size, random_state=42) if population > sample_size else data\n",
    "\n",
    "    # Save the sampled dataset\n",
    "    processed_file_path = os.path.join(save_dir, f\"sampled_{file_name}\")\n",
    "    if not os.path.exists(processed_file_path):\n",
    "        sampled_data.to_parquet(processed_file_path)\n",
    "    return sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b3d85ff-313c-41a2-9a46-261a9a2bb10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_uber_data(parquet_urls):\n",
    "    all_uber_dataframes = []\n",
    "    hvfhv_pattern = re.compile(r\"fhvhv_tripdata_(2020-(0[1-9]|1[0-2])|202[1-3]-(0[1-9]|1[0-2])|2024-(0[1-8]))\\.parquet\")\n",
    "    hvfhv_urls = [url for url in parquet_urls if hvfhv_pattern.search(url)]\n",
    "    for url in hvfhv_urls:\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_uber_month(url)\n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        \n",
    "        all_uber_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    uber_data = pd.concat(all_uber_dataframes)\n",
    "    return uber_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(uber_data):\n",
    "    uber_data['hvfhs_license_num'] = uber_data['hvfhs_license_num'].astype(str)\n",
    "    uber_data = uber_data[uber_data['hvfhs_license_num'] == 'HV0003'].copy()\n",
    "\n",
    "    # Coords matching\n",
    "    uber_data[\"PU_coords\"] = uber_data[\"PULocationID\"].apply(lambda x: lookup_coords_for_taxi_zone_id(x, taxi_zones))\n",
    "    uber_data[\"DO_coords\"] = uber_data[\"DOLocationID\"].apply(lambda x: lookup_coords_for_taxi_zone_id(x, taxi_zones))\n",
    "\n",
    "    # Remove trips with invalid location IDs (where coordinates could not be found)\n",
    "    uber_data = uber_data.dropna(subset=[\"PU_coords\", \"DO_coords\"]).reset_index(drop=True)\n",
    "\n",
    "    # Split coordinates into latitude and longitude for pickups and dropoffs\n",
    "    uber_data[[\"PU_lat\", \"PU_lon\"]] = pd.DataFrame(uber_data[\"PU_coords\"].tolist(), index=uber_data.index)\n",
    "    uber_data[[\"DO_lat\", \"DO_lon\"]] = pd.DataFrame(uber_data[\"DO_coords\"].tolist(), index=uber_data.index)\n",
    "\n",
    "    # Drop temporary coordinate columns\n",
    "    uber_data = uber_data.drop(columns=[\"PU_coords\", \"DO_coords\"])\n",
    "\n",
    "    # Step 1: Retain only the required columns\n",
    "    required_columns = [\n",
    "        'pickup_datetime', 'dropoff_datetime',\n",
    "        'PU_lat', 'PU_lon', 'DO_lat', 'DO_lon',\n",
    "        'trip_miles', 'base_passenger_fare', 'tolls', 'bcf',\n",
    "        'sales_tax', 'congestion_surcharge', 'tips', 'driver_pay'\n",
    "    ]\n",
    "    uber_data = uber_data[required_columns]\n",
    "\n",
    "    # Removing Invalid Data Points\n",
    "    # Remove rows where trip distance is less than or equal to 0\n",
    "    uber_data = uber_data[uber_data[\"trip_miles\"] > 0]\n",
    "\n",
    "    # Remove rows where fare amount or total amount is less than or equal to 0\n",
    "    uber_data = uber_data[uber_data[\"base_passenger_fare\"] > 0]\n",
    "    uber_data = uber_data[uber_data[\"driver_pay\"] > 0]\n",
    "\n",
    "    # Remove rows where pickup time is after dropoff time\n",
    "    uber_data = uber_data[uber_data[\"pickup_datetime\"] < uber_data[\"dropoff_datetime\"]]\n",
    "\n",
    "    # Normalize column names\n",
    "    uber_data.columns = [col.lower() for col in uber_data.columns]\n",
    "\n",
    "    # Ensure numeric columns are float or int\n",
    "    numeric_columns = [\n",
    "        'pu_lat', 'pu_lon', 'do_lat', 'do_lon',\n",
    "        'trip_miles', 'base_passenger_fare', 'tolls', 'bcf',\n",
    "        'sales_tax', 'congestion_surcharge', 'tips', 'driver_pay'\n",
    "    ]\n",
    "    uber_data[numeric_columns] = uber_data[numeric_columns].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    # Drop rows with invalid datetime or numeric values\n",
    "    uber_data = uber_data.dropna(subset=[\"pickup_datetime\", \"dropoff_datetime\"] + numeric_columns)\n",
    "\n",
    "    # Removing Trips Outside the Latitude/Longitude Bounding Box\n",
    "    # Latitude and longitude bounding box\n",
    "    lat_min, lon_min = 40.560445, -74.242330\n",
    "    lat_max, lon_max = 40.908524, -73.717047\n",
    "\n",
    "    # Filter rows where pickup and dropoff locations are within the bounding box\n",
    "    uber_data = uber_data[\n",
    "        (uber_data[\"pu_lat\"] >= lat_min) & (uber_data[\"pu_lat\"] <= lat_max) &\n",
    "        (uber_data[\"pu_lon\"] >= lon_min) & (uber_data[\"pu_lon\"] <= lon_max) &\n",
    "        (uber_data[\"do_lat\"] >= lat_min) & (uber_data[\"do_lat\"] <= lat_max) &\n",
    "        (uber_data[\"do_lon\"] >= lon_min) & (uber_data[\"do_lon\"] <= lon_max)\n",
    "    ]\n",
    "\n",
    "    # Reset index after filtering\n",
    "    uber_data = uber_data.reset_index(drop=True)\n",
    "    \n",
    "    return uber_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    all_urls = get_all_urls_from_page(TLC_URL)\n",
    "    all_parquet_urls = filter_parquet_urls(all_urls)\n",
    "    uber_data = get_and_clean_uber_data(all_parquet_urls)\n",
    "    uber_data = load_and_clean_uber_data(uber_data)\n",
    "    return uber_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9c2bd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "339997e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>pu_lat</th>\n",
       "      <th>pu_lon</th>\n",
       "      <th>do_lat</th>\n",
       "      <th>do_lon</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>base_passenger_fare</th>\n",
       "      <th>tolls</th>\n",
       "      <th>bcf</th>\n",
       "      <th>sales_tax</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>tips</th>\n",
       "      <th>driver_pay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-26 08:07:17</td>\n",
       "      <td>2024-01-26 08:35:38</td>\n",
       "      <td>40.646116</td>\n",
       "      <td>-73.951623</td>\n",
       "      <td>40.666559</td>\n",
       "      <td>-73.895364</td>\n",
       "      <td>4.29</td>\n",
       "      <td>27.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-19 02:17:05</td>\n",
       "      <td>2024-01-19 02:29:12</td>\n",
       "      <td>40.882403</td>\n",
       "      <td>-73.910665</td>\n",
       "      <td>40.857108</td>\n",
       "      <td>-73.932832</td>\n",
       "      <td>2.55</td>\n",
       "      <td>15.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-21 01:44:00</td>\n",
       "      <td>2024-01-21 02:08:30</td>\n",
       "      <td>40.748575</td>\n",
       "      <td>-73.985156</td>\n",
       "      <td>40.715370</td>\n",
       "      <td>-73.936794</td>\n",
       "      <td>6.37</td>\n",
       "      <td>24.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.68</td>\n",
       "      <td>2.18</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.01</td>\n",
       "      <td>23.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-20 12:58:40</td>\n",
       "      <td>2024-01-20 13:15:42</td>\n",
       "      <td>40.758028</td>\n",
       "      <td>-73.977698</td>\n",
       "      <td>40.753309</td>\n",
       "      <td>-74.004016</td>\n",
       "      <td>1.99</td>\n",
       "      <td>18.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.68</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-02 08:40:48</td>\n",
       "      <td>2024-01-02 08:54:28</td>\n",
       "      <td>40.666559</td>\n",
       "      <td>-73.895364</td>\n",
       "      <td>40.676644</td>\n",
       "      <td>-73.913632</td>\n",
       "      <td>2.23</td>\n",
       "      <td>16.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14309</th>\n",
       "      <td>2020-12-31 14:17:53</td>\n",
       "      <td>2020-12-31 14:30:13</td>\n",
       "      <td>40.849058</td>\n",
       "      <td>-73.905122</td>\n",
       "      <td>40.865264</td>\n",
       "      <td>-73.905911</td>\n",
       "      <td>1.83</td>\n",
       "      <td>12.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14310</th>\n",
       "      <td>2020-12-22 20:22:12</td>\n",
       "      <td>2020-12-22 20:37:34</td>\n",
       "      <td>40.849172</td>\n",
       "      <td>-73.831582</td>\n",
       "      <td>40.828987</td>\n",
       "      <td>-73.924410</td>\n",
       "      <td>9.01</td>\n",
       "      <td>25.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>2.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14311</th>\n",
       "      <td>2020-12-18 17:51:59</td>\n",
       "      <td>2020-12-18 18:00:14</td>\n",
       "      <td>40.857108</td>\n",
       "      <td>-73.932832</td>\n",
       "      <td>40.841708</td>\n",
       "      <td>-73.941399</td>\n",
       "      <td>1.29</td>\n",
       "      <td>6.54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14312</th>\n",
       "      <td>2020-12-27 22:52:25</td>\n",
       "      <td>2020-12-27 23:07:06</td>\n",
       "      <td>40.882157</td>\n",
       "      <td>-73.858949</td>\n",
       "      <td>40.827902</td>\n",
       "      <td>-73.869680</td>\n",
       "      <td>5.06</td>\n",
       "      <td>15.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14313</th>\n",
       "      <td>2020-12-14 15:07:14</td>\n",
       "      <td>2020-12-14 15:23:08</td>\n",
       "      <td>40.854405</td>\n",
       "      <td>-73.854394</td>\n",
       "      <td>40.877137</td>\n",
       "      <td>-73.879022</td>\n",
       "      <td>3.11</td>\n",
       "      <td>14.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14314 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          pickup_datetime    dropoff_datetime     pu_lat     pu_lon  \\\n",
       "0     2024-01-26 08:07:17 2024-01-26 08:35:38  40.646116 -73.951623   \n",
       "1     2024-01-19 02:17:05 2024-01-19 02:29:12  40.882403 -73.910665   \n",
       "2     2024-01-21 01:44:00 2024-01-21 02:08:30  40.748575 -73.985156   \n",
       "3     2024-01-20 12:58:40 2024-01-20 13:15:42  40.758028 -73.977698   \n",
       "4     2024-01-02 08:40:48 2024-01-02 08:54:28  40.666559 -73.895364   \n",
       "...                   ...                 ...        ...        ...   \n",
       "14309 2020-12-31 14:17:53 2020-12-31 14:30:13  40.849058 -73.905122   \n",
       "14310 2020-12-22 20:22:12 2020-12-22 20:37:34  40.849172 -73.831582   \n",
       "14311 2020-12-18 17:51:59 2020-12-18 18:00:14  40.857108 -73.932832   \n",
       "14312 2020-12-27 22:52:25 2020-12-27 23:07:06  40.882157 -73.858949   \n",
       "14313 2020-12-14 15:07:14 2020-12-14 15:23:08  40.854405 -73.854394   \n",
       "\n",
       "          do_lat     do_lon  trip_miles  base_passenger_fare  tolls   bcf  \\\n",
       "0      40.666559 -73.895364        4.29                27.49    0.0  0.76   \n",
       "1      40.857108 -73.932832        2.55                15.14    0.0  0.42   \n",
       "2      40.715370 -73.936794        6.37                24.57    0.0  0.68   \n",
       "3      40.753309 -74.004016        1.99                18.96    0.0  0.52   \n",
       "4      40.676644 -73.913632        2.23                16.08    0.0  0.44   \n",
       "...          ...        ...         ...                  ...    ...   ...   \n",
       "14309  40.865264 -73.905911        1.83                12.00    0.0  0.36   \n",
       "14310  40.828987 -73.924410        9.01                25.48    0.0  0.76   \n",
       "14311  40.841708 -73.941399        1.29                 6.54    0.0  0.20   \n",
       "14312  40.827902 -73.869680        5.06                15.35    0.0  0.46   \n",
       "14313  40.877137 -73.879022        3.11                14.71    0.0  0.44   \n",
       "\n",
       "       sales_tax  congestion_surcharge  tips  driver_pay  \n",
       "0           2.44                  0.00  0.00       24.88  \n",
       "1           1.34                  0.00  0.00       10.19  \n",
       "2           2.18                  2.75  3.01       23.00  \n",
       "3           1.68                  2.75  0.00       12.47  \n",
       "4           1.43                  0.00  0.00       10.64  \n",
       "...          ...                   ...   ...         ...  \n",
       "14309       1.06                  0.00  0.00        8.21  \n",
       "14310       2.26                  0.00  0.00       17.66  \n",
       "14311       0.58                  0.00  0.00        5.57  \n",
       "14312       1.36                  0.00  0.00       12.96  \n",
       "14313       1.31                  0.00  0.00       11.41  \n",
       "\n",
       "[14314 rows x 14 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "74d783db-e527-4847-bf70-2d7428ea3897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14314 entries, 0 to 14313\n",
      "Data columns (total 14 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   pickup_datetime       14314 non-null  datetime64[ns]\n",
      " 1   dropoff_datetime      14314 non-null  datetime64[ns]\n",
      " 2   pu_lat                14314 non-null  float64       \n",
      " 3   pu_lon                14314 non-null  float64       \n",
      " 4   do_lat                14314 non-null  float64       \n",
      " 5   do_lon                14314 non-null  float64       \n",
      " 6   trip_miles            14314 non-null  float64       \n",
      " 7   base_passenger_fare   14314 non-null  float64       \n",
      " 8   tolls                 14314 non-null  float64       \n",
      " 9   bcf                   14314 non-null  float64       \n",
      " 10  sales_tax             14314 non-null  float64       \n",
      " 11  congestion_surcharge  14314 non-null  float64       \n",
      " 12  tips                  14314 non-null  float64       \n",
      " 13  driver_pay            14314 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(12)\n",
      "memory usage: 1.5 MB\n"
     ]
    }
   ],
   "source": [
    "uber_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6fddeb14-cd70-4e83-8f93-974642c3bea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>pu_lat</th>\n",
       "      <th>pu_lon</th>\n",
       "      <th>do_lat</th>\n",
       "      <th>do_lon</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>base_passenger_fare</th>\n",
       "      <th>tolls</th>\n",
       "      <th>bcf</th>\n",
       "      <th>sales_tax</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>tips</th>\n",
       "      <th>driver_pay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14314</td>\n",
       "      <td>14314</td>\n",
       "      <td>14314.000000</td>\n",
       "      <td>14314.000000</td>\n",
       "      <td>14314.000000</td>\n",
       "      <td>14314.000000</td>\n",
       "      <td>14314.000000</td>\n",
       "      <td>14314.000000</td>\n",
       "      <td>14314.000000</td>\n",
       "      <td>14314.000000</td>\n",
       "      <td>14314.000000</td>\n",
       "      <td>14314.000000</td>\n",
       "      <td>14314.000000</td>\n",
       "      <td>14314.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-05-04 22:47:47.122257920</td>\n",
       "      <td>2022-05-04 23:05:35.212868352</td>\n",
       "      <td>40.737286</td>\n",
       "      <td>-73.935201</td>\n",
       "      <td>40.737268</td>\n",
       "      <td>-73.935344</td>\n",
       "      <td>4.375069</td>\n",
       "      <td>21.038508</td>\n",
       "      <td>0.680329</td>\n",
       "      <td>0.616612</td>\n",
       "      <td>1.868726</td>\n",
       "      <td>1.056047</td>\n",
       "      <td>0.796023</td>\n",
       "      <td>16.981945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-01-01 03:27:51</td>\n",
       "      <td>2020-01-01 03:30:21</td>\n",
       "      <td>40.561994</td>\n",
       "      <td>-74.170885</td>\n",
       "      <td>40.561994</td>\n",
       "      <td>-74.174002</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2021-02-28 22:55:11</td>\n",
       "      <td>2021-02-28 23:04:53.500000</td>\n",
       "      <td>40.690890</td>\n",
       "      <td>-73.984197</td>\n",
       "      <td>40.691201</td>\n",
       "      <td>-73.984197</td>\n",
       "      <td>1.550000</td>\n",
       "      <td>10.490000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-05-03 21:00:57.500000</td>\n",
       "      <td>2022-05-03 21:09:21.500000</td>\n",
       "      <td>40.736824</td>\n",
       "      <td>-73.948788</td>\n",
       "      <td>40.737698</td>\n",
       "      <td>-73.948136</td>\n",
       "      <td>2.830000</td>\n",
       "      <td>16.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-07-04 21:57:38</td>\n",
       "      <td>2023-07-04 22:06:56.249999872</td>\n",
       "      <td>40.774376</td>\n",
       "      <td>-73.900317</td>\n",
       "      <td>40.774376</td>\n",
       "      <td>-73.899735</td>\n",
       "      <td>5.520000</td>\n",
       "      <td>26.370000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>2.350000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.635000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-08-31 21:37:05</td>\n",
       "      <td>2024-08-31 21:57:29</td>\n",
       "      <td>40.899528</td>\n",
       "      <td>-73.726655</td>\n",
       "      <td>40.899528</td>\n",
       "      <td>-73.726655</td>\n",
       "      <td>33.170000</td>\n",
       "      <td>188.580000</td>\n",
       "      <td>43.310000</td>\n",
       "      <td>5.450000</td>\n",
       "      <td>17.570000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>117.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.068439</td>\n",
       "      <td>0.064694</td>\n",
       "      <td>0.068843</td>\n",
       "      <td>0.068135</td>\n",
       "      <td>4.267862</td>\n",
       "      <td>15.215758</td>\n",
       "      <td>2.718339</td>\n",
       "      <td>0.494388</td>\n",
       "      <td>1.414169</td>\n",
       "      <td>1.333108</td>\n",
       "      <td>2.413868</td>\n",
       "      <td>12.024392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     pickup_datetime               dropoff_datetime  \\\n",
       "count                          14314                          14314   \n",
       "mean   2022-05-04 22:47:47.122257920  2022-05-04 23:05:35.212868352   \n",
       "min              2020-01-01 03:27:51            2020-01-01 03:30:21   \n",
       "25%              2021-02-28 22:55:11     2021-02-28 23:04:53.500000   \n",
       "50%       2022-05-03 21:00:57.500000     2022-05-03 21:09:21.500000   \n",
       "75%              2023-07-04 21:57:38  2023-07-04 22:06:56.249999872   \n",
       "max              2024-08-31 21:37:05            2024-08-31 21:57:29   \n",
       "std                              NaN                            NaN   \n",
       "\n",
       "             pu_lat        pu_lon        do_lat        do_lon    trip_miles  \\\n",
       "count  14314.000000  14314.000000  14314.000000  14314.000000  14314.000000   \n",
       "mean      40.737286    -73.935201     40.737268    -73.935344      4.375069   \n",
       "min       40.561994    -74.170885     40.561994    -74.174002      0.010000   \n",
       "25%       40.690890    -73.984197     40.691201    -73.984197      1.550000   \n",
       "50%       40.736824    -73.948788     40.737698    -73.948136      2.830000   \n",
       "75%       40.774376    -73.900317     40.774376    -73.899735      5.520000   \n",
       "max       40.899528    -73.726655     40.899528    -73.726655     33.170000   \n",
       "std        0.068439      0.064694      0.068843      0.068135      4.267862   \n",
       "\n",
       "       base_passenger_fare         tolls           bcf     sales_tax  \\\n",
       "count         14314.000000  14314.000000  14314.000000  14314.000000   \n",
       "mean             21.038508      0.680329      0.616612      1.868726   \n",
       "min               0.610000      0.000000      0.000000      0.000000   \n",
       "25%              10.490000      0.000000      0.290000      0.910000   \n",
       "50%              16.690000      0.000000      0.470000      1.450000   \n",
       "75%              26.370000      0.000000      0.770000      2.350000   \n",
       "max             188.580000     43.310000      5.450000     17.570000   \n",
       "std              15.215758      2.718339      0.494388      1.414169   \n",
       "\n",
       "       congestion_surcharge          tips    driver_pay  \n",
       "count          14314.000000  14314.000000  14314.000000  \n",
       "mean               1.056047      0.796023     16.981945  \n",
       "min                0.000000      0.000000      0.280000  \n",
       "25%                0.000000      0.000000      8.410000  \n",
       "50%                0.000000      0.000000     13.470000  \n",
       "75%                2.750000      0.000000     21.635000  \n",
       "max                2.750000     50.000000    117.920000  \n",
       "std                1.333108      2.413868     12.024392  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing weather/2020_weather.csv...\n",
      "Processing weather/2023_weather.csv...\n",
      "Processing weather/2021_weather.csv...\n",
      "Processing weather/2024_weather.csv...\n",
      "Processing weather/2022_weather.csv...\n",
      "Cleaned data saved.\n",
      "\n",
      "Statistics and non-zero value counts:\n",
      "\n",
      "Column: precipitation\n",
      "count    1462.000000\n",
      "mean        0.144097\n",
      "std         0.419189\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.070000\n",
      "max         7.130000\n",
      "Name: precipitation, dtype: float64\n",
      "Non-zero count: 512\n",
      "\n",
      "Column: average_wind_speed\n",
      "count    1462.000000\n",
      "mean        4.848974\n",
      "std         2.499979\n",
      "min         0.000000\n",
      "25%         3.000000\n",
      "50%         4.600000\n",
      "75%         6.300000\n",
      "max        14.200000\n",
      "Name: average_wind_speed, dtype: float64\n",
      "Non-zero count: 1406\n",
      "\n",
      "Column: snowfall\n",
      "count    1462.000000\n",
      "mean        0.041792\n",
      "std         0.530350\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max        14.800000\n",
      "Name: snowfall, dtype: float64\n",
      "Non-zero count: 27\n",
      "\n",
      "Hourly Weather Data (Preview):\n",
      "                 date  precipitation  wind_speed\n",
      "0 2020-01-01 00:51:00            0.0         8.0\n",
      "1 2020-01-01 01:51:00            0.0         8.0\n",
      "2 2020-01-01 02:51:00            0.0        14.0\n",
      "3 2020-01-01 03:51:00            0.0        11.0\n",
      "4 2020-01-01 04:51:00            0.0         6.0\n",
      "\n",
      "Daily Weather Data (Preview):\n",
      "         date  precipitation  average_wind_speed  snowfall\n",
      "0  2020-01-01           0.00                 8.6       0.0\n",
      "1  2020-01-02           0.00                 5.4       0.0\n",
      "2  2020-01-03           0.15                 3.4       0.0\n",
      "3  2020-01-04           0.27                 4.4       0.0\n",
      "4  2020-01-05           0.00                11.3       0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "# Function 1: Clean hourly weather data\n",
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, low_memory=False)\n",
    "        df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "        df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], errors=\"coerce\")\n",
    "        hourly_data = df[[\"DATE\", \"HourlyPrecipitation\", \"HourlyWindSpeed\"]].copy()\n",
    "        hourly_data.replace(\"T\", 0, inplace=True)\n",
    "        hourly_data.replace(regex=r\"[^\\d.]\", value=np.nan, inplace=True)\n",
    "        hourly_data.columns = [\"date\", \"precipitation\", \"wind_speed\"]\n",
    "        hourly_data[\"precipitation\"] = pd.to_numeric(hourly_data[\"precipitation\"], errors=\"coerce\")\n",
    "        hourly_data[\"wind_speed\"] = pd.to_numeric(hourly_data[\"wind_speed\"], errors=\"coerce\")\n",
    "        return hourly_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {csv_file}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Function 2: Clean daily weather data\n",
    "def clean_month_weather_data_daily(csv_file):\n",
    "    try:\n",
    "        # 读取文件\n",
    "        df = pd.read_csv(csv_file, low_memory=False)\n",
    "        \n",
    "        # 修整列名，解析日期\n",
    "        df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "        df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], errors=\"coerce\")\n",
    "        \n",
    "        # 提取需要的列\n",
    "        daily_data = df[[\"DATE\", \"DailyPrecipitation\", \"DailyAverageWindSpeed\", \"DailySnowfall\"]].copy()\n",
    "        \n",
    "        # 处理特殊值 \"T\" 和其他非数字字符\n",
    "        daily_data.replace(\"T\", 0, inplace=True)\n",
    "        daily_data.replace(regex=r\"[^\\d.]\", value=np.nan, inplace=True)\n",
    "        \n",
    "        # 重命名列\n",
    "        daily_data.columns = [\"date\", \"precipitation\", \"average_wind_speed\", \"snowfall\"]\n",
    "        \n",
    "        # 转换为数值类型\n",
    "        daily_data[\"precipitation\"] = pd.to_numeric(daily_data[\"precipitation\"], errors=\"coerce\")\n",
    "        daily_data[\"average_wind_speed\"] = pd.to_numeric(daily_data[\"average_wind_speed\"], errors=\"coerce\")\n",
    "        daily_data[\"snowfall\"] = pd.to_numeric(daily_data[\"snowfall\"], errors=\"coerce\")\n",
    "        \n",
    "        # 确保仅保留日期部分\n",
    "        daily_data[\"date\"] = pd.to_datetime(daily_data[\"date\"]).dt.date\n",
    "        \n",
    "        # 按日期聚合数据：降水量和雪量求和，风速取均值\n",
    "        daily_data = daily_data.groupby(\"date\").agg({\n",
    "            \"precipitation\": \"sum\",  # 降水量总和\n",
    "            \"average_wind_speed\": \"mean\",  # 风速均值\n",
    "            \"snowfall\": \"sum\"  # 雪量总和\n",
    "        }).reset_index()\n",
    "        \n",
    "        return daily_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {csv_file}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Function 3: Process all files in a directory\n",
    "def load_and_clean_weather_data(directory_path):\n",
    "    hourly_weather_data = pd.DataFrame()\n",
    "    daily_weather_data = pd.DataFrame()\n",
    "\n",
    "    for file in os.listdir(directory_path):\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(directory_path, file)\n",
    "            print(f\"Processing {file_path}...\")\n",
    "            hourly_data = clean_month_weather_data_hourly(file_path)\n",
    "            daily_data = clean_month_weather_data_daily(file_path)\n",
    "            hourly_weather_data = pd.concat([hourly_weather_data, hourly_data], ignore_index=True)\n",
    "            daily_weather_data = pd.concat([daily_weather_data, daily_data], ignore_index=True)\n",
    "\n",
    "    return hourly_weather_data, daily_weather_data\n",
    "\n",
    "# Function 4: Analyze non-zero values and statistics\n",
    "def analyze_weather_data(df, columns_to_analyze):\n",
    "    print(\"\\nStatistics and non-zero value counts:\")\n",
    "    for col in columns_to_analyze:\n",
    "        print(f\"\\nColumn: {col}\")\n",
    "        print(df[col].describe())\n",
    "        non_zero_count = (df[col] > 0).sum()\n",
    "        print(f\"Non-zero count: {non_zero_count}\")\n",
    "\n",
    "# Main script\n",
    "directory_path = \"weather/\"  # Update to your directory path\n",
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data(directory_path)\n",
    "\n",
    "\n",
    "# 替换空值为0\n",
    "daily_weather_data.fillna(0, inplace=True)\n",
    "hourly_weather_data.fillna(0, inplace=True)\n",
    "\n",
    "# Save cleaned data\n",
    "hourly_weather_data.to_csv(\"cleaned_hourly_weather_data.csv\", index=False)\n",
    "daily_weather_data.to_csv(\"cleaned_daily_weather_data.csv\", index=False)\n",
    "print(\"Cleaned data saved.\")\n",
    "\n",
    "# Analyze daily weather data\n",
    "columns_to_analyze = [\"precipitation\", \"average_wind_speed\", \"snowfall\"]\n",
    "analyze_weather_data(daily_weather_data, columns_to_analyze)\n",
    "\n",
    "\n",
    "# Preview data\n",
    "print(\"\\nHourly Weather Data (Preview):\")\n",
    "print(hourly_weather_data.head())\n",
    "\n",
    "print(\"\\nDaily Weather Data (Preview):\")\n",
    "print(daily_weather_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81e1f323-7876-403b-b614-e759ea3caf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020_weather.csv: 2020-01-01 00:51:00 to 2020-12-31 23:59:00\n",
      "2023_weather.csv: 2023-01-01 00:20:00 to 2023-12-31 23:51:00\n",
      "2021_weather.csv: 2021-01-01 00:51:00 to 2021-12-31 23:59:00\n",
      "2024_weather.csv: 2020-01-01 00:51:00 to 2020-01-01 04:51:00\n",
      "2022_weather.csv: 2022-01-01 00:51:00 to 2022-12-31 23:59:00\n"
     ]
    }
   ],
   "source": [
    "directory_path = 'weather'  # 更新路径\n",
    "\n",
    "# 遍历文件并检查时间范围\n",
    "for file in os.listdir(directory_path):\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_path = os.path.join(directory_path, file)\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        if \"DATE\" in df.columns:\n",
    "            df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], errors=\"coerce\")\n",
    "            print(f\"{file}: {df['DATE'].min()} to {df['DATE'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eece1cb5-c613-43a4-8e71-ae9ba348c624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    weather_csv_files = get_all_weather_csvs(WEATHER_CSV_DIR)\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "        \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48216557",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935261b7-ae23-427c-97ff-ea31aa4e44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dcb502-d1d1-447d-aa68-11bff0dc53b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f090eb94-a5b0-4d93-bf82-a596d2521b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c074aa3-a5f2-4586-8748-411e1e6c11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query either via sqlalchemy\n",
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_1)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "pd.read_sql(QUERY_1, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
