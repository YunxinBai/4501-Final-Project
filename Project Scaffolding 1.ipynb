{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "854d1cd4-4133-4b31-a288-b51de769f9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "import os\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import fiona\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TLC_URL = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "WEATHER_CSV_DIR = \"\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "### Load Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58708809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones(file_path):\n",
    "    geofile = gpd.read_file(file_path)\n",
    "    return geofile\n",
    "    \n",
    "taxi_zones = load_taxi_zones(\"taxi_zones.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d04c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_coords_for_taxi_zone_id(zone_loc_id, loaded_taxi_zones):\n",
    "    if loaded_taxi_zones.crs is None:\n",
    "        loaded_taxi_zones = loaded_taxi_zones.set_crs(epsg=2263)\n",
    "\n",
    "    # Find the zone with the matching LocationID\n",
    "    zone = loaded_taxi_zones[loaded_taxi_zones['LocationID'] == zone_loc_id]\n",
    "\n",
    "    # If no match is found, return None\n",
    "    if zone.empty:\n",
    "        return None\n",
    "\n",
    "    # Temporarily reproject to a projected CRS for accurate centroid calculation\n",
    "    projected_zone = zone.to_crs(epsg=2263)\n",
    "    centroid = projected_zone.geometry.centroid.iloc[0]\n",
    "\n",
    "    # Transform the centroid back to geographic CRS (latitude/longitude)\n",
    "    centroid_geo = gpd.GeoSeries([centroid], crs=2263).to_crs(epsg=4326)\n",
    "\n",
    "    # Return the latitude and longitude as a tuple\n",
    "    return (centroid_geo.geometry.iloc[0].y, centroid_geo.geometry.iloc[0].x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sample_size(population, p = 0.5) -> int:\n",
    "    \"\"\"\n",
    "    Calculates the required sample size using Cochran's formula.\n",
    "\n",
    "    Args:\n",
    "        population (int): The total population size.\n",
    "        confidence_level (float): Confidence level as a proportion (default is 0.95 for 95% confidence).\n",
    "        margin_of_error (float): Desired margin of error as a proportion (default is 0.05 for 5%).\n",
    "\n",
    "    Returns:\n",
    "        int: Calculated sample size.\n",
    "    \"\"\"\n",
    "    # Z-value for confidence level (default: 1.96 for 95%)\n",
    "    z = 1.96\n",
    "    margin_of_error = 0.05\n",
    "    q = 1 - p  # Complementary proportion\n",
    "    \n",
    "    # Cochran's sample size formula for infinite population\n",
    "    n_0 = (z**2 * p * q) / (margin_of_error**2)\n",
    "    \n",
    "    # Adjust for finite population size\n",
    "    sample_size = n_0 / (1 + (n_0 - 1) / population)\n",
    "    \n",
    "    return math.ceil(sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc33eed4-b2e9-4ab3-94a8-59f04e464c98",
   "metadata": {},
   "source": [
    "### Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_page(page_url):\n",
    "    \"\"\"\n",
    "    Fetches all URLs from a given webpage.\n",
    "\n",
    "    Args:\n",
    "        page_url (str): URL of the webpage to scrape.\n",
    "\n",
    "    Returns:\n",
    "        list: List of all URLs found on the webpage.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Send a GET request to the page\n",
    "        response = requests.get(page_url)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise Exception(f\"Failed to access the URL: {page_url}. Error: {e}\")\n",
    "    \n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Find all anchor tags with href attributes\n",
    "    links = soup.find_all(\"a\", href=True)\n",
    "    \n",
    "    # Extract and return all href attributes\n",
    "    all_urls = [link[\"href\"] for link in links]\n",
    "    \n",
    "    return all_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_parquet_urls(links):\n",
    "    parquet_urls = []\n",
    "    for url in links:\n",
    "        # Normalize the URL (strip whitespace, handle cases like trailing slashes)\n",
    "        url = url.strip()\n",
    "        # Use regex to ensure matching even with query parameters\n",
    "        if re.search(r\"\\.parquet(\\?.*)?$\", url):\n",
    "            parquet_urls.append(url)\n",
    "    return parquet_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Process Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_month(parquet_url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downloads, processes, and saves Yellow Taxi dataset for a given month.\n",
    "\n",
    "    Args:\n",
    "        parquet_url (str): URL of the Yellow Taxi Parquet file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Sampled and processed DataFrame.\n",
    "    \"\"\"\n",
    "    # Default directory for processed Yellow Taxi data\n",
    "    save_dir = \"processed_data/yellow_taxi\"\n",
    "\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Extract file name and define local path\n",
    "    file_name = parquet_url.split(\"/\")[-1]\n",
    "    local_file_path = os.path.join(save_dir, file_name)\n",
    "\n",
    "    # Download the file if not already downloaded\n",
    "    if not os.path.exists(local_file_path):\n",
    "        response = requests.get(parquet_url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(local_file_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024 * 1024):  # 1MB chunks\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "    # Load the dataset\n",
    "    data = pd.read_parquet(local_file_path)\n",
    "\n",
    "    # Determine population size\n",
    "    population = len(data)\n",
    "\n",
    "    # Calculate sample size (using p = 0.5 for Yellow Taxi data)\n",
    "    sample_size = calculate_sample_size(population, p = 0.5)\n",
    "\n",
    "    # Sample the dataset\n",
    "    sampled_data = data.sample(n=sample_size, random_state=42) if population > sample_size else data\n",
    "\n",
    "    # Save the sampled dataset\n",
    "    processed_file_path = os.path.join(save_dir, f\"sampled_{file_name}\")\n",
    "    sampled_data.to_parquet(processed_file_path)\n",
    "    return sampled_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(parquet_urls):\n",
    "    all_taxi_dataframes = []\n",
    "    yellow_taxi_pattern = re.compile(r\"yellow_tripdata_(2020-(0[1-9]|1[0-2])|202[1-3]-(0[1-9]|1[0-2])|2024-(0[1-8]))\\.parquet\")\n",
    "\n",
    "    # Filter URLs matching the pattern\n",
    "    yellow_taxi_urls = [url for url in parquet_urls if yellow_taxi_pattern.search(url)]\n",
    "    \n",
    "    for url in yellow_taxi_urls:\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_taxi_month(url)\n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        \n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    \n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "830ef27d-90a7-4ed8-aaa2-da3a7e5c8403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_taxi_data(taxi_data):\n",
    "    \"\"\"\n",
    "    Cleans the taxi data by retaining specified columns, normalizing column names,\n",
    "    converting column types, and removing invalid trips. Also unifies column names \n",
    "    across different data sources using a mapping.\n",
    "\n",
    "    Args:\n",
    "        taxi_data (pd.DataFrame): The input taxi data DataFrame.\n",
    "        column_mapping (dict): A dictionary mapping original column names to standardized names.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned and filtered taxi data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 0: Normalize column names using the column_mapping\n",
    "    column_mapping = {\n",
    "    'tpep_pickup_datetime': 'pickup_datetime',\n",
    "    'tpep_dropoff_datetime': 'dropoff_datetime',\n",
    "    'trip_distance': 'trip_miles'}\n",
    "   \n",
    "    taxi_data = taxi_data.rename(columns=column_mapping)\n",
    "\n",
    "    \n",
    "    # Add latitude and longitude for PULocationID and DOLocationID\n",
    "    taxi_data[\"PU_coords\"] = taxi_data[\"PULocationID\"].apply(lambda x: lookup_coords_for_taxi_zone_id(x, taxi_zones))\n",
    "    taxi_data[\"DO_coords\"] = taxi_data[\"DOLocationID\"].apply(lambda x: lookup_coords_for_taxi_zone_id(x, taxi_zones))\n",
    "\n",
    "    # Remove trips with invalid location IDs (where coordinates could not be found)\n",
    "    taxi_data = taxi_data.dropna(subset=[\"PU_coords\", \"DO_coords\"]).reset_index(drop=True)\n",
    "\n",
    "    # Split coordinates into latitude and longitude for pickups and dropoffs\n",
    "    taxi_data[[\"PU_lat\", \"PU_lon\"]] = pd.DataFrame(taxi_data[\"PU_coords\"].tolist(), index=taxi_data.index)\n",
    "    taxi_data[[\"DO_lat\", \"DO_lon\"]] = pd.DataFrame(taxi_data[\"DO_coords\"].tolist(), index=taxi_data.index)\n",
    "\n",
    "    # Drop temporary coordinate columns\n",
    "    taxi_data = taxi_data.drop(columns=[\"PU_coords\", \"DO_coords\"])\n",
    "    \n",
    "    # Step 1: Retain only the required columns\n",
    "    required_columns = [\n",
    "        'pickup_datetime', 'dropoff_datetime', 'trip_miles',\n",
    "        'PU_lat', 'PU_lon', 'DO_lat', 'DO_lon', 'total_amount'\n",
    "    ]\n",
    "    taxi_data = taxi_data[required_columns]\n",
    "\n",
    "    # Removing Invalid Data Points\n",
    "    # Remove rows where trip distance is less than or equal to 0\n",
    "    taxi_data = taxi_data[taxi_data[\"trip_miles\"] > 0]\n",
    "\n",
    "    # Remove rows where fare amount or total amount is less than or equal to 0\n",
    "    taxi_data = taxi_data[taxi_data[\"total_amount\"] > 0]\n",
    "\n",
    "    # Remove rows where pickup time is after dropoff time\n",
    "    taxi_data = taxi_data[taxi_data[\"pickup_datetime\"] < taxi_data[\"dropoff_datetime\"]]\n",
    "\n",
    "    # Reset index after filtering\n",
    "    taxi_data = taxi_data.reset_index(drop=True)\n",
    "\n",
    "    #Normalize column names\n",
    "    taxi_data.columns = [col.lower() for col in taxi_data.columns]\n",
    "\n",
    "    # Normalizing and Using Appropriate Column Types\n",
    "    taxi_data[\"pickup_datetime\"] = pd.to_datetime(taxi_data[\"pickup_datetime\"], errors=\"coerce\")\n",
    "    taxi_data[\"dropoff_datetime\"] = pd.to_datetime(taxi_data[\"dropoff_datetime\"], errors=\"coerce\")\n",
    "\n",
    "    # Ensure numeric columns are float or int\n",
    "    numeric_columns = [\"trip_miles\", \"pu_lat\", \"pu_lon\", \"do_lat\", \"do_lon\", \"total_amount\"]\n",
    "    taxi_data[numeric_columns] = taxi_data[numeric_columns].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    # Drop rows with invalid datetime or numeric values\n",
    "    taxi_data = taxi_data.dropna(subset=[\"pickup_datetime\", \"dropoff_datetime\"] + numeric_columns)\n",
    "\n",
    "    # Reset index after dropping invalid rows\n",
    "    taxi_data = taxi_data.reset_index(drop=True)\n",
    "\n",
    "    # Removing Trips Outside the Latitude/Longitude Bounding Box\n",
    "    # Latitude and longitude bounding box\n",
    "    lat_min, lon_min = 40.560445, -74.242330\n",
    "    lat_max, lon_max = 40.908524, -73.717047\n",
    "\n",
    "    # Filter rows where pickup and dropoff locations are within the bounding box\n",
    "    taxi_data = taxi_data[\n",
    "        (taxi_data[\"pu_lat\"] >= lat_min) & (taxi_data[\"pu_lat\"] <= lat_max) &\n",
    "        (taxi_data[\"pu_lon\"] >= lon_min) & (taxi_data[\"pu_lon\"] <= lon_max) &\n",
    "        (taxi_data[\"do_lat\"] >= lat_min) & (taxi_data[\"do_lat\"] <= lat_max) &\n",
    "        (taxi_data[\"do_lon\"] >= lon_min) & (taxi_data[\"do_lon\"] <= lon_max)\n",
    "    ]\n",
    "\n",
    "    # Reset index after filtering\n",
    "    taxi_data = taxi_data.reset_index(drop=True)\n",
    "\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "200776ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data():\n",
    "    all_urls = get_all_urls_from_page(TLC_URL)\n",
    "    all_parquet_urls = filter_parquet_urls(all_urls)\n",
    "    taxi_data = get_and_clean_taxi_data(all_parquet_urls)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "876bd645",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data = get_taxi_data()\n",
    "taxi_data = clean_taxi_data(taxi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10ebd75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>pu_lat</th>\n",
       "      <th>pu_lon</th>\n",
       "      <th>do_lat</th>\n",
       "      <th>do_lon</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-20 13:31:30</td>\n",
       "      <td>2024-01-20 14:03:25</td>\n",
       "      <td>17.14</td>\n",
       "      <td>40.646985</td>\n",
       "      <td>-73.786530</td>\n",
       "      <td>40.749914</td>\n",
       "      <td>-73.970443</td>\n",
       "      <td>90.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-18 21:52:46</td>\n",
       "      <td>2024-01-18 22:03:21</td>\n",
       "      <td>2.49</td>\n",
       "      <td>40.764421</td>\n",
       "      <td>-73.977569</td>\n",
       "      <td>40.790011</td>\n",
       "      <td>-73.945750</td>\n",
       "      <td>22.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01 03:43:58</td>\n",
       "      <td>2024-01-01 03:50:47</td>\n",
       "      <td>1.84</td>\n",
       "      <td>40.866075</td>\n",
       "      <td>-73.919308</td>\n",
       "      <td>40.857779</td>\n",
       "      <td>-73.885867</td>\n",
       "      <td>12.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-19 22:20:12</td>\n",
       "      <td>2024-01-19 22:50:12</td>\n",
       "      <td>3.60</td>\n",
       "      <td>40.748497</td>\n",
       "      <td>-73.992438</td>\n",
       "      <td>40.778766</td>\n",
       "      <td>-73.951010</td>\n",
       "      <td>33.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-06 22:41:50</td>\n",
       "      <td>2024-01-06 22:43:24</td>\n",
       "      <td>0.04</td>\n",
       "      <td>40.791705</td>\n",
       "      <td>-73.973049</td>\n",
       "      <td>40.791705</td>\n",
       "      <td>-73.973049</td>\n",
       "      <td>6.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pickup_datetime    dropoff_datetime  trip_miles     pu_lat     pu_lon  \\\n",
       "0 2024-01-20 13:31:30 2024-01-20 14:03:25       17.14  40.646985 -73.786530   \n",
       "1 2024-01-18 21:52:46 2024-01-18 22:03:21        2.49  40.764421 -73.977569   \n",
       "2 2024-01-01 03:43:58 2024-01-01 03:50:47        1.84  40.866075 -73.919308   \n",
       "3 2024-01-19 22:20:12 2024-01-19 22:50:12        3.60  40.748497 -73.992438   \n",
       "4 2024-01-06 22:41:50 2024-01-06 22:43:24        0.04  40.791705 -73.973049   \n",
       "\n",
       "      do_lat     do_lon  total_amount  \n",
       "0  40.749914 -73.970443         90.96  \n",
       "1  40.790011 -73.945750         22.50  \n",
       "2  40.857779 -73.885867         12.50  \n",
       "3  40.778766 -73.951010         33.95  \n",
       "4  40.791705 -73.973049          6.20  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9da7089-3f6b-4f93-a22e-76bf554daca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20727 entries, 0 to 20726\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   pickup_datetime   20727 non-null  datetime64[ns]\n",
      " 1   dropoff_datetime  20727 non-null  datetime64[ns]\n",
      " 2   trip_miles        20727 non-null  float64       \n",
      " 3   pu_lat            20727 non-null  float64       \n",
      " 4   pu_lon            20727 non-null  float64       \n",
      " 5   do_lat            20727 non-null  float64       \n",
      " 6   do_lon            20727 non-null  float64       \n",
      " 7   total_amount      20727 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(6)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "taxi_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50c85e25-6416-4c16-b98c-09596cdc6865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>pu_lat</th>\n",
       "      <th>pu_lon</th>\n",
       "      <th>do_lat</th>\n",
       "      <th>do_lon</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20727</td>\n",
       "      <td>20727</td>\n",
       "      <td>20727.000000</td>\n",
       "      <td>20727.000000</td>\n",
       "      <td>20727.000000</td>\n",
       "      <td>20727.000000</td>\n",
       "      <td>20727.000000</td>\n",
       "      <td>20727.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-04-30 03:11:05.817195008</td>\n",
       "      <td>2022-04-30 03:27:08.452115712</td>\n",
       "      <td>3.271662</td>\n",
       "      <td>40.753350</td>\n",
       "      <td>-73.966955</td>\n",
       "      <td>40.755752</td>\n",
       "      <td>-73.970757</td>\n",
       "      <td>22.482544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-01-01 00:11:06</td>\n",
       "      <td>2020-01-01 00:30:50</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>40.576961</td>\n",
       "      <td>-74.029893</td>\n",
       "      <td>40.576961</td>\n",
       "      <td>-74.174002</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2021-02-27 15:22:24</td>\n",
       "      <td>2021-02-27 15:32:43.500000</td>\n",
       "      <td>1.090000</td>\n",
       "      <td>40.740337</td>\n",
       "      <td>-73.989845</td>\n",
       "      <td>40.740337</td>\n",
       "      <td>-73.989845</td>\n",
       "      <td>12.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-04-29 09:06:52</td>\n",
       "      <td>2022-04-29 09:22:22</td>\n",
       "      <td>1.810000</td>\n",
       "      <td>40.758028</td>\n",
       "      <td>-73.977698</td>\n",
       "      <td>40.758028</td>\n",
       "      <td>-73.977698</td>\n",
       "      <td>17.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-06-28 11:02:35</td>\n",
       "      <td>2023-06-28 11:17:36</td>\n",
       "      <td>3.310000</td>\n",
       "      <td>40.773633</td>\n",
       "      <td>-73.961764</td>\n",
       "      <td>40.775932</td>\n",
       "      <td>-73.959635</td>\n",
       "      <td>24.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-08-31 22:43:47</td>\n",
       "      <td>2024-08-31 23:26:23</td>\n",
       "      <td>67.900000</td>\n",
       "      <td>40.899528</td>\n",
       "      <td>-73.739337</td>\n",
       "      <td>40.899528</td>\n",
       "      <td>-73.726655</td>\n",
       "      <td>262.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.110867</td>\n",
       "      <td>0.032385</td>\n",
       "      <td>0.045035</td>\n",
       "      <td>0.033130</td>\n",
       "      <td>0.036128</td>\n",
       "      <td>17.359922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     pickup_datetime               dropoff_datetime  \\\n",
       "count                          20727                          20727   \n",
       "mean   2022-04-30 03:11:05.817195008  2022-04-30 03:27:08.452115712   \n",
       "min              2020-01-01 00:11:06            2020-01-01 00:30:50   \n",
       "25%              2021-02-27 15:22:24     2021-02-27 15:32:43.500000   \n",
       "50%              2022-04-29 09:06:52            2022-04-29 09:22:22   \n",
       "75%              2023-06-28 11:02:35            2023-06-28 11:17:36   \n",
       "max              2024-08-31 22:43:47            2024-08-31 23:26:23   \n",
       "std                              NaN                            NaN   \n",
       "\n",
       "         trip_miles        pu_lat        pu_lon        do_lat        do_lon  \\\n",
       "count  20727.000000  20727.000000  20727.000000  20727.000000  20727.000000   \n",
       "mean       3.271662     40.753350    -73.966955     40.755752    -73.970757   \n",
       "min        0.010000     40.576961    -74.029893     40.576961    -74.174002   \n",
       "25%        1.090000     40.740337    -73.989845     40.740337    -73.989845   \n",
       "50%        1.810000     40.758028    -73.977698     40.758028    -73.977698   \n",
       "75%        3.310000     40.773633    -73.961764     40.775932    -73.959635   \n",
       "max       67.900000     40.899528    -73.739337     40.899528    -73.726655   \n",
       "std        4.110867      0.032385      0.045035      0.033130      0.036128   \n",
       "\n",
       "       total_amount  \n",
       "count  20727.000000  \n",
       "mean      22.482544  \n",
       "min        1.000000  \n",
       "25%       12.800000  \n",
       "50%       17.020000  \n",
       "75%       24.500000  \n",
       "max      262.700000  \n",
       "std       17.359922  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a3ec0e5e-387e-499d-9ba5-a80b85697d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"taxi_data_cleaned.csv\"\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "taxi_data.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07574983-f41d-4cd6-8f70-489493089b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_uber_month(parquet_url):\n",
    "    save_dir = \"processed_data/hvhf\"\n",
    "\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Extract file name and define local path\n",
    "    file_name = parquet_url.split(\"/\")[-1]\n",
    "    local_file_path = os.path.join(save_dir, file_name)\n",
    "\n",
    "    # Download the file if not already downloaded\n",
    "    if not os.path.exists(local_file_path):\n",
    "        response = requests.get(parquet_url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(local_file_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024 * 1024):  # 1MB chunks\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "    data = pd.read_parquet(local_file_path)\n",
    "\n",
    "    # Determine population size\n",
    "    population = len(data)\n",
    "\n",
    "    # Calculate sample size (using p = 0.5 for Yellow Taxi data)\n",
    "    sample_size = calculate_sample_size(population, p = 0.4)\n",
    "\n",
    "    # Sample the dataset\n",
    "    sampled_data = data.sample(n=sample_size, random_state=42) if population > sample_size else data\n",
    "\n",
    "    # Save the sampled dataset\n",
    "    processed_file_path = os.path.join(save_dir, f\"sampled_{file_name}\")\n",
    "    if not os.path.exists(processed_file_path):\n",
    "        sampled_data.to_parquet(processed_file_path)\n",
    "    return sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b3d85ff-313c-41a2-9a46-261a9a2bb10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_uber_data(parquet_urls):\n",
    "    all_uber_dataframes = []\n",
    "    hvfhv_pattern = re.compile(r\"fhvhv_tripdata_(2020-(0[1-9]|1[0-2])|202[1-3]-(0[1-9]|1[0-2])|2024-(0[1-8]))\\.parquet\")\n",
    "    hvfhv_urls = [url for url in parquet_urls if hvfhv_pattern.search(url)]\n",
    "    for url in hvfhv_urls:\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_uber_month(url)\n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        \n",
    "        all_uber_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    uber_data = pd.concat(all_uber_dataframes)\n",
    "    return uber_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(uber_data):\n",
    "    uber_data['hvfhs_license_num'] = uber_data['hvfhs_license_num'].astype(str)\n",
    "    uber_data = uber_data[uber_data['hvfhs_license_num'] == 'HV0003'].copy()\n",
    "\n",
    "    # Coords matching\n",
    "    uber_data[\"PU_coords\"] = uber_data[\"PULocationID\"].apply(lambda x: lookup_coords_for_taxi_zone_id(x, taxi_zones))\n",
    "    uber_data[\"DO_coords\"] = uber_data[\"DOLocationID\"].apply(lambda x: lookup_coords_for_taxi_zone_id(x, taxi_zones))\n",
    "\n",
    "    # Remove trips with invalid location IDs (where coordinates could not be found)\n",
    "    uber_data = uber_data.dropna(subset=[\"PU_coords\", \"DO_coords\"]).reset_index(drop=True)\n",
    "\n",
    "    # Split coordinates into latitude and longitude for pickups and dropoffs\n",
    "    uber_data[[\"PU_lat\", \"PU_lon\"]] = pd.DataFrame(uber_data[\"PU_coords\"].tolist(), index=uber_data.index)\n",
    "    uber_data[[\"DO_lat\", \"DO_lon\"]] = pd.DataFrame(uber_data[\"DO_coords\"].tolist(), index=uber_data.index)\n",
    "\n",
    "    # Drop temporary coordinate columns\n",
    "    uber_data = uber_data.drop(columns=[\"PU_coords\", \"DO_coords\"])\n",
    "\n",
    "    # Step 1: Retain only the required columns\n",
    "    required_columns = [\n",
    "        'pickup_datetime', 'dropoff_datetime',\n",
    "        'PU_lat', 'PU_lon', 'DO_lat', 'DO_lon',\n",
    "        'trip_miles', 'base_passenger_fare', 'tolls', 'bcf',\n",
    "        'sales_tax', 'congestion_surcharge', 'tips','airport_fee'\n",
    "    ]\n",
    "    uber_data = uber_data[required_columns]\n",
    "\n",
    "    # Removing Invalid Data Points\n",
    "    # Remove rows where trip distance is less than or equal to 0\n",
    "    uber_data = uber_data[uber_data[\"trip_miles\"] > 0]\n",
    "\n",
    "    # Remove rows where fare amount or total amount is less than or equal to 0\n",
    "    uber_data = uber_data[uber_data[\"base_passenger_fare\"] > 0]\n",
    "\n",
    "    # Remove rows where pickup time is after dropoff time\n",
    "    uber_data = uber_data[uber_data[\"pickup_datetime\"] < uber_data[\"dropoff_datetime\"]]\n",
    "\n",
    "    # Normalize column names\n",
    "    uber_data.columns = [col.lower() for col in uber_data.columns]\n",
    "\n",
    "    # Ensure numeric columns are float or int\n",
    "    numeric_columns = [\n",
    "        'pu_lat', 'pu_lon', 'do_lat', 'do_lon',\n",
    "        'trip_miles', 'base_passenger_fare', 'tolls', 'bcf',\n",
    "        'sales_tax', 'congestion_surcharge', 'tips', 'airport_fee'\n",
    "    ]\n",
    "    uber_data[numeric_columns] = uber_data[numeric_columns].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    # Drop rows with invalid datetime or numeric values\n",
    "    uber_data = uber_data.dropna(subset=[\"pickup_datetime\", \"dropoff_datetime\"] + numeric_columns)\n",
    "    uber_data['total_amount'] = (uber_data['base_passenger_fare'] + uber_data['tolls'] + uber_data['bcf'] + uber_data['sales_tax'] + uber_data['congestion_surcharge'] +\n",
    "                                 uber_data['airport_fee'] + uber_data['tips'])\n",
    "    updated_columns = ['pickup_datetime', 'dropoff_datetime', 'pu_lat', 'pu_lon', 'do_lat', 'do_lon','trip_miles','total_amount']\n",
    "    uber_data = uber_data[updated_columns]\n",
    "\n",
    "    # Removing Trips Outside the Latitude/Longitude Bounding Box\n",
    "    # Latitude and longitude bounding box\n",
    "    lat_min, lon_min = 40.560445, -74.242330\n",
    "    lat_max, lon_max = 40.908524, -73.717047\n",
    "\n",
    "    # Filter rows where pickup and dropoff locations are within the bounding box\n",
    "    uber_data = uber_data[\n",
    "        (uber_data[\"pu_lat\"] >= lat_min) & (uber_data[\"pu_lat\"] <= lat_max) &\n",
    "        (uber_data[\"pu_lon\"] >= lon_min) & (uber_data[\"pu_lon\"] <= lon_max) &\n",
    "        (uber_data[\"do_lat\"] >= lat_min) & (uber_data[\"do_lat\"] <= lat_max) &\n",
    "        (uber_data[\"do_lon\"] >= lon_min) & (uber_data[\"do_lon\"] <= lon_max)\n",
    "    ]\n",
    "\n",
    "    # Reset index after filtering\n",
    "    uber_data = uber_data.reset_index(drop=True)\n",
    "    \n",
    "    return uber_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    all_urls = get_all_urls_from_page(TLC_URL)\n",
    "    all_parquet_urls = filter_parquet_urls(all_urls)\n",
    "    uber_data = get_and_clean_uber_data(all_parquet_urls)\n",
    "    uber_data = load_and_clean_uber_data(uber_data)\n",
    "    return uber_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c2bd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "339997e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>pu_lat</th>\n",
       "      <th>pu_lon</th>\n",
       "      <th>do_lat</th>\n",
       "      <th>do_lon</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-26 08:07:17</td>\n",
       "      <td>2024-01-26 08:35:38</td>\n",
       "      <td>40.646116</td>\n",
       "      <td>-73.951623</td>\n",
       "      <td>40.666559</td>\n",
       "      <td>-73.895364</td>\n",
       "      <td>4.29</td>\n",
       "      <td>30.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-19 02:17:05</td>\n",
       "      <td>2024-01-19 02:29:12</td>\n",
       "      <td>40.882403</td>\n",
       "      <td>-73.910665</td>\n",
       "      <td>40.857108</td>\n",
       "      <td>-73.932832</td>\n",
       "      <td>2.55</td>\n",
       "      <td>16.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-21 01:44:00</td>\n",
       "      <td>2024-01-21 02:08:30</td>\n",
       "      <td>40.748575</td>\n",
       "      <td>-73.985156</td>\n",
       "      <td>40.715370</td>\n",
       "      <td>-73.936794</td>\n",
       "      <td>6.37</td>\n",
       "      <td>33.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-20 12:58:40</td>\n",
       "      <td>2024-01-20 13:15:42</td>\n",
       "      <td>40.758028</td>\n",
       "      <td>-73.977698</td>\n",
       "      <td>40.753309</td>\n",
       "      <td>-74.004016</td>\n",
       "      <td>1.99</td>\n",
       "      <td>23.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-02 08:40:48</td>\n",
       "      <td>2024-01-02 08:54:28</td>\n",
       "      <td>40.666559</td>\n",
       "      <td>-73.895364</td>\n",
       "      <td>40.676644</td>\n",
       "      <td>-73.913632</td>\n",
       "      <td>2.23</td>\n",
       "      <td>17.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pickup_datetime    dropoff_datetime     pu_lat     pu_lon     do_lat  \\\n",
       "0 2024-01-26 08:07:17 2024-01-26 08:35:38  40.646116 -73.951623  40.666559   \n",
       "1 2024-01-19 02:17:05 2024-01-19 02:29:12  40.882403 -73.910665  40.857108   \n",
       "2 2024-01-21 01:44:00 2024-01-21 02:08:30  40.748575 -73.985156  40.715370   \n",
       "3 2024-01-20 12:58:40 2024-01-20 13:15:42  40.758028 -73.977698  40.753309   \n",
       "4 2024-01-02 08:40:48 2024-01-02 08:54:28  40.666559 -73.895364  40.676644   \n",
       "\n",
       "      do_lon  trip_miles  total_amount  \n",
       "0 -73.895364        4.29         30.69  \n",
       "1 -73.932832        2.55         16.90  \n",
       "2 -73.936794        6.37         33.19  \n",
       "3 -74.004016        1.99         23.91  \n",
       "4 -73.913632        2.23         17.95  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "944ae7fb-e7e0-4ff8-b956-993854dcc826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10545 entries, 0 to 10544\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   pickup_datetime   10545 non-null  datetime64[ns]\n",
      " 1   dropoff_datetime  10545 non-null  datetime64[ns]\n",
      " 2   pu_lat            10545 non-null  float64       \n",
      " 3   pu_lon            10545 non-null  float64       \n",
      " 4   do_lat            10545 non-null  float64       \n",
      " 5   do_lon            10545 non-null  float64       \n",
      " 6   trip_miles        10545 non-null  float64       \n",
      " 7   total_amount      10545 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(6)\n",
      "memory usage: 659.2 KB\n"
     ]
    }
   ],
   "source": [
    "uber_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6fddeb14-cd70-4e83-8f93-974642c3bea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>pu_lat</th>\n",
       "      <th>pu_lon</th>\n",
       "      <th>do_lat</th>\n",
       "      <th>do_lon</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10545</td>\n",
       "      <td>10545</td>\n",
       "      <td>10545.000000</td>\n",
       "      <td>10545.000000</td>\n",
       "      <td>10545.000000</td>\n",
       "      <td>10545.000000</td>\n",
       "      <td>10545.000000</td>\n",
       "      <td>10545.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-12-17 14:17:10.341299200</td>\n",
       "      <td>2022-12-17 14:35:41.576481536</td>\n",
       "      <td>40.736481</td>\n",
       "      <td>-73.937300</td>\n",
       "      <td>40.735830</td>\n",
       "      <td>-73.937359</td>\n",
       "      <td>4.455277</td>\n",
       "      <td>28.322973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2021-03-29 10:06:06</td>\n",
       "      <td>2021-03-29 10:18:54</td>\n",
       "      <td>40.561994</td>\n",
       "      <td>-74.170885</td>\n",
       "      <td>40.561994</td>\n",
       "      <td>-74.174002</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2022-02-07 08:12:21</td>\n",
       "      <td>2022-02-07 09:06:15</td>\n",
       "      <td>40.691507</td>\n",
       "      <td>-73.985937</td>\n",
       "      <td>40.691507</td>\n",
       "      <td>-73.985156</td>\n",
       "      <td>1.550000</td>\n",
       "      <td>13.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-12-16 22:51:49</td>\n",
       "      <td>2022-12-16 23:02:11</td>\n",
       "      <td>40.736824</td>\n",
       "      <td>-73.951010</td>\n",
       "      <td>40.736824</td>\n",
       "      <td>-73.949540</td>\n",
       "      <td>2.850000</td>\n",
       "      <td>21.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-10-28 23:36:51</td>\n",
       "      <td>2023-10-28 23:41:22</td>\n",
       "      <td>40.771570</td>\n",
       "      <td>-73.901709</td>\n",
       "      <td>40.773633</td>\n",
       "      <td>-73.899735</td>\n",
       "      <td>5.640000</td>\n",
       "      <td>35.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-08-31 21:37:05</td>\n",
       "      <td>2024-08-31 21:57:29</td>\n",
       "      <td>40.899528</td>\n",
       "      <td>-73.726655</td>\n",
       "      <td>40.899528</td>\n",
       "      <td>-73.726655</td>\n",
       "      <td>33.170000</td>\n",
       "      <td>239.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.066292</td>\n",
       "      <td>0.065408</td>\n",
       "      <td>0.066639</td>\n",
       "      <td>0.069448</td>\n",
       "      <td>4.371850</td>\n",
       "      <td>21.284107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     pickup_datetime               dropoff_datetime  \\\n",
       "count                          10545                          10545   \n",
       "mean   2022-12-17 14:17:10.341299200  2022-12-17 14:35:41.576481536   \n",
       "min              2021-03-29 10:06:06            2021-03-29 10:18:54   \n",
       "25%              2022-02-07 08:12:21            2022-02-07 09:06:15   \n",
       "50%              2022-12-16 22:51:49            2022-12-16 23:02:11   \n",
       "75%              2023-10-28 23:36:51            2023-10-28 23:41:22   \n",
       "max              2024-08-31 21:37:05            2024-08-31 21:57:29   \n",
       "std                              NaN                            NaN   \n",
       "\n",
       "             pu_lat        pu_lon        do_lat        do_lon    trip_miles  \\\n",
       "count  10545.000000  10545.000000  10545.000000  10545.000000  10545.000000   \n",
       "mean      40.736481    -73.937300     40.735830    -73.937359      4.455277   \n",
       "min       40.561994    -74.170885     40.561994    -74.174002      0.010000   \n",
       "25%       40.691507    -73.985937     40.691507    -73.985156      1.550000   \n",
       "50%       40.736824    -73.951010     40.736824    -73.949540      2.850000   \n",
       "75%       40.771570    -73.901709     40.773633    -73.899735      5.640000   \n",
       "max       40.899528    -73.726655     40.899528    -73.726655     33.170000   \n",
       "std        0.066292      0.065408      0.066639      0.069448      4.371850   \n",
       "\n",
       "       total_amount  \n",
       "count  10545.000000  \n",
       "mean      28.322973  \n",
       "min        0.680000  \n",
       "25%       13.960000  \n",
       "50%       21.940000  \n",
       "75%       35.150000  \n",
       "max      239.830000  \n",
       "std       21.284107  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "33bf45d4-dbc2-4928-b45b-fe29e4d20683",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"uber_data_cleaned.csv\"\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "uber_data.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eece1cb5-c613-43a4-8e71-ae9ba348c624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory):\n",
    "    \"\"\"\n",
    "    Get all CSV files in the specified directory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing CSV files.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of file paths to all CSV files.\n",
    "    \"\"\"\n",
    "    csv_files = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".csv\"):\n",
    "            csv_files.append(os.path.join(directory, file))\n",
    "    return csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    \"\"\"\n",
    "    Clean and process hourly weather data from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Processed hourly weather data.\n",
    "    \"\"\"    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, low_memory=False)\n",
    "        df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "        df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], errors=\"coerce\")\n",
    "        \n",
    "        hourly_data = df[[\"DATE\", \"HourlyPrecipitation\", \"HourlyWindSpeed\"]].copy()\n",
    "        hourly_data.replace(\"T\", 0, inplace=True)\n",
    "        hourly_data.replace(regex=r\"[^\\d.]\", value=np.nan, inplace=True)\n",
    "        \n",
    "        hourly_data.columns = [\"date\", \"precipitation\", \"wind_speed\"]\n",
    "        hourly_data[\"precipitation\"] = pd.to_numeric(hourly_data[\"precipitation\"], errors=\"coerce\")\n",
    "        hourly_data[\"wind_speed\"] = pd.to_numeric(hourly_data[\"wind_speed\"], errors=\"coerce\")\n",
    "        \n",
    "        return hourly_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {csv_file}: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    try:\n",
    "        # \n",
    "        df = pd.read_csv(csv_file, low_memory=False)\n",
    "        \n",
    "        # \n",
    "        df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "        df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], errors=\"coerce\")\n",
    "        \n",
    "        #  Daily \n",
    "        daily_data = df[[\"DATE\", \"DailyPrecipitation\", \"DailyAverageWindSpeed\", \"DailySnowfall\"]].copy()\n",
    "        \n",
    "        #  \"T\" \n",
    "        daily_data.replace(\"T\", 0, inplace=True)\n",
    "        daily_data.replace(regex=r\"[^\\d.]\", value=np.nan, inplace=True)\n",
    "        \n",
    "        # \n",
    "        daily_data.columns = [\"date\", \"precipitation\", \"average_wind_speed\", \"snowfall\"]\n",
    "        \n",
    "        # \n",
    "        daily_data[\"precipitation\"] = pd.to_numeric(daily_data[\"precipitation\"], errors=\"coerce\")\n",
    "        daily_data[\"average_wind_speed\"] = pd.to_numeric(daily_data[\"average_wind_speed\"], errors=\"coerce\")\n",
    "        daily_data[\"snowfall\"] = pd.to_numeric(daily_data[\"snowfall\"], errors=\"coerce\")\n",
    "        \n",
    "        # \n",
    "        daily_data = daily_data.dropna(subset=[\"precipitation\", \"average_wind_speed\", \"snowfall\"], how=\"all\")\n",
    "        \n",
    "        # \n",
    "        daily_data[\"date\"] = daily_data[\"date\"].dt.date\n",
    "        \n",
    "        return daily_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {csv_file}: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data(directory):\n",
    "    \"\"\"\n",
    "    Load and clean weather data from all CSV files in the directory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing CSV files.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Two DataFrames - hourly and daily weather data.\n",
    "    \"\"\"\n",
    "    \n",
    "    weather_csv_files = get_all_weather_csvs(directory)\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "        \n",
    "    for csv_file in weather_csv_files:\n",
    "        print(f\"Processing {csv_file}...\")\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # Concatenate dataframes\n",
    "    hourly_data = pd.concat(hourly_dataframes, ignore_index=True)\n",
    "    daily_data = pd.concat(daily_dataframes, ignore_index=True)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dcf3c2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing weather/2020_weather.csv...\n",
      "Processing weather/2023_weather.csv...\n",
      "Processing weather/2021_weather.csv...\n",
      "Processing weather/2024_weather.csv...\n",
      "Processing weather/2022_weather.csv...\n",
      "Cleaned data saved.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    WEATHER_CSV_DIR = \"weather/\"  # Path to your weather data directory\n",
    "\n",
    "    # Process weather data\n",
    "    hourly_weather_data, daily_weather_data = load_and_clean_weather_data(WEATHER_CSV_DIR)\n",
    "\n",
    "    # Replace NaN values with 0\n",
    "    hourly_weather_data.fillna(0, inplace=True)\n",
    "    daily_weather_data.fillna(0, inplace=True)\n",
    "\n",
    "    # Save cleaned data to CSV\n",
    "    hourly_weather_data.to_csv(\"cleaned_hourly_weather_data.csv\", index=False)\n",
    "    daily_weather_data.to_csv(\"cleaned_daily_weather_data.csv\", index=False)\n",
    "    print(\"Cleaned data saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "48216557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 00:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01 01:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01 02:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01 03:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01 04:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date  precipitation  wind_speed\n",
       "0 2020-01-01 00:51:00            0.0         8.0\n",
       "1 2020-01-01 01:51:00            0.0         8.0\n",
       "2 2020-01-01 02:51:00            0.0        14.0\n",
       "3 2020-01-01 03:51:00            0.0        11.0\n",
       "4 2020-01-01 04:51:00            0.0         6.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "935261b7-ae23-427c-97ff-ea31aa4e44c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 56098 entries, 0 to 56097\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   date           56098 non-null  datetime64[ns]\n",
      " 1   precipitation  56098 non-null  float64       \n",
      " 2   wind_speed     56098 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(2)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "hourly_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a7dcb502-d1d1-447d-aa68-11bff0dc53b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>56098</td>\n",
       "      <td>56098.000000</td>\n",
       "      <td>56098.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-05-29 21:14:19.618881024</td>\n",
       "      <td>0.010288</td>\n",
       "      <td>4.537238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-01-01 00:51:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2021-03-18 19:01:45</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-05-28 01:21:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-08-15 05:39:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-10-22 18:51:00</td>\n",
       "      <td>3.470000</td>\n",
       "      <td>2237.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.056033</td>\n",
       "      <td>13.883208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                date  precipitation    wind_speed\n",
       "count                          56098   56098.000000  56098.000000\n",
       "mean   2022-05-29 21:14:19.618881024       0.010288      4.537238\n",
       "min              2020-01-01 00:51:00       0.000000      0.000000\n",
       "25%              2021-03-18 19:01:45       0.000000      0.000000\n",
       "50%              2022-05-28 01:21:00       0.000000      5.000000\n",
       "75%              2023-08-15 05:39:00       0.000000      7.000000\n",
       "max              2024-10-22 18:51:00       3.470000   2237.000000\n",
       "std                              NaN       0.056033     13.883208"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_weather_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>average_wind_speed</th>\n",
       "      <th>snowfall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>0.27</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  precipitation  average_wind_speed  snowfall\n",
       "0  2020-01-01           0.00                 8.6       0.0\n",
       "1  2020-01-02           0.00                 5.4       0.0\n",
       "2  2020-01-03           0.15                 3.4       0.0\n",
       "3  2020-01-04           0.27                 4.4       0.0\n",
       "4  2020-01-05           0.00                11.3       0.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f090eb94-a5b0-4d93-bf82-a596d2521b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1755 entries, 0 to 1754\n",
      "Data columns (total 4 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   date                1755 non-null   object \n",
      " 1   precipitation       1755 non-null   float64\n",
      " 2   average_wind_speed  1755 non-null   float64\n",
      " 3   snowfall            1755 non-null   float64\n",
      "dtypes: float64(3), object(1)\n",
      "memory usage: 55.0+ KB\n"
     ]
    }
   ],
   "source": [
    "daily_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8c074aa3-a5f2-4586-8748-411e1e6c11da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precipitation</th>\n",
       "      <th>average_wind_speed</th>\n",
       "      <th>snowfall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1755.000000</td>\n",
       "      <td>1755.000000</td>\n",
       "      <td>1755.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.141966</td>\n",
       "      <td>4.835499</td>\n",
       "      <td>0.039088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.414574</td>\n",
       "      <td>2.467952</td>\n",
       "      <td>0.493457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.060000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.130000</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>14.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       precipitation  average_wind_speed     snowfall\n",
       "count    1755.000000         1755.000000  1755.000000\n",
       "mean        0.141966            4.835499     0.039088\n",
       "std         0.414574            2.467952     0.493457\n",
       "min         0.000000            0.000000     0.000000\n",
       "25%         0.000000            3.000000     0.000000\n",
       "50%         0.000000            4.500000     0.000000\n",
       "75%         0.060000            6.200000     0.000000\n",
       "max         7.130000           14.200000    14.800000"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Load cleaned datasets\n",
    "daily_data = pd.read_csv(\"/cleaned_daily_weather_data.csv\")\n",
    "hourly_data = pd.read_csv(\"/cleaned_hourly_weather_data.csv\")\n",
    "taxi_data = pd.read_csv(\"/taxi_data_cleaned.csv\")\n",
    "uber_data = pd.read_csv(\"/uber_data_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE hourly_weather (\n",
    "    datetime TEXT NOT NULL,\n",
    "    temperature FLOAT,\n",
    "    precipitation FLOAT,\n",
    "    wind_speed FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE daily_weather (\n",
    "    date TEXT NOT NULL,\n",
    "    avg_precipitation FLOAT,\n",
    "    avg_wind_speed FLOAT,\n",
    "    total_snowfall FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE taxi_trips (\n",
    "    pickup_datetime TEXT NOT NULL,\n",
    "    dropoff_datetime TEXT NOT NULL,\n",
    "    pu_lat FLOAT,\n",
    "    pu_lon FLOAT,\n",
    "    do_lat FLOAT,\n",
    "    do_lon FLOAT,\n",
    "    trip_miles FLOAT,\n",
    "    total_amount FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE uber_trips (\n",
    "    pickup_datetime TEXT NOT NULL,\n",
    "    dropoff_datetime TEXT NOT NULL,\n",
    "    pu_lat FLOAT,\n",
    "    pu_lon FLOAT,\n",
    "    do_lat FLOAT,\n",
    "    do_lon FLOAT,\n",
    "    trip_miles FLOAT,\n",
    "    total_amount FLOAT\n",
    ");\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add DROP TABLE statements\n",
    "DROP_TABLES = [\n",
    "    \"DROP TABLE IF EXISTS hourly_weather;\",\n",
    "    \"DROP TABLE IF EXISTS daily_weather;\",\n",
    "    \"DROP TABLE IF EXISTS taxi_trips;\",\n",
    "    \"DROP TABLE IF EXISTS uber_trips;\"\n",
    "]\n",
    "\n",
    "# Create SQLite engine\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# Create tables using raw connection\n",
    "conn = engine.raw_connection()\n",
    "try:\n",
    "    cursor = conn.cursor()\n",
    "    # Drop existing tables individually\n",
    "    for drop_table in DROP_TABLES:\n",
    "        cursor.execute(drop_table)\n",
    "    \n",
    "    # Create new tables\n",
    "    cursor.execute(HOURLY_WEATHER_SCHEMA.strip())\n",
    "    cursor.execute(DAILY_WEATHER_SCHEMA.strip())\n",
    "    cursor.execute(TAXI_TRIPS_SCHEMA.strip())\n",
    "    cursor.execute(UBER_TRIPS_SCHEMA.strip())\n",
    "    conn.commit()  # Commit changes\n",
    "finally:\n",
    "    cursor.close()  # Always close the cursor\n",
    "    conn.close()    # Always close the connection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    \"\"\"\n",
    "    Writes dataframes to the corresponding SQL tables.\n",
    "    Args:\n",
    "        table_to_df_dict (dict): A dictionary where keys are table names\n",
    "                                 and values are the respective DataFrames.\n",
    "    \"\"\"\n",
    "    for table_name, df in table_to_df_dict.items():\n",
    "        df.to_sql(table_name, con=engine, if_exists='replace', index=False)\n",
    "\n",
    "# Map table names to cleaned DataFrames\n",
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data to the database\n",
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ae0f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Query the first 5 rows of the taxi_trips table\n",
    "taxi_data_preview = pd.read_sql(\"SELECT * FROM taxi_trips LIMIT 5;\", con=engine)\n",
    "print(taxi_data_preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query either via sqlalchemy\n",
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_1)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "pd.read_sql(QUERY_1, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
