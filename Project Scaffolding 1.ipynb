{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "854d1cd4-4133-4b31-a288-b51de769f9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "import os\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import fiona\n",
    "import math\n",
    "import pytest\n",
    "from shapely.geometry import Point\n",
    "from typing import Optional, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TLC_URL = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "WEATHER_CSV_DIR = \"\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "### Load Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58708809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones(file_path: str) -> GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Load taxi zones from a shapefile or GeoJSON file into a GeoDataFrame.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the geospatial file (e.g., a shapefile or GeoJSON).\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: A GeoDataFrame containing the taxi zones data.\n",
    "    \"\"\"\n",
    "    geofile: GeoDataFrame = gpd.read_file(file_path)\n",
    "    return geofile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "783ca81b-319c-4a64-91e6-b11f268e3819",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zones = load_taxi_zones(\"taxi_zones.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d04c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_coords_for_taxi_zone_id(\n",
    "    zone_loc_id: int, \n",
    "    loaded_taxi_zones: GeoDataFrame\n",
    ") -> Optional[Tuple[float, float]]:\n",
    "    \"\"\"\n",
    "    Look up the latitude and longitude coordinates for a taxi zone by its LocationID.\n",
    "\n",
    "    Args:\n",
    "        zone_loc_id (int): The LocationID of the taxi zone.\n",
    "        loaded_taxi_zones (GeoDataFrame): A GeoDataFrame containing taxi zone geometries.\n",
    "\n",
    "    Returns:\n",
    "        Optional[Tuple[float, float]]: A tuple containing the latitude and longitude \n",
    "        coordinates if the zone is found, or None if the LocationID is not in the data.\n",
    "    \"\"\"\n",
    "    # Ensure the CRS is set if missing\n",
    "    if loaded_taxi_zones.crs is None:\n",
    "        loaded_taxi_zones = loaded_taxi_zones.set_crs(epsg=2263)\n",
    "\n",
    "    # Find the zone matching the LocationID\n",
    "    zone = loaded_taxi_zones[loaded_taxi_zones['LocationID'] == zone_loc_id]\n",
    "    if zone.empty:\n",
    "        return None\n",
    "    projected_zone = zone.to_crs(epsg=2263)\n",
    "    centroid = projected_zone.geometry.centroid.iloc[0]\n",
    "    centroid_geo = gpd.GeoSeries([centroid], crs=2263).to_crs(epsg=4326)\n",
    "\n",
    "    # Return latitude and longitude as a tuple\n",
    "    return (centroid_geo.geometry.iloc[0].y, centroid_geo.geometry.iloc[0].x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sample_size(population: int, p: float = 0.5) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the required sample size using Cochran's formula, adjusted for finite populations.\n",
    "\n",
    "    Args:\n",
    "        population (int): The total population size.\n",
    "        p (float, optional): Proportion of the population with the desired characteristic. \n",
    "            Defaults to 0.5 for maximum variability.\n",
    "\n",
    "    Returns:\n",
    "        int: The calculated sample size, rounded up to the nearest whole number.\n",
    "    \"\"\"\n",
    "    # Z-value for 95% confidence level\n",
    "    z: float = 1.96\n",
    "    \n",
    "    # Margin of error (5% by default)\n",
    "    margin_of_error: float = 0.05\n",
    "    \n",
    "    # Complementary proportion\n",
    "    q: float = 1 - p\n",
    "    \n",
    "    # Cochran's formula\n",
    "    n_0: float = (z**2 * p * q) / (margin_of_error**2)\n",
    "    \n",
    "    # Adjust for finite population\n",
    "    sample_size: float = n_0 / (1 + (n_0 - 1) / population)\n",
    "    \n",
    "    return math.ceil(sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc33eed4-b2e9-4ab3-94a8-59f04e464c98",
   "metadata": {},
   "source": [
    "### Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_page(page_url: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Fetch all URLs from a given webpage.\n",
    "\n",
    "    Args:\n",
    "        page_url (str): URL of the webpage to scrape.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of all URLs found on the webpage.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an error accessing the webpage or scraping its content.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(page_url)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise Exception(f\"Failed to access the URL: {page_url}. Error: {e}\")\n",
    "    \n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    links = soup.find_all(\"a\", href=True)\n",
    "    all_urls: List[str] = [link[\"href\"] for link in links]\n",
    "    \n",
    "    return all_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_parquet_urls(links: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Filter a list of URLs to include only those pointing to Parquet files.\n",
    "\n",
    "    Args:\n",
    "        links (List[str]): A list of URLs to filter.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of URLs that point to Parquet files.\n",
    "    \"\"\"\n",
    "    parquet_urls: List[str] = []\n",
    "    for url in links:\n",
    "        # Normalize the URL\n",
    "        url = url.strip()\n",
    "        if re.search(r\"\\.parquet(\\?.*)?$\", url):\n",
    "            parquet_urls.append(url)\n",
    "    \n",
    "    return parquet_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Process Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_month(parquet_url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download, process, and sample a Yellow Taxi dataset for a given month.\n",
    "\n",
    "    Args:\n",
    "        parquet_url (str): URL of the Yellow Taxi Parquet file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A sampled and processed DataFrame.\n",
    "    \"\"\"\n",
    "    # Default directory for Yellow Taxi data\n",
    "    save_dir = \"processed_data/yellow_taxi\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Extract file name and define local path\n",
    "    file_name = parquet_url.split(\"/\")[-1]\n",
    "    local_file_path = os.path.join(save_dir, file_name)\n",
    "\n",
    "    # Download the file if not exist\n",
    "    if not os.path.exists(local_file_path):\n",
    "        response = requests.get(parquet_url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(local_file_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024 * 1024):  # 1MB chunks\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "    # Load the dataset\n",
    "    data = pd.read_parquet(local_file_path)\n",
    "\n",
    "    # Calculate sample size (using p = 0.5 for Yellow Taxi data)\n",
    "    population = len(data)\n",
    "    sample_size = calculate_sample_size(population, p = 0.5)\n",
    "    sampled_data = data.sample(n=sample_size, random_state=42) if population > sample_size else data\n",
    "    \n",
    "    processed_file_path = os.path.join(save_dir, f\"sampled_{file_name}\")\n",
    "    sampled_data.to_parquet(processed_file_path)\n",
    "    return sampled_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(parquet_urls: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download, process, and combine Yellow Taxi data from multiple Parquet file URLs.\n",
    "\n",
    "    Args:\n",
    "        parquet_urls (List[str]): A list of URLs pointing to Yellow Taxi Parquet files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A combined DataFrame containing processed data from all valid Parquet files.\n",
    "    \"\"\"\n",
    "    all_taxi_dataframes: List[pd.DataFrame] = []\n",
    "\n",
    "    # Regex pattern for valid Yellow Taxi Parquet files (2020-2024 dates)\n",
    "    yellow_taxi_pattern: re.Pattern = re.compile(\n",
    "        r\"yellow_tripdata_(2020-(0[1-9]|1[0-2])|202[1-3]-(0[1-9]|1[0-2])|2024-(0[1-8]))\\.parquet\"\n",
    "    )\n",
    "\n",
    "    # Filter URLs matching the pattern\n",
    "    yellow_taxi_urls: List[str] = [url for url in parquet_urls if yellow_taxi_pattern.search(url)]\n",
    "\n",
    "    for url in yellow_taxi_urls:\n",
    "        save_dir: str = \"processed_data/yellow_taxi\"\n",
    "        file_name: str = f\"sampled_{url.split('/')[-1]}\"\n",
    "        processed_file_path: str = os.path.join(save_dir, file_name)\n",
    "\n",
    "        if os.path.exists(processed_file_path):\n",
    "            dataframe: pd.DataFrame = pd.read_parquet(processed_file_path)\n",
    "        else:\n",
    "            dataframe: pd.DataFrame = get_and_clean_taxi_month(url)\n",
    "\n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "\n",
    "    # Combine all DataFrames into one\n",
    "    taxi_data: pd.DataFrame = pd.concat(all_taxi_dataframes, ignore_index=True)\n",
    "\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "830ef27d-90a7-4ed8-aaa2-da3a7e5c8403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_taxi_data(taxi_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean the taxi data by retaining specific columns, normalizing column names,\n",
    "    converting column types, and removing invalid trips. This includes unifying column names\n",
    "    across different data sources using a predefined mapping.\n",
    "\n",
    "    Args:\n",
    "        taxi_data (pd.DataFrame): The input taxi data DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned and filtered taxi data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize column names using the column_mapping\n",
    "    column_mapping = {\n",
    "    'tpep_pickup_datetime': 'pickup_datetime',\n",
    "    'tpep_dropoff_datetime': 'dropoff_datetime',\n",
    "    'trip_distance': 'trip_miles'}\n",
    "   \n",
    "    taxi_data = taxi_data.rename(columns=column_mapping)\n",
    "\n",
    "    \n",
    "    # Add latitude and longitude\n",
    "    taxi_data[\"PU_coords\"] = taxi_data[\"PULocationID\"].apply(lambda x: lookup_coords_for_taxi_zone_id(x, taxi_zones))\n",
    "    taxi_data[\"DO_coords\"] = taxi_data[\"DOLocationID\"].apply(lambda x: lookup_coords_for_taxi_zone_id(x, taxi_zones))\n",
    "    taxi_data = taxi_data.dropna(subset=[\"PU_coords\", \"DO_coords\"]).reset_index(drop=True)\n",
    "    taxi_data[[\"PU_lat\", \"PU_lon\"]] = pd.DataFrame(taxi_data[\"PU_coords\"].tolist(), index=taxi_data.index)\n",
    "    taxi_data[[\"DO_lat\", \"DO_lon\"]] = pd.DataFrame(taxi_data[\"DO_coords\"].tolist(), index=taxi_data.index)\n",
    "    taxi_data = taxi_data.drop(columns=[\"PU_coords\", \"DO_coords\"])\n",
    "    \n",
    "    # Retain only the required columns\n",
    "    required_columns = [\n",
    "        'pickup_datetime', 'dropoff_datetime', 'trip_miles',\n",
    "        'PU_lat', 'PU_lon', 'DO_lat', 'DO_lon', 'total_amount'\n",
    "    ]\n",
    "    taxi_data = taxi_data[required_columns]\n",
    "\n",
    "    # Removing Invalid Data Points\n",
    "    taxi_data = taxi_data[taxi_data[\"trip_miles\"] > 0]\n",
    "    taxi_data = taxi_data[taxi_data[\"total_amount\"] > 0]\n",
    "    taxi_data = taxi_data[taxi_data[\"pickup_datetime\"] < taxi_data[\"dropoff_datetime\"]]\n",
    "\n",
    "    #Normalize column names\n",
    "    taxi_data.columns = [col.lower() for col in taxi_data.columns]\n",
    "\n",
    "    # Normalizing and Using Appropriate Column Types\n",
    "    taxi_data[\"pickup_datetime\"] = pd.to_datetime(taxi_data[\"pickup_datetime\"], errors=\"coerce\")\n",
    "    taxi_data[\"dropoff_datetime\"] = pd.to_datetime(taxi_data[\"dropoff_datetime\"], errors=\"coerce\")\n",
    "    numeric_columns = [\"trip_miles\", \"pu_lat\", \"pu_lon\", \"do_lat\", \"do_lon\", \"total_amount\"]\n",
    "    taxi_data[numeric_columns] = taxi_data[numeric_columns].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    taxi_data = taxi_data.dropna(subset=[\"pickup_datetime\", \"dropoff_datetime\"] + numeric_columns)\n",
    "\n",
    "    # Removing Trips Outside the Latitude/Longitude Bounding Box\n",
    "    lat_min, lon_min = 40.560445, -74.242330\n",
    "    lat_max, lon_max = 40.908524, -73.717047\n",
    "    taxi_data = taxi_data[\n",
    "        (taxi_data[\"pu_lat\"] >= lat_min) & (taxi_data[\"pu_lat\"] <= lat_max) &\n",
    "        (taxi_data[\"pu_lon\"] >= lon_min) & (taxi_data[\"pu_lon\"] <= lon_max) &\n",
    "        (taxi_data[\"do_lat\"] >= lat_min) & (taxi_data[\"do_lat\"] <= lat_max) &\n",
    "        (taxi_data[\"do_lon\"] >= lon_min) & (taxi_data[\"do_lon\"] <= lon_max)\n",
    "    ]\n",
    "\n",
    "    # Reset index after filtering\n",
    "    taxi_data = taxi_data.reset_index(drop=True)\n",
    "\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "200776ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve, filter, and clean taxi data from the TLC data page.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Fetches all URLs from the TLC webpage.\n",
    "    2. Filters the URLs to retain only those pointing to Parquet files.\n",
    "    3. Downloads, processes, and combines the data from these Parquet files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the combined and cleaned taxi data.\n",
    "    \"\"\"\n",
    "    all_urls: list[str] = get_all_urls_from_page(TLC_URL)\n",
    "    all_parquet_urls: list[str] = filter_parquet_urls(all_urls)\n",
    "    taxi_data: pd.DataFrame = get_and_clean_taxi_data(all_parquet_urls)\n",
    "\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "876bd645",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data = get_taxi_data()\n",
    "taxi_data = clean_taxi_data(taxi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10ebd75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>pu_lat</th>\n",
       "      <th>pu_lon</th>\n",
       "      <th>do_lat</th>\n",
       "      <th>do_lon</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-20 13:31:30</td>\n",
       "      <td>2024-01-20 14:03:25</td>\n",
       "      <td>17.14</td>\n",
       "      <td>40.646985</td>\n",
       "      <td>-73.786530</td>\n",
       "      <td>40.749914</td>\n",
       "      <td>-73.970443</td>\n",
       "      <td>90.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-18 21:52:46</td>\n",
       "      <td>2024-01-18 22:03:21</td>\n",
       "      <td>2.49</td>\n",
       "      <td>40.764421</td>\n",
       "      <td>-73.977569</td>\n",
       "      <td>40.790011</td>\n",
       "      <td>-73.945750</td>\n",
       "      <td>22.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01 03:43:58</td>\n",
       "      <td>2024-01-01 03:50:47</td>\n",
       "      <td>1.84</td>\n",
       "      <td>40.866075</td>\n",
       "      <td>-73.919308</td>\n",
       "      <td>40.857779</td>\n",
       "      <td>-73.885867</td>\n",
       "      <td>12.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-19 22:20:12</td>\n",
       "      <td>2024-01-19 22:50:12</td>\n",
       "      <td>3.60</td>\n",
       "      <td>40.748497</td>\n",
       "      <td>-73.992438</td>\n",
       "      <td>40.778766</td>\n",
       "      <td>-73.951010</td>\n",
       "      <td>33.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-06 22:41:50</td>\n",
       "      <td>2024-01-06 22:43:24</td>\n",
       "      <td>0.04</td>\n",
       "      <td>40.791705</td>\n",
       "      <td>-73.973049</td>\n",
       "      <td>40.791705</td>\n",
       "      <td>-73.973049</td>\n",
       "      <td>6.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pickup_datetime    dropoff_datetime  trip_miles     pu_lat     pu_lon  \\\n",
       "0 2024-01-20 13:31:30 2024-01-20 14:03:25       17.14  40.646985 -73.786530   \n",
       "1 2024-01-18 21:52:46 2024-01-18 22:03:21        2.49  40.764421 -73.977569   \n",
       "2 2024-01-01 03:43:58 2024-01-01 03:50:47        1.84  40.866075 -73.919308   \n",
       "3 2024-01-19 22:20:12 2024-01-19 22:50:12        3.60  40.748497 -73.992438   \n",
       "4 2024-01-06 22:41:50 2024-01-06 22:43:24        0.04  40.791705 -73.973049   \n",
       "\n",
       "      do_lat     do_lon  total_amount  \n",
       "0  40.749914 -73.970443         90.96  \n",
       "1  40.790011 -73.945750         22.50  \n",
       "2  40.857779 -73.885867         12.50  \n",
       "3  40.778766 -73.951010         33.95  \n",
       "4  40.791705 -73.973049          6.20  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9da7089-3f6b-4f93-a22e-76bf554daca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20727 entries, 0 to 20726\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   pickup_datetime   20727 non-null  datetime64[ns]\n",
      " 1   dropoff_datetime  20727 non-null  datetime64[ns]\n",
      " 2   trip_miles        20727 non-null  float64       \n",
      " 3   pu_lat            20727 non-null  float64       \n",
      " 4   pu_lon            20727 non-null  float64       \n",
      " 5   do_lat            20727 non-null  float64       \n",
      " 6   do_lon            20727 non-null  float64       \n",
      " 7   total_amount      20727 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(6)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "taxi_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50c85e25-6416-4c16-b98c-09596cdc6865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>pu_lat</th>\n",
       "      <th>pu_lon</th>\n",
       "      <th>do_lat</th>\n",
       "      <th>do_lon</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20727</td>\n",
       "      <td>20727</td>\n",
       "      <td>20727.000000</td>\n",
       "      <td>20727.000000</td>\n",
       "      <td>20727.000000</td>\n",
       "      <td>20727.000000</td>\n",
       "      <td>20727.000000</td>\n",
       "      <td>20727.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-04-30 03:11:05.817195008</td>\n",
       "      <td>2022-04-30 03:27:08.452115712</td>\n",
       "      <td>3.271662</td>\n",
       "      <td>40.753350</td>\n",
       "      <td>-73.966955</td>\n",
       "      <td>40.755752</td>\n",
       "      <td>-73.970757</td>\n",
       "      <td>22.482544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-01-01 00:11:06</td>\n",
       "      <td>2020-01-01 00:30:50</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>40.576961</td>\n",
       "      <td>-74.029893</td>\n",
       "      <td>40.576961</td>\n",
       "      <td>-74.174002</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2021-02-27 15:22:24</td>\n",
       "      <td>2021-02-27 15:32:43.500000</td>\n",
       "      <td>1.090000</td>\n",
       "      <td>40.740337</td>\n",
       "      <td>-73.989845</td>\n",
       "      <td>40.740337</td>\n",
       "      <td>-73.989845</td>\n",
       "      <td>12.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-04-29 09:06:52</td>\n",
       "      <td>2022-04-29 09:22:22</td>\n",
       "      <td>1.810000</td>\n",
       "      <td>40.758028</td>\n",
       "      <td>-73.977698</td>\n",
       "      <td>40.758028</td>\n",
       "      <td>-73.977698</td>\n",
       "      <td>17.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-06-28 11:02:35</td>\n",
       "      <td>2023-06-28 11:17:36</td>\n",
       "      <td>3.310000</td>\n",
       "      <td>40.773633</td>\n",
       "      <td>-73.961764</td>\n",
       "      <td>40.775932</td>\n",
       "      <td>-73.959635</td>\n",
       "      <td>24.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-08-31 22:43:47</td>\n",
       "      <td>2024-08-31 23:26:23</td>\n",
       "      <td>67.900000</td>\n",
       "      <td>40.899528</td>\n",
       "      <td>-73.739337</td>\n",
       "      <td>40.899528</td>\n",
       "      <td>-73.726655</td>\n",
       "      <td>262.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.110867</td>\n",
       "      <td>0.032385</td>\n",
       "      <td>0.045035</td>\n",
       "      <td>0.033130</td>\n",
       "      <td>0.036128</td>\n",
       "      <td>17.359922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     pickup_datetime               dropoff_datetime  \\\n",
       "count                          20727                          20727   \n",
       "mean   2022-04-30 03:11:05.817195008  2022-04-30 03:27:08.452115712   \n",
       "min              2020-01-01 00:11:06            2020-01-01 00:30:50   \n",
       "25%              2021-02-27 15:22:24     2021-02-27 15:32:43.500000   \n",
       "50%              2022-04-29 09:06:52            2022-04-29 09:22:22   \n",
       "75%              2023-06-28 11:02:35            2023-06-28 11:17:36   \n",
       "max              2024-08-31 22:43:47            2024-08-31 23:26:23   \n",
       "std                              NaN                            NaN   \n",
       "\n",
       "         trip_miles        pu_lat        pu_lon        do_lat        do_lon  \\\n",
       "count  20727.000000  20727.000000  20727.000000  20727.000000  20727.000000   \n",
       "mean       3.271662     40.753350    -73.966955     40.755752    -73.970757   \n",
       "min        0.010000     40.576961    -74.029893     40.576961    -74.174002   \n",
       "25%        1.090000     40.740337    -73.989845     40.740337    -73.989845   \n",
       "50%        1.810000     40.758028    -73.977698     40.758028    -73.977698   \n",
       "75%        3.310000     40.773633    -73.961764     40.775932    -73.959635   \n",
       "max       67.900000     40.899528    -73.739337     40.899528    -73.726655   \n",
       "std        4.110867      0.032385      0.045035      0.033130      0.036128   \n",
       "\n",
       "       total_amount  \n",
       "count  20727.000000  \n",
       "mean      22.482544  \n",
       "min        1.000000  \n",
       "25%       12.800000  \n",
       "50%       17.020000  \n",
       "75%       24.500000  \n",
       "max      262.700000  \n",
       "std       17.359922  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3ec0e5e-387e-499d-9ba5-a80b85697d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"taxi_data_cleaned.csv\"\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "taxi_data.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07574983-f41d-4cd6-8f70-489493089b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_uber_month(parquet_url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download, process, and sample an Uber HVHF dataset for a given month.\n",
    "\n",
    "    Args:\n",
    "        parquet_url (str): URL of the Uber HVHF Parquet file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A sampled and processed DataFrame.\n",
    "    \"\"\"\n",
    "    # Define the directory for processed Uber HVHF data\n",
    "    save_dir: str = \"processed_data/hvhf\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Extract file name and define local path\n",
    "    file_name: str = parquet_url.split(\"/\")[-1]\n",
    "    local_file_path: str = os.path.join(save_dir, file_name)\n",
    "\n",
    "    # Download the file\n",
    "    if not os.path.exists(local_file_path):\n",
    "        try:\n",
    "            response = requests.get(parquet_url, stream=True)\n",
    "            response.raise_for_status()\n",
    "            with open(local_file_path, \"wb\") as f:\n",
    "                for chunk in response.iter_content(chunk_size=1024 * 1024):  # 1MB chunks\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            raise Exception(f\"Failed to download the file from {parquet_url}. Error: {e}\")\n",
    "    data: pd.DataFrame = pd.read_parquet(local_file_path)\n",
    "\n",
    "    # Sample the dataset\n",
    "    population: int = len(data)\n",
    "    sample_size: int = calculate_sample_size(population, p=0.4)\n",
    "    sampled_data: pd.DataFrame = data.sample(n=sample_size, random_state=42) if population > sample_size else data\n",
    "    processed_file_path: str = os.path.join(save_dir, f\"sampled_{file_name}\")\n",
    "    if not os.path.exists(processed_file_path):\n",
    "        sampled_data.to_parquet(processed_file_path)\n",
    "\n",
    "    return sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b3d85ff-313c-41a2-9a46-261a9a2bb10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_uber_data(parquet_urls: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download, process, and combine Uber HVHF data from multiple Parquet file URLs.\n",
    "\n",
    "    Args:\n",
    "        parquet_urls (List[str]): A list of URLs pointing to Uber HVHF Parquet files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A combined DataFrame containing processed data from all valid Parquet files.\n",
    "    \"\"\"\n",
    "    all_uber_dataframes: List[pd.DataFrame] = []\n",
    "\n",
    "    # Regex pattern to filter Uber HVHF Parquet files for specific dates (2020-2024)\n",
    "    hvfhv_pattern: re.Pattern = re.compile(\n",
    "        r\"fhvhv_tripdata_(2020-(0[1-9]|1[0-2])|202[1-3]-(0[1-9]|1[0-2])|2024-(0[1-8]))\\.parquet\"\n",
    "    )\n",
    "    hvfhv_urls: List[str] = [url for url in parquet_urls if hvfhv_pattern.search(url)]\n",
    "\n",
    "    for url in hvfhv_urls:\n",
    "        save_dir: str = \"processed_data/hvhf\"\n",
    "        file_name: str = f\"sampled_{url.split('/')[-1]}\"\n",
    "        processed_file_path: str = os.path.join(save_dir, file_name)\n",
    "\n",
    "        if os.path.exists(processed_file_path):\n",
    "            dataframe: pd.DataFrame = pd.read_parquet(processed_file_path)\n",
    "        else:\n",
    "            dataframe: pd.DataFrame = get_and_clean_uber_month(url)\n",
    "\n",
    "        all_uber_dataframes.append(dataframe)\n",
    "    uber_data: pd.DataFrame = pd.concat(all_uber_dataframes, ignore_index=True)\n",
    "    return uber_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(uber_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean and process Uber HVHF data, including filtering by license, adding coordinates,\n",
    "    retaining required columns, and normalizing data.\n",
    "\n",
    "    Args:\n",
    "        uber_data (pd.DataFrame): The raw Uber HVHF data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned and filtered DataFrame with necessary columns and normalized values.\n",
    "    \"\"\"\n",
    "    # Filter by HVFH license number\n",
    "    uber_data['hvfhs_license_num'] = uber_data['hvfhs_license_num'].astype(str)\n",
    "    uber_data = uber_data[uber_data['hvfhs_license_num'] == 'HV0003'].copy()\n",
    "\n",
    "    # Add coordinates for pickup and dropoff locations\n",
    "    uber_data[\"PU_coords\"] = uber_data[\"PULocationID\"].apply(lambda x: lookup_coords_for_taxi_zone_id(x, taxi_zones))\n",
    "    uber_data[\"DO_coords\"] = uber_data[\"DOLocationID\"].apply(lambda x: lookup_coords_for_taxi_zone_id(x, taxi_zones))\n",
    "    uber_data = uber_data.dropna(subset=[\"PU_coords\", \"DO_coords\"]).reset_index(drop=True)\n",
    "    uber_data[[\"PU_lat\", \"PU_lon\"]] = pd.DataFrame(uber_data[\"PU_coords\"].tolist(), index=uber_data.index)\n",
    "    uber_data[[\"DO_lat\", \"DO_lon\"]] = pd.DataFrame(uber_data[\"DO_coords\"].tolist(), index=uber_data.index)\n",
    "    uber_data = uber_data.drop(columns=[\"PU_coords\", \"DO_coords\"])\n",
    "\n",
    "    # Retain required columns\n",
    "    required_columns = [\n",
    "        'pickup_datetime', 'dropoff_datetime', 'PU_lat', 'PU_lon', \n",
    "        'DO_lat', 'DO_lon', 'trip_miles', 'base_passenger_fare', \n",
    "        'tolls', 'bcf', 'sales_tax', 'congestion_surcharge', \n",
    "        'tips', 'airport_fee'\n",
    "    ]\n",
    "    uber_data = uber_data[required_columns]\n",
    "\n",
    "    # Remove invalid data points\n",
    "    uber_data = uber_data[uber_data[\"trip_miles\"] > 0]\n",
    "    uber_data = uber_data[uber_data[\"base_passenger_fare\"] > 0]\n",
    "    uber_data = uber_data[uber_data[\"pickup_datetime\"] < uber_data[\"dropoff_datetime\"]]\n",
    "\n",
    "    # Normalize column names\n",
    "    uber_data.columns = [col.lower() for col in uber_data.columns]\n",
    "\n",
    "    # Ensure numeric columns are float or int\n",
    "    numeric_columns = [\n",
    "        'pu_lat', 'pu_lon', 'do_lat', 'do_lon', 'trip_miles', \n",
    "        'base_passenger_fare', 'tolls', 'bcf', 'sales_tax', \n",
    "        'congestion_surcharge', 'tips', 'airport_fee'\n",
    "    ]\n",
    "    uber_data[numeric_columns] = uber_data[numeric_columns].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    uber_data[numeric_columns] = uber_data[numeric_columns].fillna(0)\n",
    "    uber_data = uber_data.dropna(subset=[\"pickup_datetime\", \"dropoff_datetime\"])\n",
    "\n",
    "    # Calculate the total amount\n",
    "    uber_data['total_amount'] = (\n",
    "        uber_data['base_passenger_fare'] + uber_data['tolls'] + \n",
    "        uber_data['bcf'] + uber_data['sales_tax'] + \n",
    "        uber_data['congestion_surcharge'] + uber_data['airport_fee'] + uber_data['tips']\n",
    "    )\n",
    "\n",
    "    # Retain only the updated columns\n",
    "    updated_columns = [\n",
    "        'pickup_datetime', 'dropoff_datetime', 'pu_lat', \n",
    "        'pu_lon', 'do_lat', 'do_lon', 'trip_miles', 'total_amount'\n",
    "    ]\n",
    "    uber_data = uber_data[updated_columns]\n",
    "\n",
    "    # Filter trips within the latitude/longitude bounding box\n",
    "    lat_min, lon_min = 40.560445, -74.242330\n",
    "    lat_max, lon_max = 40.908524, -73.717047\n",
    "    uber_data = uber_data[\n",
    "        (uber_data[\"pu_lat\"] >= lat_min) & (uber_data[\"pu_lat\"] <= lat_max) &\n",
    "        (uber_data[\"pu_lon\"] >= lon_min) & (uber_data[\"pu_lon\"] <= lon_max) &\n",
    "        (uber_data[\"do_lat\"] >= lat_min) & (uber_data[\"do_lat\"] <= lat_max) &\n",
    "        (uber_data[\"do_lon\"] >= lon_min) & (uber_data[\"do_lon\"] <= lon_max)\n",
    "    ]\n",
    "\n",
    "    # Reset index after filtering\n",
    "    uber_data = uber_data.reset_index(drop=True)\n",
    "\n",
    "    return uber_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve, filter, process, and clean Uber HVHF data from the TLC data page.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Fetch all URLs from the TLC webpage.\n",
    "    2. Filter the URLs to retain only those pointing to Parquet files.\n",
    "    3. Download, process, and combine data from these Parquet files.\n",
    "    4. Perform additional cleaning and normalization on the combined data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned and processed DataFrame containing Uber HVHF trip data.\n",
    "    \"\"\"\n",
    "    all_urls: list[str] = get_all_urls_from_page(TLC_URL)\n",
    "    all_parquet_urls: list[str] = filter_parquet_urls(all_urls)\n",
    "    uber_data: pd.DataFrame = get_and_clean_uber_data(all_parquet_urls)\n",
    "    uber_data = load_and_clean_uber_data(uber_data)\n",
    "    return uber_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c2bd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "339997e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>pu_lat</th>\n",
       "      <th>pu_lon</th>\n",
       "      <th>do_lat</th>\n",
       "      <th>do_lon</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-26 08:07:17</td>\n",
       "      <td>2024-01-26 08:35:38</td>\n",
       "      <td>40.646116</td>\n",
       "      <td>-73.951623</td>\n",
       "      <td>40.666559</td>\n",
       "      <td>-73.895364</td>\n",
       "      <td>4.29</td>\n",
       "      <td>30.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-19 02:17:05</td>\n",
       "      <td>2024-01-19 02:29:12</td>\n",
       "      <td>40.882403</td>\n",
       "      <td>-73.910665</td>\n",
       "      <td>40.857108</td>\n",
       "      <td>-73.932832</td>\n",
       "      <td>2.55</td>\n",
       "      <td>16.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-21 01:44:00</td>\n",
       "      <td>2024-01-21 02:08:30</td>\n",
       "      <td>40.748575</td>\n",
       "      <td>-73.985156</td>\n",
       "      <td>40.715370</td>\n",
       "      <td>-73.936794</td>\n",
       "      <td>6.37</td>\n",
       "      <td>33.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-20 12:58:40</td>\n",
       "      <td>2024-01-20 13:15:42</td>\n",
       "      <td>40.758028</td>\n",
       "      <td>-73.977698</td>\n",
       "      <td>40.753309</td>\n",
       "      <td>-74.004016</td>\n",
       "      <td>1.99</td>\n",
       "      <td>23.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-02 08:40:48</td>\n",
       "      <td>2024-01-02 08:54:28</td>\n",
       "      <td>40.666559</td>\n",
       "      <td>-73.895364</td>\n",
       "      <td>40.676644</td>\n",
       "      <td>-73.913632</td>\n",
       "      <td>2.23</td>\n",
       "      <td>17.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pickup_datetime    dropoff_datetime     pu_lat     pu_lon     do_lat  \\\n",
       "0 2024-01-26 08:07:17 2024-01-26 08:35:38  40.646116 -73.951623  40.666559   \n",
       "1 2024-01-19 02:17:05 2024-01-19 02:29:12  40.882403 -73.910665  40.857108   \n",
       "2 2024-01-21 01:44:00 2024-01-21 02:08:30  40.748575 -73.985156  40.715370   \n",
       "3 2024-01-20 12:58:40 2024-01-20 13:15:42  40.758028 -73.977698  40.753309   \n",
       "4 2024-01-02 08:40:48 2024-01-02 08:54:28  40.666559 -73.895364  40.676644   \n",
       "\n",
       "      do_lon  trip_miles  total_amount  \n",
       "0 -73.895364        4.29         30.69  \n",
       "1 -73.932832        2.55         16.90  \n",
       "2 -73.936794        6.37         33.19  \n",
       "3 -74.004016        1.99         23.91  \n",
       "4 -73.913632        2.23         17.95  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "944ae7fb-e7e0-4ff8-b956-993854dcc826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14383 entries, 0 to 14382\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   pickup_datetime   14383 non-null  datetime64[ns]\n",
      " 1   dropoff_datetime  14383 non-null  datetime64[ns]\n",
      " 2   pu_lat            14383 non-null  float64       \n",
      " 3   pu_lon            14383 non-null  float64       \n",
      " 4   do_lat            14383 non-null  float64       \n",
      " 5   do_lon            14383 non-null  float64       \n",
      " 6   trip_miles        14383 non-null  float64       \n",
      " 7   total_amount      14383 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(6)\n",
      "memory usage: 899.1 KB\n"
     ]
    }
   ],
   "source": [
    "uber_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6fddeb14-cd70-4e83-8f93-974642c3bea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>pu_lat</th>\n",
       "      <th>pu_lon</th>\n",
       "      <th>do_lat</th>\n",
       "      <th>do_lon</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14383</td>\n",
       "      <td>14383</td>\n",
       "      <td>14383.000000</td>\n",
       "      <td>14383.000000</td>\n",
       "      <td>14383.000000</td>\n",
       "      <td>14383.000000</td>\n",
       "      <td>14383.000000</td>\n",
       "      <td>14383.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-05-03 15:37:43.487311360</td>\n",
       "      <td>2022-05-03 15:55:31.077104896</td>\n",
       "      <td>40.737349</td>\n",
       "      <td>-73.935103</td>\n",
       "      <td>40.737331</td>\n",
       "      <td>-73.935269</td>\n",
       "      <td>4.372007</td>\n",
       "      <td>26.172938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-01-01 03:27:51</td>\n",
       "      <td>2020-01-01 03:30:21</td>\n",
       "      <td>40.561994</td>\n",
       "      <td>-74.170885</td>\n",
       "      <td>40.561994</td>\n",
       "      <td>-74.174002</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2021-02-27 15:08:56</td>\n",
       "      <td>2021-02-27 15:21:33</td>\n",
       "      <td>40.691201</td>\n",
       "      <td>-73.984197</td>\n",
       "      <td>40.691201</td>\n",
       "      <td>-73.984197</td>\n",
       "      <td>1.550000</td>\n",
       "      <td>12.855000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-05-01 06:12:14</td>\n",
       "      <td>2022-05-01 06:17:35</td>\n",
       "      <td>40.736824</td>\n",
       "      <td>-73.948788</td>\n",
       "      <td>40.737698</td>\n",
       "      <td>-73.948136</td>\n",
       "      <td>2.830000</td>\n",
       "      <td>20.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-07-02 21:51:33.500000</td>\n",
       "      <td>2023-07-02 22:17:46.500000</td>\n",
       "      <td>40.774376</td>\n",
       "      <td>-73.900317</td>\n",
       "      <td>40.774376</td>\n",
       "      <td>-73.899735</td>\n",
       "      <td>5.530000</td>\n",
       "      <td>32.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-08-31 21:37:05</td>\n",
       "      <td>2024-08-31 21:57:29</td>\n",
       "      <td>40.899528</td>\n",
       "      <td>-73.726655</td>\n",
       "      <td>40.899528</td>\n",
       "      <td>-73.726655</td>\n",
       "      <td>33.170000</td>\n",
       "      <td>239.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.068382</td>\n",
       "      <td>0.064742</td>\n",
       "      <td>0.068795</td>\n",
       "      <td>0.068168</td>\n",
       "      <td>4.263149</td>\n",
       "      <td>19.989583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     pickup_datetime               dropoff_datetime  \\\n",
       "count                          14383                          14383   \n",
       "mean   2022-05-03 15:37:43.487311360  2022-05-03 15:55:31.077104896   \n",
       "min              2020-01-01 03:27:51            2020-01-01 03:30:21   \n",
       "25%              2021-02-27 15:08:56            2021-02-27 15:21:33   \n",
       "50%              2022-05-01 06:12:14            2022-05-01 06:17:35   \n",
       "75%       2023-07-02 21:51:33.500000     2023-07-02 22:17:46.500000   \n",
       "max              2024-08-31 21:37:05            2024-08-31 21:57:29   \n",
       "std                              NaN                            NaN   \n",
       "\n",
       "             pu_lat        pu_lon        do_lat        do_lon    trip_miles  \\\n",
       "count  14383.000000  14383.000000  14383.000000  14383.000000  14383.000000   \n",
       "mean      40.737349    -73.935103     40.737331    -73.935269      4.372007   \n",
       "min       40.561994    -74.170885     40.561994    -74.174002      0.010000   \n",
       "25%       40.691201    -73.984197     40.691201    -73.984197      1.550000   \n",
       "50%       40.736824    -73.948788     40.737698    -73.948136      2.830000   \n",
       "75%       40.774376    -73.900317     40.774376    -73.899735      5.530000   \n",
       "max       40.899528    -73.726655     40.899528    -73.726655     33.170000   \n",
       "std        0.068382      0.064742      0.068795      0.068168      4.263149   \n",
       "\n",
       "       total_amount  \n",
       "count  14383.000000  \n",
       "mean      26.172938  \n",
       "min        0.680000  \n",
       "25%       12.855000  \n",
       "50%       20.050000  \n",
       "75%       32.370000  \n",
       "max      239.830000  \n",
       "std       19.989583  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33bf45d4-dbc2-4928-b45b-fe29e4d20683",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"uber_data_cleaned.csv\"\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "uber_data.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eece1cb5-c613-43a4-8e71-ae9ba348c624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieve all CSV file paths from a specified directory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing CSV files.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of file paths to all CSV files in the directory.\n",
    "    \"\"\"\n",
    "    csv_files: List[str] = [\n",
    "        os.path.join(directory, file) for file in os.listdir(directory) if file.endswith(\".csv\")\n",
    "    ]\n",
    "    return csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean and process hourly weather data from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file containing weather data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing cleaned and processed hourly weather data.\n",
    "                      Columns include 'date', 'precipitation', and 'wind_speed'.\n",
    "                      Returns an empty DataFrame if the processing fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df: pd.DataFrame = pd.read_csv(csv_file, low_memory=False)\n",
    "        df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "\n",
    "        # Convert 'DATE' column to datetime\n",
    "        df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], errors=\"coerce\")\n",
    "\n",
    "        # Select and clean relevant columns\n",
    "        hourly_data: pd.DataFrame = df[[\"DATE\", \"HourlyPrecipitation\", \"HourlyWindSpeed\"]].copy()\n",
    "        hourly_data.replace(\"T\", 0, inplace=True)\n",
    "        hourly_data.replace(regex=r\"[^\\d.]\", value=np.nan, inplace=True)\n",
    "\n",
    "        # Rename columns for consistency\n",
    "        hourly_data.columns = [\"date\", \"precipitation\", \"wind_speed\"]\n",
    "\n",
    "        # Convert columns to numeric\n",
    "        hourly_data[\"precipitation\"] = pd.to_numeric(hourly_data[\"precipitation\"], errors=\"coerce\")\n",
    "        hourly_data[\"wind_speed\"] = pd.to_numeric(hourly_data[\"wind_speed\"], errors=\"coerce\")\n",
    "\n",
    "        return hourly_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {csv_file}: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean and process daily weather data from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file containing weather data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing cleaned and processed daily weather data.\n",
    "                      Columns include 'date', 'precipitation', 'average_wind_speed', and 'snowfall'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df: pd.DataFrame = pd.read_csv(csv_file, low_memory=False)\n",
    "        df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "\n",
    "        # Convert 'DATE' column to datetime\n",
    "        df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], errors=\"coerce\")\n",
    "\n",
    "        # Select and clean relevant columns\n",
    "        daily_data: pd.DataFrame = df[[\"DATE\", \"DailyPrecipitation\", \"DailyAverageWindSpeed\", \"DailySnowfall\"]].copy()\n",
    "        daily_data.replace(\"T\", 0, inplace=True)\n",
    "        daily_data.replace(regex=r\"[^\\d.]\", value=np.nan, inplace=True)\n",
    "\n",
    "        # Rename columns for consistency\n",
    "        daily_data.columns = [\"date\", \"precipitation\", \"average_wind_speed\", \"snowfall\"]\n",
    "\n",
    "        # Convert columns to numeric\n",
    "        daily_data[\"precipitation\"] = pd.to_numeric(daily_data[\"precipitation\"], errors=\"coerce\")\n",
    "        daily_data[\"average_wind_speed\"] = pd.to_numeric(daily_data[\"average_wind_speed\"], errors=\"coerce\")\n",
    "        daily_data[\"snowfall\"] = pd.to_numeric(daily_data[\"snowfall\"], errors=\"coerce\")\n",
    "\n",
    "        # Drop rows for NaN\n",
    "        daily_data = daily_data.dropna(subset=[\"precipitation\", \"average_wind_speed\", \"snowfall\"], how=\"all\")\n",
    "\n",
    "        # Convert 'date' column to date\n",
    "        daily_data[\"date\"] = daily_data[\"date\"].dt.date\n",
    "\n",
    "        return daily_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {csv_file}: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data(directory: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load and clean weather data from all CSV files in the specified directory.\n",
    "\n",
    "    This function processes each file to extract and clean both hourly and daily weather data.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing CSV files.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Two DataFrames:\n",
    "            - hourly_data (pd.DataFrame): Combined cleaned hourly weather data.\n",
    "            - daily_data (pd.DataFrame): Combined cleaned daily weather data.\n",
    "    \"\"\"\n",
    "    weather_csv_files = get_all_weather_csvs(directory)\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "\n",
    "        # Append cleaned data to the respective lists\n",
    "        if not hourly_dataframe.empty:\n",
    "            hourly_dataframes.append(hourly_dataframe)\n",
    "        if not daily_dataframe.empty:\n",
    "            daily_dataframes.append(daily_dataframe)\n",
    "\n",
    "    hourly_data = pd.concat(hourly_dataframes, ignore_index=True) if hourly_dataframes else pd.DataFrame()\n",
    "    daily_data = pd.concat(daily_dataframes, ignore_index=True) if daily_dataframes else pd.DataFrame()\n",
    "    \n",
    "    # Replace NaN values with 0\n",
    "    hourly_data.fillna(0, inplace=True)\n",
    "    daily_data.fillna(0, inplace=True)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dcf3c2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    WEATHER_CSV_DIR = \"weather/\"  # Path to your weather data directory\n",
    "    hourly_weather_data, daily_weather_data = load_and_clean_weather_data(WEATHER_CSV_DIR)\n",
    "\n",
    "    hourly_weather_data.to_csv(\"cleaned_hourly_weather_data.csv\", index=False)\n",
    "    daily_weather_data.to_csv(\"cleaned_daily_weather_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48216557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 00:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01 01:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01 02:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01 03:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01 04:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date  precipitation  wind_speed\n",
       "0 2020-01-01 00:51:00            0.0         8.0\n",
       "1 2020-01-01 01:51:00            0.0         8.0\n",
       "2 2020-01-01 02:51:00            0.0        14.0\n",
       "3 2020-01-01 03:51:00            0.0        11.0\n",
       "4 2020-01-01 04:51:00            0.0         6.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "935261b7-ae23-427c-97ff-ea31aa4e44c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 56098 entries, 0 to 56097\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   date           56098 non-null  datetime64[ns]\n",
      " 1   precipitation  56098 non-null  float64       \n",
      " 2   wind_speed     56098 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(2)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "hourly_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7dcb502-d1d1-447d-aa68-11bff0dc53b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>56098</td>\n",
       "      <td>56098.000000</td>\n",
       "      <td>56098.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-05-29 21:14:19.618881024</td>\n",
       "      <td>0.010288</td>\n",
       "      <td>4.537238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-01-01 00:51:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2021-03-18 19:01:45</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-05-28 01:21:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-08-15 05:39:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-10-22 18:51:00</td>\n",
       "      <td>3.470000</td>\n",
       "      <td>2237.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.056033</td>\n",
       "      <td>13.883208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                date  precipitation    wind_speed\n",
       "count                          56098   56098.000000  56098.000000\n",
       "mean   2022-05-29 21:14:19.618881024       0.010288      4.537238\n",
       "min              2020-01-01 00:51:00       0.000000      0.000000\n",
       "25%              2021-03-18 19:01:45       0.000000      0.000000\n",
       "50%              2022-05-28 01:21:00       0.000000      5.000000\n",
       "75%              2023-08-15 05:39:00       0.000000      7.000000\n",
       "max              2024-10-22 18:51:00       3.470000   2237.000000\n",
       "std                              NaN       0.056033     13.883208"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_weather_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>average_wind_speed</th>\n",
       "      <th>snowfall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>0.27</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  precipitation  average_wind_speed  snowfall\n",
       "0  2020-01-01           0.00                 8.6       0.0\n",
       "1  2020-01-02           0.00                 5.4       0.0\n",
       "2  2020-01-03           0.15                 3.4       0.0\n",
       "3  2020-01-04           0.27                 4.4       0.0\n",
       "4  2020-01-05           0.00                11.3       0.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f090eb94-a5b0-4d93-bf82-a596d2521b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1755 entries, 0 to 1754\n",
      "Data columns (total 4 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   date                1755 non-null   object \n",
      " 1   precipitation       1755 non-null   float64\n",
      " 2   average_wind_speed  1755 non-null   float64\n",
      " 3   snowfall            1755 non-null   float64\n",
      "dtypes: float64(3), object(1)\n",
      "memory usage: 55.0+ KB\n"
     ]
    }
   ],
   "source": [
    "daily_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8c074aa3-a5f2-4586-8748-411e1e6c11da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precipitation</th>\n",
       "      <th>average_wind_speed</th>\n",
       "      <th>snowfall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1755.000000</td>\n",
       "      <td>1755.000000</td>\n",
       "      <td>1755.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.141966</td>\n",
       "      <td>4.835499</td>\n",
       "      <td>0.039088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.414574</td>\n",
       "      <td>2.467952</td>\n",
       "      <td>0.493457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.060000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.130000</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>14.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       precipitation  average_wind_speed     snowfall\n",
       "count    1755.000000         1755.000000  1755.000000\n",
       "mean        0.141966            4.835499     0.039088\n",
       "std         0.414574            2.467952     0.493457\n",
       "min         0.000000            0.000000     0.000000\n",
       "25%         0.000000            3.000000     0.000000\n",
       "50%         0.000000            4.500000     0.000000\n",
       "75%         0.060000            6.200000     0.000000\n",
       "max         7.130000           14.200000    14.800000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2df280ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned datasets\n",
    "daily_data = pd.read_csv(\"cleaned_daily_weather_data.csv\")\n",
    "hourly_data = pd.read_csv(\"cleaned_hourly_weather_data.csv\")\n",
    "taxi_data = pd.read_csv(\"taxi_data_cleaned.csv\")\n",
    "uber_data = pd.read_csv(\"uber_data_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather (\n",
    "    datetime TEXT NOT NULL,\n",
    "    temperature FLOAT,\n",
    "    precipitation FLOAT,\n",
    "    wind_speed FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather (\n",
    "    date TEXT NOT NULL,\n",
    "    avg_precipitation FLOAT,\n",
    "    avg_wind_speed FLOAT,\n",
    "    total_snowfall FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips (\n",
    "    pickup_datetime TEXT NOT NULL,\n",
    "    dropoff_datetime TEXT NOT NULL,\n",
    "    pu_lat FLOAT,\n",
    "    pu_lon FLOAT,\n",
    "    do_lat FLOAT,\n",
    "    do_lon FLOAT,\n",
    "    trip_miles FLOAT,\n",
    "    total_amount FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips (\n",
    "    pickup_datetime TEXT NOT NULL,\n",
    "    dropoff_datetime TEXT NOT NULL,\n",
    "    pu_lat FLOAT,\n",
    "    pu_lon FLOAT,\n",
    "    do_lat FLOAT,\n",
    "    do_lon FLOAT,\n",
    "    trip_miles FLOAT,\n",
    "    total_amount FLOAT\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    with open(DATABASE_SCHEMA_FILE, \"r\") as schema_file:\n",
    "        schema_script = schema_file.read()\n",
    "        statements = schema_script.split(\";\")\n",
    "        for statement in statements:\n",
    "            statement = statement.strip()\n",
    "            if statement:\n",
    "                connection.execute(db.text(statement))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    \"\"\"\n",
    "    Writes dataframes to the corresponding SQL tables.\n",
    "    Args:\n",
    "        table_to_df_dict (dict): A dictionary where keys are table names\n",
    "                                 and values are the respective DataFrames.\n",
    "    \"\"\"\n",
    "    for table_name, df in table_to_df_dict.items():\n",
    "        df.to_sql(table_name, con=engine, if_exists='replace', index=False)\n",
    "\n",
    "# Map table names to cleaned DataFrames\n",
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}\n",
    "\n",
    "# Write data to the database\n",
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query either via sqlalchemy\n",
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_1)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "pd.read_sql(QUERY_1, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
