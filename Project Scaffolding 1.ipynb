{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "854d1cd4-4133-4b31-a288-b51de769f9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "import os\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import fiona\n",
    "import math\n",
    "import pytest\n",
    "from shapely.geometry import Point\n",
    "from typing import Optional, Tuple, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06485b0b-7046-4fe7-a507-0387960d8766",
   "metadata": {},
   "source": [
    "To accomplish our project, we use several libraries to handle tasks such as data processing, visualization, database interaction, and geospatial analysis. Here’s a brief overview:\n",
    "\n",
    "- **File and Directory Management**:\n",
    "  - `os`: Helps manage directories and file paths.\n",
    "\n",
    "- **Web Scraping**:\n",
    "  - `bs4` and `BeautifulSoup`: Extract data from web pages.\n",
    "\n",
    "- **Data Handling**:\n",
    "  - `pandas`: Works with tabular datasets, like spreadsheets.\n",
    "  - `geopandas`: Adds geospatial capabilities to pandas for working with maps and location data.\n",
    "\n",
    "- **Database Interaction**:\n",
    "  - `sqlalchemy`: Connects our data to a SQLite database for storage and querying.\n",
    "\n",
    "- **Math and Statistics**:\n",
    "  - `math` and `numpy`: Perform calculations and handle arrays of data.\n",
    "\n",
    "- **Date and Time**:\n",
    "  - `datetime`: Manage dates and times for our analysis.\n",
    "\n",
    "- **Regular Expressions**:\n",
    "  - `re`: Helps search for patterns in text, like extracting URLs.\n",
    "\n",
    "- **Testing**:\n",
    "  - `pytest`: Ensures our functions work as expected through automated testing.\n",
    "\n",
    "- **Visualization**:\n",
    "  - `matplotlib.pyplot`: Creates graphs and charts to visualize trends and patterns.\n",
    "\n",
    "- **Geospatial Data**:\n",
    "  - `fiona`, `shapely`, and `GeoDataFrame`: Handle maps, locations, and geographic shapes.\n",
    "\n",
    "This mix of tools enables us to handle various aspects of the project, from downloading and cleaning data to storing it in a database and creating visualizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TLC_URL = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "WEATHER_CSV_DIR = \"\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98107997-851e-4d82-967e-c15a5a52974a",
   "metadata": {},
   "source": [
    "## Defining Project Constants\n",
    "\n",
    "Constants are fixed values that we use throughout the project. By defining them in one place, it’s easier to maintain consistency and make changes if needed. Below is a simplified explanation of the constants used in the project:\n",
    "\n",
    "### Data Source URLs\n",
    "- **`TLC_URL`**:  \n",
    "  The website link where we download ride data for New York City, including taxi and Uber trips.\n",
    "\n",
    "### Taxi Zones Information\n",
    "- **`TAXI_ZONES_DIR`**:  \n",
    "  The folder where information about NYC taxi zones is stored.\n",
    "- **`TAXI_ZONES_SHAPEFILE`**:  \n",
    "  A file that maps the boundaries of taxi zones in NYC. This is important for identifying pickup and drop-off locations geographically.\n",
    "\n",
    "### Weather Data Location\n",
    "- **`WEATHER_CSV_DIR`**:  \n",
    "  The folder where we keep cleaned weather data files.\n",
    "\n",
    "### Geographic Settings\n",
    "- **`CRS`**:  \n",
    "  This stands for \"Coordinate Reference System,\" a standard used to define locations on Earth (here, latitude and longitude).\n",
    "\n",
    "#### Bounding Boxes for Geographic Areas\n",
    "Bounding boxes are defined by latitude and longitude pairs and are used to filter rides or weather data based on specific locations:\n",
    "- **`NEW_YORK_BOX_COORDS`**:  \n",
    "  The boundaries of New York City. Used to filter rides within the city.\n",
    "- **`LGA_BOX_COORDS`**:  \n",
    "  The boundaries of LaGuardia Airport.\n",
    "- **`JFK_BOX_COORDS`**:  \n",
    "  The boundaries of John F. Kennedy International Airport.\n",
    "- **`EWR_BOX_COORDS`**:  \n",
    "  The boundaries of Newark Liberty International Airport.\n",
    "\n",
    "### Database Configuration\n",
    "- **`DATABASE_URL`**:  \n",
    "  The location of the SQLite database where all cleaned data will be stored.\n",
    "- **`DATABASE_SCHEMA_FILE`**:  \n",
    "  A file that defines the structure (or schema) of the database tables.\n",
    "- **`QUERY_DIRECTORY`**:  \n",
    "  A folder where SQL query files for analysis will be saved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93926239-6cbe-476d-8948-980dc49aeafc",
   "metadata": {},
   "source": [
    "## Ensuring the Existence of the Query Directory\n",
    "\n",
    "This part of the code ensures that the folder (directory) where we will save our SQL query files is created and ready to use. The directory is defined by the constant `QUERY_DIRECTORY`.\n",
    "\n",
    "### Why Is This Important?\n",
    "During the project, we will generate and save SQL query files. To do this, we need a specific folder to keep them organized. This code checks if the folder already exists, and if not, it creates it.\n",
    "\n",
    "### How It Works\n",
    "1. **Create the Folder**:  \n",
    "   The program attempts to create the folder specified by `QUERY_DIRECTORY`.\n",
    "\n",
    "2. **Handle Existing Folders**:  \n",
    "   - If the folder already exists, Python might raise an error. The code is designed to detect this specific situation and ignore the error, allowing the program to continue running without issues.\n",
    "   - If a different error occurs (unrelated to the folder already existing), the program will stop and notify the user.\n",
    "\n",
    "This process ensures the folder is always available without accidentally overwriting or creating duplicate folders.\n",
    "\n",
    "By doing this early in the program, we avoid problems later when saving query files, keeping everything organized and efficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "# Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "## Load Taxi Zones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e949f21b-f158-44e4-bbf2-88ee9480c02f",
   "metadata": {},
   "source": [
    "This section focuses on handling geographic data related to NYC taxi zones, which is essential for mapping and analyzing taxi and Uber rides.\n",
    "\n",
    "### 1. Loading Taxi Zone Information\n",
    "\n",
    "We use the **`load_taxi_zones`** function to load data about taxi zones from a geospatial file, such as a shapefile or GeoJSON. This data is stored in a **GeoDataFrame**, which allows us to perform geographic operations (e.g., finding locations or boundaries).\n",
    "\n",
    "#### Why Is This Important?\n",
    "- Taxi zones help us identify where rides start and end geographically.\n",
    "- This information is used throughout the project for mapping and spatial analysis.\n",
    "\n",
    "### 2. Getting the Coordinates for a Taxi Zone\n",
    "\n",
    "The **`lookup_coords_for_taxi_zone_id`** function helps find the central point (latitude and longitude) of a taxi zone using its **LocationID**.\n",
    "\n",
    "#### How It Works:\n",
    "- If the LocationID is valid, the function calculates the **centroid** (geographic center) of the taxi zone and returns the coordinates (latitude and longitude).\n",
    "- If the LocationID is not found, the function returns `None`, indicating the zone is invalid or missing.\n",
    "\n",
    "#### Why Is This Important?\n",
    "- Coordinates allow us to connect geographic zones with ride data, making it possible to analyze trips based on pickup and drop-off locations.\n",
    "- This is especially helpful for visualizing the data on maps or filtering rides by specific areas.\n",
    "\n",
    "These functions are crucial for linking spatial data with ride information, enabling us to analyze and visualize geographic trends effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58708809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones(file_path: str) -> GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Load taxi zones from a shapefile or GeoJSON file into a GeoDataFrame.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the geospatial file (e.g., a shapefile or GeoJSON).\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: A GeoDataFrame containing the taxi zones data.\n",
    "    \"\"\"\n",
    "    geofile: GeoDataFrame = gpd.read_file(file_path)\n",
    "    return geofile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "783ca81b-319c-4a64-91e6-b11f268e3819",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zones = load_taxi_zones(\"taxi_zones/taxi_zones.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d04c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_coords_for_taxi_zone_id(\n",
    "    zone_loc_id: int, \n",
    "    loaded_taxi_zones: GeoDataFrame\n",
    ") -> Optional[Tuple[float, float]]:\n",
    "    \"\"\"\n",
    "    Look up the latitude and longitude coordinates for a taxi zone by its LocationID.\n",
    "\n",
    "    Args:\n",
    "        zone_loc_id (int): The LocationID of the taxi zone.\n",
    "        loaded_taxi_zones (GeoDataFrame): A GeoDataFrame containing taxi zone geometries.\n",
    "\n",
    "    Returns:\n",
    "        Optional[Tuple[float, float]]: A tuple containing the latitude and longitude \n",
    "        coordinates if the zone is found, or None if the LocationID is not in the data.\n",
    "    \"\"\"\n",
    "    # Ensure the CRS is set if missing\n",
    "    if loaded_taxi_zones.crs is None:\n",
    "        loaded_taxi_zones = loaded_taxi_zones.set_crs(epsg=2263)\n",
    "\n",
    "    # Find the zone matching the LocationID\n",
    "    zone = loaded_taxi_zones[loaded_taxi_zones['LocationID'] == zone_loc_id]\n",
    "    if zone.empty:\n",
    "        return None\n",
    "    projected_zone = zone.to_crs(epsg=2263)\n",
    "    centroid = projected_zone.geometry.centroid.iloc[0]\n",
    "    centroid_geo = gpd.GeoSeries([centroid], crs=2263).to_crs(epsg=4326)\n",
    "\n",
    "    # Return latitude and longitude as a tuple\n",
    "    return (centroid_geo.geometry.iloc[0].y, centroid_geo.geometry.iloc[0].x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "## Calculate Sample Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1396cc7-fda0-4458-9cc5-357be5076b5e",
   "metadata": {},
   "source": [
    "This section explains how we determine the required sample size for analyzing large datasets. We use a statistical method called **Cochran's formula**, which is adjusted for finite population sizes.\n",
    "\n",
    "### Purpose\n",
    "When working with massive datasets, analyzing every single data point can be inefficient. Instead, we calculate a representative sample size to ensure accurate results while saving time and resources.\n",
    "\n",
    "### Function: `calculate_sample_size`\n",
    "\n",
    "This function calculates the sample size based on the population size and desired statistical confidence.\n",
    "\n",
    "#### Inputs:\n",
    "1. **`population`**: The total number of available data points.\n",
    "2. **`p`** (default: 0.5): Represents the proportion of the population with the characteristic of interest. A default of 0.5 is used for maximum variability, meaning we assume the most uncertain case.\n",
    "\n",
    "#### Process:\n",
    "1. **Confidence Level**: The function uses a Z-value of **1.96**, which corresponds to a **95% confidence level**.\n",
    "2. **Margin of Error**: Assumes a **5% margin of error** by default.\n",
    "3. **Cochran's Formula**: The initial sample size (`n_0`) is calculated using the formula:\n",
    "   $$\n",
    "   n_0 = \\frac{Z^2 \\cdot p \\cdot (1-p)}{(\\text{margin of error})^2}\n",
    "   $$\n",
    "4. **Adjustment for Finite Population**: The sample size is refined to account for finite population sizes:\n",
    "   $$\n",
    "   \\text{Adjusted Sample Size} = \\frac{n_0}{1 + \\frac{n_0 - 1}{\\text{population}}}\n",
    "   $$\n",
    "\n",
    "#### Output:\n",
    "- The function returns the **adjusted sample size**, rounded up to the nearest whole number.\n",
    "\n",
    "#### Why Is This Important?\n",
    "- Ensures the sample is large enough to represent the dataset accurately.\n",
    "- Reduces the computational load by avoiding the need to process the entire dataset.\n",
    "- Maintains statistical reliability for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sample_size(population: int, p: float = 0.5) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the required sample size using Cochran's formula, adjusted for finite populations.\n",
    "\n",
    "    Args:\n",
    "        population (int): The total population size.\n",
    "        p (float, optional): Proportion of the population with the desired characteristic. \n",
    "            Defaults to 0.5 for maximum variability.\n",
    "\n",
    "    Returns:\n",
    "        int: The calculated sample size, rounded up to the nearest whole number.\n",
    "    \"\"\"\n",
    "    # Z-value for 95% confidence level\n",
    "    z: float = 1.96\n",
    "    \n",
    "    # Margin of error (5% by default)\n",
    "    margin_of_error: float = 0.05\n",
    "    \n",
    "    # Complementary proportion\n",
    "    q: float = 1 - p\n",
    "    \n",
    "    # Cochran's formula\n",
    "    n_0: float = (z**2 * p * q) / (margin_of_error**2)\n",
    "    \n",
    "    # Adjust for finite population\n",
    "    sample_size: float = n_0 / (1 + (n_0 - 1) / population)\n",
    "    \n",
    "    return math.ceil(sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc33eed4-b2e9-4ab3-94a8-59f04e464c98",
   "metadata": {},
   "source": [
    "## Common Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c7fbd8-c5bc-4c48-a47d-5cb4fcd2f8f9",
   "metadata": {},
   "source": [
    "This section provides two utility functions that automate the process of retrieving and filtering dataset links from a webpage. These functions help streamline the downloading of required files for the project. (By calling function directly, we can speed up the progress without rewriting the code again and again)\n",
    "\n",
    "### 1. `get_all_urls_from_page`\n",
    "\n",
    "#### Purpose:\n",
    "- Extracts all web links (URLs) from a given webpage.\n",
    "\n",
    "#### Key Features:\n",
    "- **Fetching the Webpage**: Uses the `requests` library to download the HTML content of the page.\n",
    "- **Parsing Links**: Employs `BeautifulSoup` to find all anchor tags (`<a>`) with `href` attributes, which contain the links.\n",
    "- **Error Handling**: Ensures the function handles connectivity issues or invalid webpages gracefully, raising descriptive errors when necessary.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `filter_parquet_urls`\n",
    "\n",
    "#### Purpose:\n",
    "- Filters the list of extracted links to include only Parquet files.\n",
    "\n",
    "#### Key Features:\n",
    "- **URL Normalization**: Strips unnecessary whitespace from each link.\n",
    "- **Regular Expression Matching**: Uses a pattern to identify URLs that end with `.parquet` (e.g., `file.parquet` or `file.parquet?params`).\n",
    "- **Efficient Filtering**: Ensures only relevant file links are returned, saving time and effort during the data downloading process.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Are These Functions Important?\n",
    "\n",
    "- They automate the process of collecting and filtering dataset links, eliminating the need for manual URL extraction.\n",
    "- Ensures consistency and accuracy in identifying the correct file formats (e.g., Parquet files) needed for the project.\n",
    "- Saves significant time when dealing with large or frequently updated webpages containing multiple links.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_page(page_url: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Fetch all URLs from a given webpage.\n",
    "\n",
    "    Args:\n",
    "        page_url (str): URL of the webpage to scrape.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of all URLs found on the webpage.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an error accessing the webpage or scraping its content.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(page_url)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise Exception(f\"Failed to access the URL: {page_url}. Error: {e}\")\n",
    "    \n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    links = soup.find_all(\"a\", href=True)\n",
    "    all_urls: List[str] = [link[\"href\"] for link in links]\n",
    "    \n",
    "    return all_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_parquet_urls(links: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Filter a list of URLs to include only those pointing to Parquet files.\n",
    "\n",
    "    Args:\n",
    "        links (List[str]): A list of URLs to filter.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of URLs that point to Parquet files.\n",
    "    \"\"\"\n",
    "    parquet_urls: List[str] = []\n",
    "    for url in links:\n",
    "        # Normalize the URL\n",
    "        url = url.strip()\n",
    "        if re.search(r\"\\.parquet(\\?.*)?$\", url):\n",
    "            parquet_urls.append(url)\n",
    "    \n",
    "    return parquet_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "## Process Taxi Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4308313-a99e-443b-9841-3fa154281e96",
   "metadata": {},
   "source": [
    "This section processes Yellow Taxi data from the NYC Taxi & Limousine Commission (TLC) webpage after extracting all parquet urls from the website. The goal is to clean and prepare the data for analysis, ensuring it is accurate, consistent, and manageable.\n",
    "\n",
    "### Steps Involved\n",
    "\n",
    "#### 1. Download and Process Monthly Data\n",
    "##### Function: `get_and_clean_taxi_month`\n",
    "- Downloads a monthly Yellow Taxi data file in Parquet format.\n",
    "- Creates a sample dataset to make large datasets more manageable.\n",
    "- Saves the processed data locally for reuse.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Process Multiple Files\n",
    "##### Function: `get_and_clean_taxi_data`\n",
    "- Collects and processes multiple Yellow Taxi files from a list of URLs.\n",
    "- Combines all the monthly datasets into one comprehensive dataset.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Clean and Filter the Data\n",
    "##### Function: `clean_taxi_data`\n",
    "This function cleans the combined dataset to make it consistent and ready for analysis:\n",
    "- **Renames Columns**: Normalizes column names for consistency.\n",
    "- **Data Types**: Converts data into the correct formats (e.g., dates and numbers).\n",
    "- **Coordinates**: Adds latitude and longitude for pickup and drop-off zones.\n",
    "- **Invalid Data**: Removes trips with errors like zero distance or invalid times.\n",
    "- **NYC Bounding Box**: Filters trips to include only those within New York City's geographic boundaries.\n",
    "- **Relevant Columns**: Keeps only the essential columns needed for analysis.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Combine Everything\n",
    "##### Function: `get_taxi_data`\n",
    "- Combines the above steps to:\n",
    "  1. Fetch URLs for Yellow Taxi data files from the TLC webpage.\n",
    "  2. Filter URLs to keep only relevant files.\n",
    "  3. Download, process, and clean the taxi data.\n",
    "\n",
    "This process ensures the data is ready for analysis and visualization in later stages of the project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_month(parquet_url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download, process, and sample a Yellow Taxi dataset for a given month.\n",
    "\n",
    "    Args:\n",
    "        parquet_url (str): URL of the Yellow Taxi Parquet file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A sampled and processed DataFrame.\n",
    "    \"\"\"\n",
    "    # Default directory for Yellow Taxi data\n",
    "    save_dir = \"processed_data/yellow_taxi\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Extract file name and define local path\n",
    "    file_name = parquet_url.split(\"/\")[-1]\n",
    "    local_file_path = os.path.join(save_dir, file_name)\n",
    "\n",
    "    # Download the file if not exist\n",
    "    if not os.path.exists(local_file_path):\n",
    "        response = requests.get(parquet_url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(local_file_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024 * 1024):  # 1MB chunks\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "    # Load the dataset\n",
    "    data = pd.read_parquet(local_file_path)\n",
    "\n",
    "    # Calculate sample size (using p = 0.5 for Yellow Taxi data)\n",
    "    population = len(data)\n",
    "    sample_size = calculate_sample_size(population, p = 0.5)\n",
    "    sampled_data = data.sample(n=sample_size, random_state=42) if population > sample_size else data\n",
    "    \n",
    "    processed_file_path = os.path.join(save_dir, f\"sampled_{file_name}\")\n",
    "    sampled_data.to_parquet(processed_file_path)\n",
    "    return sampled_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(parquet_urls: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download, process, and combine Yellow Taxi data from multiple Parquet file URLs.\n",
    "\n",
    "    Args:\n",
    "        parquet_urls (List[str]): A list of URLs pointing to Yellow Taxi Parquet files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A combined DataFrame containing processed data from all valid Parquet files.\n",
    "    \"\"\"\n",
    "    all_taxi_dataframes: List[pd.DataFrame] = []\n",
    "\n",
    "    # Regex pattern for valid Yellow Taxi Parquet files (2020-2024 dates)\n",
    "    yellow_taxi_pattern: re.Pattern = re.compile(\n",
    "        r\"yellow_tripdata_(2020-(0[1-9]|1[0-2])|202[1-3]-(0[1-9]|1[0-2])|2024-(0[1-8]))\\.parquet\"\n",
    "    )\n",
    "\n",
    "    # Filter URLs matching the pattern\n",
    "    yellow_taxi_urls: List[str] = [url for url in parquet_urls if yellow_taxi_pattern.search(url)]\n",
    "\n",
    "    for url in yellow_taxi_urls:\n",
    "        save_dir: str = \"processed_data/yellow_taxi\"\n",
    "        file_name: str = f\"sampled_{url.split('/')[-1]}\"\n",
    "        processed_file_path: str = os.path.join(save_dir, file_name)\n",
    "\n",
    "        if os.path.exists(processed_file_path):\n",
    "            dataframe: pd.DataFrame = pd.read_parquet(processed_file_path)\n",
    "        else:\n",
    "            dataframe: pd.DataFrame = get_and_clean_taxi_month(url)\n",
    "\n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "\n",
    "    # Combine all DataFrames into one\n",
    "    taxi_data: pd.DataFrame = pd.concat(all_taxi_dataframes, ignore_index=True)\n",
    "\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201f2cde-7550-4079-975b-ad1763f3b554",
   "metadata": {},
   "source": [
    "### Note for column(variable) selection\n",
    "Regarding the formula (base fare + all surcharges + taxes + tolls), we calculate the total amount as follows: it is equal to fare_amount + extra + mta_tax + tip_amount + tolls_amount + improvement_surcharge + congestion_surcharge + airport_fee, we will assume the subsurcharges segment includes extra, tip_amount, improvement_surcharge, congestion_surcharge and airport_fee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "830ef27d-90a7-4ed8-aaa2-da3a7e5c8403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_taxi_data(taxi_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean the taxi data by retaining specific columns, normalizing column names,\n",
    "    converting column types, and removing invalid trips. This includes unifying column names\n",
    "    across different data sources using a predefined mapping.\n",
    "\n",
    "    Args:\n",
    "        taxi_data (pd.DataFrame): The input taxi data DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned and filtered taxi data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize column names using the column_mapping\n",
    "    column_mapping = {\n",
    "    'tpep_pickup_datetime': 'pickup_datetime',\n",
    "    'tpep_dropoff_datetime': 'dropoff_datetime',\n",
    "    'trip_distance': 'trip_miles'}\n",
    "   \n",
    "    taxi_data = taxi_data.rename(columns=column_mapping)\n",
    "\n",
    "    \n",
    "    # Add latitude and longitude\n",
    "    taxi_data[\"PU_coords\"] = taxi_data[\"PULocationID\"].apply(lambda x: lookup_coords_for_taxi_zone_id(x, taxi_zones))\n",
    "    taxi_data[\"DO_coords\"] = taxi_data[\"DOLocationID\"].apply(lambda x: lookup_coords_for_taxi_zone_id(x, taxi_zones))\n",
    "    taxi_data = taxi_data.dropna(subset=[\"PU_coords\", \"DO_coords\"]).reset_index(drop=True)\n",
    "    taxi_data[[\"PU_lat\", \"PU_lon\"]] = pd.DataFrame(taxi_data[\"PU_coords\"].tolist(), index=taxi_data.index)\n",
    "    taxi_data[[\"DO_lat\", \"DO_lon\"]] = pd.DataFrame(taxi_data[\"DO_coords\"].tolist(), index=taxi_data.index)\n",
    "    taxi_data = taxi_data.drop(columns=[\"PU_coords\", \"DO_coords\"])\n",
    "    \n",
    "    # Retain only the required columns\n",
    "    required_columns = [\n",
    "        'pickup_datetime', 'dropoff_datetime', 'trip_miles',\n",
    "        'PU_lat', 'PU_lon', 'DO_lat', 'DO_lon', 'total_amount',\n",
    "        'fare_amount', 'extra', 'mta_tax', 'tip_amount', \n",
    "        'tolls_amount', 'improvement_surcharge', 'congestion_surcharge', 'airport_fee'\n",
    "    ]\n",
    "    taxi_data = taxi_data[required_columns]\n",
    "\n",
    "    # Removing Invalid Data Points\n",
    "    taxi_data = taxi_data[taxi_data[\"trip_miles\"] > 0]\n",
    "    taxi_data = taxi_data[taxi_data[\"total_amount\"] > 0]\n",
    "    taxi_data = taxi_data[taxi_data[\"fare_amount\"] > 0]\n",
    "    taxi_data = taxi_data[taxi_data[\"pickup_datetime\"] < taxi_data[\"dropoff_datetime\"]]\n",
    "\n",
    "    #Normalize column names\n",
    "    taxi_data.columns = [col.lower() for col in taxi_data.columns]\n",
    "\n",
    "    # Normalizing and Using Appropriate Column Types\n",
    "    taxi_data[\"pickup_datetime\"] = pd.to_datetime(taxi_data[\"pickup_datetime\"], errors=\"coerce\")\n",
    "    taxi_data[\"dropoff_datetime\"] = pd.to_datetime(taxi_data[\"dropoff_datetime\"], errors=\"coerce\")\n",
    "    numeric_columns = [\n",
    "        \"trip_miles\", \"pu_lat\", \"pu_lon\", \"do_lat\", \"do_lon\", \"total_amount\",\n",
    "        'fare_amount', 'extra', 'mta_tax', 'tip_amount', \n",
    "        'tolls_amount', 'improvement_surcharge', 'congestion_surcharge', 'airport_fee'\n",
    "    ]\n",
    "    # Process numeric columns\n",
    "    taxi_data[numeric_columns] = taxi_data[numeric_columns].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    \n",
    "    # Drop rows where pickup_datetime or dropoff_datetime is NaN\n",
    "    taxi_data = taxi_data.dropna(subset=[\"pickup_datetime\", \"dropoff_datetime\"])\n",
    "    \n",
    "    # Fill NaN values in numeric columns with 0\n",
    "    taxi_data[numeric_columns] = taxi_data[numeric_columns].fillna(0)\n",
    "\n",
    "    # Removing Trips Outside the Latitude/Longitude Bounding Box\n",
    "    lat_min, lon_min = 40.560445, -74.242330\n",
    "    lat_max, lon_max = 40.908524, -73.717047\n",
    "    taxi_data = taxi_data[\n",
    "        (taxi_data[\"pu_lat\"] >= lat_min) & (taxi_data[\"pu_lat\"] <= lat_max) &\n",
    "        (taxi_data[\"pu_lon\"] >= lon_min) & (taxi_data[\"pu_lon\"] <= lon_max) &\n",
    "        (taxi_data[\"do_lat\"] >= lat_min) & (taxi_data[\"do_lat\"] <= lat_max) &\n",
    "        (taxi_data[\"do_lon\"] >= lon_min) & (taxi_data[\"do_lon\"] <= lon_max)\n",
    "    ]\n",
    "\n",
    "    # Reset index after filtering\n",
    "    taxi_data = taxi_data.reset_index(drop=True)\n",
    "\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "200776ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve, filter, and clean taxi data from the TLC data page.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Fetches all URLs from the TLC webpage.\n",
    "    2. Filters the URLs to retain only those pointing to Parquet files.\n",
    "    3. Downloads, processes, and combines the data from these Parquet files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the combined and cleaned taxi data.\n",
    "    \"\"\"\n",
    "    all_urls: list[str] = get_all_urls_from_page(TLC_URL)\n",
    "    all_parquet_urls: list[str] = filter_parquet_urls(all_urls)\n",
    "    taxi_data: pd.DataFrame = get_and_clean_taxi_data(all_parquet_urls)\n",
    "\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "876bd645",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data = get_taxi_data()\n",
    "taxi_data = clean_taxi_data(taxi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "10ebd75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>pu_lat</th>\n",
       "      <th>pu_lon</th>\n",
       "      <th>do_lat</th>\n",
       "      <th>do_lon</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-20 13:31:30</td>\n",
       "      <td>2024-01-20 14:03:25</td>\n",
       "      <td>17.14</td>\n",
       "      <td>40.646985</td>\n",
       "      <td>-73.786530</td>\n",
       "      <td>40.749914</td>\n",
       "      <td>-73.970443</td>\n",
       "      <td>90.96</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8.27</td>\n",
       "      <td>6.94</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-18 21:52:46</td>\n",
       "      <td>2024-01-18 22:03:21</td>\n",
       "      <td>2.49</td>\n",
       "      <td>40.764421</td>\n",
       "      <td>-73.977569</td>\n",
       "      <td>40.790011</td>\n",
       "      <td>-73.945750</td>\n",
       "      <td>22.50</td>\n",
       "      <td>13.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01 03:43:58</td>\n",
       "      <td>2024-01-01 03:50:47</td>\n",
       "      <td>1.84</td>\n",
       "      <td>40.866075</td>\n",
       "      <td>-73.919308</td>\n",
       "      <td>40.857779</td>\n",
       "      <td>-73.885867</td>\n",
       "      <td>12.50</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-19 22:20:12</td>\n",
       "      <td>2024-01-19 22:50:12</td>\n",
       "      <td>3.60</td>\n",
       "      <td>40.748497</td>\n",
       "      <td>-73.992438</td>\n",
       "      <td>40.778766</td>\n",
       "      <td>-73.951010</td>\n",
       "      <td>33.95</td>\n",
       "      <td>23.3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-06 22:41:50</td>\n",
       "      <td>2024-01-06 22:43:24</td>\n",
       "      <td>0.04</td>\n",
       "      <td>40.791705</td>\n",
       "      <td>-73.973049</td>\n",
       "      <td>40.791705</td>\n",
       "      <td>-73.973049</td>\n",
       "      <td>6.20</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pickup_datetime    dropoff_datetime  trip_miles     pu_lat     pu_lon  \\\n",
       "0 2024-01-20 13:31:30 2024-01-20 14:03:25       17.14  40.646985 -73.786530   \n",
       "1 2024-01-18 21:52:46 2024-01-18 22:03:21        2.49  40.764421 -73.977569   \n",
       "2 2024-01-01 03:43:58 2024-01-01 03:50:47        1.84  40.866075 -73.919308   \n",
       "3 2024-01-19 22:20:12 2024-01-19 22:50:12        3.60  40.748497 -73.992438   \n",
       "4 2024-01-06 22:41:50 2024-01-06 22:43:24        0.04  40.791705 -73.973049   \n",
       "\n",
       "      do_lat     do_lon  total_amount  fare_amount  extra  mta_tax  \\\n",
       "0  40.749914 -73.970443         90.96         70.0    0.0      0.5   \n",
       "1  40.790011 -73.945750         22.50         13.5    1.0      0.5   \n",
       "2  40.857779 -73.885867         12.50         10.0    1.0      0.5   \n",
       "3  40.778766 -73.951010         33.95         23.3    3.5      0.5   \n",
       "4  40.791705 -73.973049          6.20          3.7    1.0      0.5   \n",
       "\n",
       "   tip_amount  tolls_amount  improvement_surcharge  congestion_surcharge  \\\n",
       "0        8.27          6.94                    1.0                   2.5   \n",
       "1        4.00          0.00                    1.0                   2.5   \n",
       "2        0.00          0.00                    1.0                   0.0   \n",
       "3        5.65          0.00                    1.0                   2.5   \n",
       "4        0.00          0.00                    1.0                   0.0   \n",
       "\n",
       "   airport_fee  \n",
       "0          0.0  \n",
       "1          0.0  \n",
       "2          0.0  \n",
       "3          0.0  \n",
       "4          0.0  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c9da7089-3f6b-4f93-a22e-76bf554daca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20710 entries, 0 to 20709\n",
      "Data columns (total 16 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   pickup_datetime        20710 non-null  datetime64[ns]\n",
      " 1   dropoff_datetime       20710 non-null  datetime64[ns]\n",
      " 2   trip_miles             20710 non-null  float64       \n",
      " 3   pu_lat                 20710 non-null  float64       \n",
      " 4   pu_lon                 20710 non-null  float64       \n",
      " 5   do_lat                 20710 non-null  float64       \n",
      " 6   do_lon                 20710 non-null  float64       \n",
      " 7   total_amount           20710 non-null  float64       \n",
      " 8   fare_amount            20710 non-null  float64       \n",
      " 9   extra                  20710 non-null  float64       \n",
      " 10  mta_tax                20710 non-null  float64       \n",
      " 11  tip_amount             20710 non-null  float64       \n",
      " 12  tolls_amount           20710 non-null  float64       \n",
      " 13  improvement_surcharge  20710 non-null  float64       \n",
      " 14  congestion_surcharge   20710 non-null  float64       \n",
      " 15  airport_fee            20710 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(14)\n",
      "memory usage: 2.5 MB\n"
     ]
    }
   ],
   "source": [
    "taxi_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "50c85e25-6416-4c16-b98c-09596cdc6865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>pu_lat</th>\n",
       "      <th>pu_lon</th>\n",
       "      <th>do_lat</th>\n",
       "      <th>do_lon</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20710</td>\n",
       "      <td>20710</td>\n",
       "      <td>20710.000000</td>\n",
       "      <td>20710.000000</td>\n",
       "      <td>20710.000000</td>\n",
       "      <td>20710.000000</td>\n",
       "      <td>20710.000000</td>\n",
       "      <td>20710.000000</td>\n",
       "      <td>20710.000000</td>\n",
       "      <td>20710.000000</td>\n",
       "      <td>20710.000000</td>\n",
       "      <td>20710.000000</td>\n",
       "      <td>20710.000000</td>\n",
       "      <td>20710.000000</td>\n",
       "      <td>20710.000000</td>\n",
       "      <td>20710.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-04-29 15:32:02.506904832</td>\n",
       "      <td>2022-04-29 15:48:05.330613248</td>\n",
       "      <td>3.271791</td>\n",
       "      <td>40.753381</td>\n",
       "      <td>-73.966982</td>\n",
       "      <td>40.755782</td>\n",
       "      <td>-73.970786</td>\n",
       "      <td>22.497828</td>\n",
       "      <td>15.378428</td>\n",
       "      <td>1.221689</td>\n",
       "      <td>0.497586</td>\n",
       "      <td>2.701182</td>\n",
       "      <td>0.437548</td>\n",
       "      <td>0.551994</td>\n",
       "      <td>2.212820</td>\n",
       "      <td>0.033679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-01-01 00:11:06</td>\n",
       "      <td>2020-01-01 00:30:50</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>40.576961</td>\n",
       "      <td>-74.029893</td>\n",
       "      <td>40.576961</td>\n",
       "      <td>-74.174002</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2021-02-27 01:11:26.750000128</td>\n",
       "      <td>2021-02-27 01:26:18.249999872</td>\n",
       "      <td>1.090000</td>\n",
       "      <td>40.740337</td>\n",
       "      <td>-73.989845</td>\n",
       "      <td>40.740337</td>\n",
       "      <td>-73.989845</td>\n",
       "      <td>12.800000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-04-28 13:02:38.500000</td>\n",
       "      <td>2022-04-28 13:12:18</td>\n",
       "      <td>1.805000</td>\n",
       "      <td>40.758028</td>\n",
       "      <td>-73.977698</td>\n",
       "      <td>40.758028</td>\n",
       "      <td>-73.977698</td>\n",
       "      <td>17.020000</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.160000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-06-27 15:31:13</td>\n",
       "      <td>2023-06-27 15:53:30.500000</td>\n",
       "      <td>3.310000</td>\n",
       "      <td>40.773633</td>\n",
       "      <td>-73.961764</td>\n",
       "      <td>40.775932</td>\n",
       "      <td>-73.959635</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.450000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-08-31 22:43:47</td>\n",
       "      <td>2024-08-31 23:26:23</td>\n",
       "      <td>67.900000</td>\n",
       "      <td>40.899528</td>\n",
       "      <td>-73.739337</td>\n",
       "      <td>40.899528</td>\n",
       "      <td>-73.726655</td>\n",
       "      <td>262.700000</td>\n",
       "      <td>209.500000</td>\n",
       "      <td>11.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.111148</td>\n",
       "      <td>0.032344</td>\n",
       "      <td>0.044978</td>\n",
       "      <td>0.033091</td>\n",
       "      <td>0.036046</td>\n",
       "      <td>17.358570</td>\n",
       "      <td>13.648988</td>\n",
       "      <td>1.507951</td>\n",
       "      <td>0.034661</td>\n",
       "      <td>3.160579</td>\n",
       "      <td>1.795556</td>\n",
       "      <td>0.336437</td>\n",
       "      <td>0.797188</td>\n",
       "      <td>0.202403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     pickup_datetime               dropoff_datetime  \\\n",
       "count                          20710                          20710   \n",
       "mean   2022-04-29 15:32:02.506904832  2022-04-29 15:48:05.330613248   \n",
       "min              2020-01-01 00:11:06            2020-01-01 00:30:50   \n",
       "25%    2021-02-27 01:11:26.750000128  2021-02-27 01:26:18.249999872   \n",
       "50%       2022-04-28 13:02:38.500000            2022-04-28 13:12:18   \n",
       "75%              2023-06-27 15:31:13     2023-06-27 15:53:30.500000   \n",
       "max              2024-08-31 22:43:47            2024-08-31 23:26:23   \n",
       "std                              NaN                            NaN   \n",
       "\n",
       "         trip_miles        pu_lat        pu_lon        do_lat        do_lon  \\\n",
       "count  20710.000000  20710.000000  20710.000000  20710.000000  20710.000000   \n",
       "mean       3.271791     40.753381    -73.966982     40.755782    -73.970786   \n",
       "min        0.010000     40.576961    -74.029893     40.576961    -74.174002   \n",
       "25%        1.090000     40.740337    -73.989845     40.740337    -73.989845   \n",
       "50%        1.805000     40.758028    -73.977698     40.758028    -73.977698   \n",
       "75%        3.310000     40.773633    -73.961764     40.775932    -73.959635   \n",
       "max       67.900000     40.899528    -73.739337     40.899528    -73.726655   \n",
       "std        4.111148      0.032344      0.044978      0.033091      0.036046   \n",
       "\n",
       "       total_amount   fare_amount         extra       mta_tax    tip_amount  \\\n",
       "count  20710.000000  20710.000000  20710.000000  20710.000000  20710.000000   \n",
       "mean      22.497828     15.378428      1.221689      0.497586      2.701182   \n",
       "min        1.300000      1.000000      0.000000      0.000000      0.000000   \n",
       "25%       12.800000      7.500000      0.000000      0.500000      0.000000   \n",
       "50%       17.020000     10.700000      0.500000      0.500000      2.160000   \n",
       "75%       24.500000     17.000000      2.500000      0.500000      3.450000   \n",
       "max      262.700000    209.500000     11.750000      0.500000     50.000000   \n",
       "std       17.358570     13.648988      1.507951      0.034661      3.160579   \n",
       "\n",
       "       tolls_amount  improvement_surcharge  congestion_surcharge   airport_fee  \n",
       "count  20710.000000           20710.000000          20710.000000  20710.000000  \n",
       "mean       0.437548               0.551994              2.212820      0.033679  \n",
       "min        0.000000               0.000000              0.000000      0.000000  \n",
       "25%        0.000000               0.300000              2.500000      0.000000  \n",
       "50%        0.000000               0.300000              2.500000      0.000000  \n",
       "75%        0.000000               1.000000              2.500000      0.000000  \n",
       "max       40.000000               1.000000              2.500000      1.250000  \n",
       "std        1.795556               0.336437              0.797188      0.202403  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a3ec0e5e-387e-499d-9ba5-a80b85697d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"taxi_data_cleaned.csv\"\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "taxi_data.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "## Processing Uber Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d4767a-1a35-494d-9db4-448969b929e6",
   "metadata": {},
   "source": [
    "This section processes Uber High-Volume For-Hire (HVHF) trip data, ensuring it is clean, consistent, and ready for analysis. The process involves downloading, sampling, cleaning, and saving the data.\n",
    "\n",
    "### Steps Involved\n",
    "\n",
    "#### 1. Download and Sample Data\n",
    "##### **Function: `get_and_clean_uber_month`**\n",
    "- Downloads data for a specific month if it doesn't already exist locally.\n",
    "- Samples the dataset using Cochran's formula, which ensures a manageable file size while maintaining statistical relevance.\n",
    "- Saves the processed sample locally for reuse.\n",
    "\n",
    "##### **Function: `get_and_clean_uber_data`**\n",
    "- Loops through multiple Parquet file URLs for HVHF data.\n",
    "- Downloads and processes each month's data, sampling where necessary.\n",
    "- Combines all monthly datasets into a single, larger DataFrame.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Clean and Process the Data\n",
    "##### **Function: `load_and_clean_uber_data`**\n",
    "This function ensures the Uber dataset is ready for analysis by performing the following steps:\n",
    "- **Filter Uber Rides**: Includes only trips with the license number `HV0003` (Uber-specific rides).\n",
    "- **Add Coordinates**: Converts location IDs into latitude and longitude coordinates for both pickup and dropoff locations.\n",
    "- **Keep Essential Columns**: Retains only the columns required for analysis, such as:\n",
    "  - Pickup/Dropoff datetime\n",
    "  - Coordinates\n",
    "  - Trip distance\n",
    "  - Fare details\n",
    "- **Remove Invalid Data**: Excludes trips with issues like:\n",
    "  - Zero or negative distance\n",
    "  - Negative fare amounts\n",
    "  - Incorrect timestamps\n",
    "- **Normalize Data**:\n",
    "  - Renames columns for consistency.\n",
    "  - Ensures numeric fields (e.g., distance, fares) are in the correct format.\n",
    "- **Filter NYC Trips**: Keeps only trips within NYC’s geographic boundaries using latitude and longitude filtering.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Combine All Steps\n",
    "##### **Function: `get_uber_data`**\n",
    "- Fetches all URLs for Uber HVHF Parquet files from the TLC webpage.\n",
    "- Filters the URLs to retain only relevant data files.\n",
    "- Downloads, processes, cleans, and combines all monthly datasets.\n",
    "\n",
    "The cleaned Uber data is now ready for analysis and integration with other datasets like weather or Yellow Taxi data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07574983-f41d-4cd6-8f70-489493089b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_uber_month(parquet_url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download, process, and sample an Uber HVHF dataset for a given month.\n",
    "\n",
    "    Args:\n",
    "        parquet_url (str): URL of the Uber HVHF Parquet file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A sampled and processed DataFrame.\n",
    "    \"\"\"\n",
    "    # Define the directory for processed Uber HVHF data\n",
    "    save_dir: str = \"processed_data/hvhf\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Extract file name and define local path\n",
    "    file_name: str = parquet_url.split(\"/\")[-1]\n",
    "    local_file_path: str = os.path.join(save_dir, file_name)\n",
    "\n",
    "    # Download the file\n",
    "    if not os.path.exists(local_file_path):\n",
    "        try:\n",
    "            response = requests.get(parquet_url, stream=True)\n",
    "            response.raise_for_status()\n",
    "            with open(local_file_path, \"wb\") as f:\n",
    "                for chunk in response.iter_content(chunk_size=1024 * 1024):  # 1MB chunks\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            raise Exception(f\"Failed to download the file from {parquet_url}. Error: {e}\")\n",
    "    data: pd.DataFrame = pd.read_parquet(local_file_path)\n",
    "\n",
    "    # Sample the dataset\n",
    "    population: int = len(data)\n",
    "    sample_size: int = calculate_sample_size(population, p=0.4)\n",
    "    sampled_data: pd.DataFrame = data.sample(n=sample_size, random_state=42) if population > sample_size else data\n",
    "    processed_file_path: str = os.path.join(save_dir, f\"sampled_{file_name}\")\n",
    "    if not os.path.exists(processed_file_path):\n",
    "        sampled_data.to_parquet(processed_file_path)\n",
    "\n",
    "    return sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1b3d85ff-313c-41a2-9a46-261a9a2bb10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_uber_data(parquet_urls: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download, process, and combine Uber HVHF data from multiple Parquet file URLs.\n",
    "\n",
    "    Args:\n",
    "        parquet_urls (List[str]): A list of URLs pointing to Uber HVHF Parquet files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A combined DataFrame containing processed data from all valid Parquet files.\n",
    "    \"\"\"\n",
    "    all_uber_dataframes: List[pd.DataFrame] = []\n",
    "\n",
    "    # Regex pattern to filter Uber HVHF Parquet files for specific dates (2020-2024)\n",
    "    hvfhv_pattern: re.Pattern = re.compile(\n",
    "        r\"fhvhv_tripdata_(2020-(0[1-9]|1[0-2])|202[1-3]-(0[1-9]|1[0-2])|2024-(0[1-8]))\\.parquet\"\n",
    "    )\n",
    "    hvfhv_urls: List[str] = [url for url in parquet_urls if hvfhv_pattern.search(url)]\n",
    "\n",
    "    for url in hvfhv_urls:\n",
    "        save_dir: str = \"processed_data/hvhf\"\n",
    "        file_name: str = f\"sampled_{url.split('/')[-1]}\"\n",
    "        processed_file_path: str = os.path.join(save_dir, file_name)\n",
    "\n",
    "        if os.path.exists(processed_file_path):\n",
    "            dataframe: pd.DataFrame = pd.read_parquet(processed_file_path)\n",
    "        else:\n",
    "            dataframe: pd.DataFrame = get_and_clean_uber_month(url)\n",
    "\n",
    "        all_uber_dataframes.append(dataframe)\n",
    "    uber_data: pd.DataFrame = pd.concat(all_uber_dataframes, ignore_index=True)\n",
    "    return uber_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d98f94-fbb0-4851-a1cc-8e5806989567",
   "metadata": {},
   "source": [
    "### Note: Column (variable) selection:\n",
    "Regarding the formula (base fare + all surcharges + taxes + tolls), we calculate the total amount as follows: it is equal to fare_amount + tolls + bcf + sales_tax + tips + congestion_surcharge + airport_fee, we will assume the subsurcharges segment includes bcf, tips, congestion_surcharge and airport_fee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(uber_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean and process Uber HVHF data, including filtering by license, adding coordinates,\n",
    "    retaining required columns, and normalizing data.\n",
    "\n",
    "    Args:\n",
    "        uber_data (pd.DataFrame): The raw Uber HVHF data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned and filtered DataFrame with necessary columns and normalized values.\n",
    "    \"\"\"\n",
    "    # Filter by HVFH license number\n",
    "    uber_data['hvfhs_license_num'] = uber_data['hvfhs_license_num'].astype(str)\n",
    "    uber_data = uber_data[uber_data['hvfhs_license_num'] == 'HV0003'].copy()\n",
    "\n",
    "    # Add coordinates for pickup and dropoff locations\n",
    "    uber_data[\"PU_coords\"] = uber_data[\"PULocationID\"].apply(lambda x: lookup_coords_for_taxi_zone_id(x, taxi_zones))\n",
    "    uber_data[\"DO_coords\"] = uber_data[\"DOLocationID\"].apply(lambda x: lookup_coords_for_taxi_zone_id(x, taxi_zones))\n",
    "    uber_data = uber_data.dropna(subset=[\"PU_coords\", \"DO_coords\"]).reset_index(drop=True)\n",
    "    uber_data[[\"PU_lat\", \"PU_lon\"]] = pd.DataFrame(uber_data[\"PU_coords\"].tolist(), index=uber_data.index)\n",
    "    uber_data[[\"DO_lat\", \"DO_lon\"]] = pd.DataFrame(uber_data[\"DO_coords\"].tolist(), index=uber_data.index)\n",
    "    uber_data = uber_data.drop(columns=[\"PU_coords\", \"DO_coords\"])\n",
    "\n",
    "    # Retain required columns\n",
    "    required_columns = [\n",
    "        'pickup_datetime', 'dropoff_datetime', 'PU_lat', 'PU_lon', \n",
    "        'DO_lat', 'DO_lon', 'trip_miles', 'base_passenger_fare', \n",
    "        'tolls', 'bcf', 'sales_tax', 'tips', 'airport_fee', 'congestion_surcharge'\n",
    "    ]\n",
    "    uber_data = uber_data[required_columns]\n",
    "\n",
    "    # Remove invalid data points\n",
    "    uber_data = uber_data[uber_data[\"trip_miles\"] > 0]\n",
    "    uber_data = uber_data[uber_data[\"base_passenger_fare\"] > 0]\n",
    "    uber_data = uber_data[uber_data[\"pickup_datetime\"] < uber_data[\"dropoff_datetime\"]]\n",
    "\n",
    "    # Normalize column names\n",
    "    uber_data.columns = [col.lower() for col in uber_data.columns]\n",
    "\n",
    "    # Ensure numeric columns are float or int\n",
    "    numeric_columns = [\n",
    "        'pu_lat', 'pu_lon', 'do_lat', 'do_lon', 'trip_miles', \n",
    "        'base_passenger_fare', 'tolls', 'bcf', 'sales_tax', \n",
    "        'tips', 'airport_fee', 'congestion_surcharge'\n",
    "    ]\n",
    "    uber_data[numeric_columns] = uber_data[numeric_columns].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    uber_data[numeric_columns] = uber_data[numeric_columns].fillna(0)\n",
    "    uber_data = uber_data.dropna(subset=[\"pickup_datetime\", \"dropoff_datetime\"])\n",
    "\n",
    "    # Normalize column names using the column_mapping\n",
    "    column_mapping = {\n",
    "    'base_passenger_fare': 'fare_amount',\n",
    "    'tips': 'tip_amount',\n",
    "    'tolls': 'tolls_amount'}\n",
    "   \n",
    "    uber_data = uber_data.rename(columns=column_mapping)\n",
    "\n",
    "    # Filter trips within the latitude/longitude bounding box\n",
    "    lat_min, lon_min = 40.560445, -74.242330\n",
    "    lat_max, lon_max = 40.908524, -73.717047\n",
    "    uber_data = uber_data[\n",
    "        (uber_data[\"pu_lat\"] >= lat_min) & (uber_data[\"pu_lat\"] <= lat_max) &\n",
    "        (uber_data[\"pu_lon\"] >= lon_min) & (uber_data[\"pu_lon\"] <= lon_max) &\n",
    "        (uber_data[\"do_lat\"] >= lat_min) & (uber_data[\"do_lat\"] <= lat_max) &\n",
    "        (uber_data[\"do_lon\"] >= lon_min) & (uber_data[\"do_lon\"] <= lon_max)\n",
    "    ]\n",
    "\n",
    "    # Reset index after filtering\n",
    "    uber_data = uber_data.reset_index(drop=True)\n",
    "\n",
    "    return uber_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve, filter, process, and clean Uber HVHF data from the TLC data page.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Fetch all URLs from the TLC webpage.\n",
    "    2. Filter the URLs to retain only those pointing to Parquet files.\n",
    "    3. Download, process, and combine data from these Parquet files.\n",
    "    4. Perform additional cleaning and normalization on the combined data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned and processed DataFrame containing Uber HVHF trip data.\n",
    "    \"\"\"\n",
    "    all_urls: list[str] = get_all_urls_from_page(TLC_URL)\n",
    "    all_parquet_urls: list[str] = filter_parquet_urls(all_urls)\n",
    "    uber_data: pd.DataFrame = get_and_clean_uber_data(all_parquet_urls)\n",
    "    uber_data = load_and_clean_uber_data(uber_data)\n",
    "    return uber_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c2bd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "339997e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>pu_lat</th>\n",
       "      <th>pu_lon</th>\n",
       "      <th>do_lat</th>\n",
       "      <th>do_lon</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>bcf</th>\n",
       "      <th>sales_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-26 08:07:17</td>\n",
       "      <td>2024-01-26 08:35:38</td>\n",
       "      <td>40.646116</td>\n",
       "      <td>-73.951623</td>\n",
       "      <td>40.666559</td>\n",
       "      <td>-73.895364</td>\n",
       "      <td>4.29</td>\n",
       "      <td>27.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-19 02:17:05</td>\n",
       "      <td>2024-01-19 02:29:12</td>\n",
       "      <td>40.882403</td>\n",
       "      <td>-73.910665</td>\n",
       "      <td>40.857108</td>\n",
       "      <td>-73.932832</td>\n",
       "      <td>2.55</td>\n",
       "      <td>15.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-21 01:44:00</td>\n",
       "      <td>2024-01-21 02:08:30</td>\n",
       "      <td>40.748575</td>\n",
       "      <td>-73.985156</td>\n",
       "      <td>40.715370</td>\n",
       "      <td>-73.936794</td>\n",
       "      <td>6.37</td>\n",
       "      <td>24.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.68</td>\n",
       "      <td>2.18</td>\n",
       "      <td>3.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-20 12:58:40</td>\n",
       "      <td>2024-01-20 13:15:42</td>\n",
       "      <td>40.758028</td>\n",
       "      <td>-73.977698</td>\n",
       "      <td>40.753309</td>\n",
       "      <td>-74.004016</td>\n",
       "      <td>1.99</td>\n",
       "      <td>18.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-02 08:40:48</td>\n",
       "      <td>2024-01-02 08:54:28</td>\n",
       "      <td>40.666559</td>\n",
       "      <td>-73.895364</td>\n",
       "      <td>40.676644</td>\n",
       "      <td>-73.913632</td>\n",
       "      <td>2.23</td>\n",
       "      <td>16.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pickup_datetime    dropoff_datetime     pu_lat     pu_lon     do_lat  \\\n",
       "0 2024-01-26 08:07:17 2024-01-26 08:35:38  40.646116 -73.951623  40.666559   \n",
       "1 2024-01-19 02:17:05 2024-01-19 02:29:12  40.882403 -73.910665  40.857108   \n",
       "2 2024-01-21 01:44:00 2024-01-21 02:08:30  40.748575 -73.985156  40.715370   \n",
       "3 2024-01-20 12:58:40 2024-01-20 13:15:42  40.758028 -73.977698  40.753309   \n",
       "4 2024-01-02 08:40:48 2024-01-02 08:54:28  40.666559 -73.895364  40.676644   \n",
       "\n",
       "      do_lon  trip_miles  fare_amount  tolls_amount   bcf  sales_tax  \\\n",
       "0 -73.895364        4.29        27.49           0.0  0.76       2.44   \n",
       "1 -73.932832        2.55        15.14           0.0  0.42       1.34   \n",
       "2 -73.936794        6.37        24.57           0.0  0.68       2.18   \n",
       "3 -74.004016        1.99        18.96           0.0  0.52       1.68   \n",
       "4 -73.913632        2.23        16.08           0.0  0.44       1.43   \n",
       "\n",
       "   tip_amount  airport_fee  congestion_surcharge  \n",
       "0        0.00          0.0                  0.00  \n",
       "1        0.00          0.0                  0.00  \n",
       "2        3.01          0.0                  2.75  \n",
       "3        0.00          0.0                  2.75  \n",
       "4        0.00          0.0                  0.00  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "944ae7fb-e7e0-4ff8-b956-993854dcc826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14383 entries, 0 to 14382\n",
      "Data columns (total 14 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   pickup_datetime       14383 non-null  datetime64[ns]\n",
      " 1   dropoff_datetime      14383 non-null  datetime64[ns]\n",
      " 2   pu_lat                14383 non-null  float64       \n",
      " 3   pu_lon                14383 non-null  float64       \n",
      " 4   do_lat                14383 non-null  float64       \n",
      " 5   do_lon                14383 non-null  float64       \n",
      " 6   trip_miles            14383 non-null  float64       \n",
      " 7   fare_amount           14383 non-null  float64       \n",
      " 8   tolls_amount          14383 non-null  float64       \n",
      " 9   bcf                   14383 non-null  float64       \n",
      " 10  sales_tax             14383 non-null  float64       \n",
      " 11  tip_amount            14383 non-null  float64       \n",
      " 12  airport_fee           14383 non-null  float64       \n",
      " 13  congestion_surcharge  14383 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(12)\n",
      "memory usage: 1.5 MB\n"
     ]
    }
   ],
   "source": [
    "uber_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6fddeb14-cd70-4e83-8f93-974642c3bea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>pu_lat</th>\n",
       "      <th>pu_lon</th>\n",
       "      <th>do_lat</th>\n",
       "      <th>do_lon</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>bcf</th>\n",
       "      <th>sales_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14383</td>\n",
       "      <td>14383</td>\n",
       "      <td>14383.000000</td>\n",
       "      <td>14383.000000</td>\n",
       "      <td>14383.000000</td>\n",
       "      <td>14383.000000</td>\n",
       "      <td>14383.000000</td>\n",
       "      <td>14383.000000</td>\n",
       "      <td>14383.000000</td>\n",
       "      <td>14383.000000</td>\n",
       "      <td>14383.000000</td>\n",
       "      <td>14383.00000</td>\n",
       "      <td>14383.000000</td>\n",
       "      <td>14383.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-05-03 15:37:43.487311360</td>\n",
       "      <td>2022-05-03 15:55:31.077104896</td>\n",
       "      <td>40.737349</td>\n",
       "      <td>-73.935103</td>\n",
       "      <td>40.737331</td>\n",
       "      <td>-73.935269</td>\n",
       "      <td>4.372007</td>\n",
       "      <td>21.021585</td>\n",
       "      <td>0.678372</td>\n",
       "      <td>0.616084</td>\n",
       "      <td>1.867329</td>\n",
       "      <td>0.79651</td>\n",
       "      <td>0.136533</td>\n",
       "      <td>1.056525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-01-01 03:27:51</td>\n",
       "      <td>2020-01-01 03:30:21</td>\n",
       "      <td>40.561994</td>\n",
       "      <td>-74.170885</td>\n",
       "      <td>40.561994</td>\n",
       "      <td>-74.174002</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2021-02-27 15:08:56</td>\n",
       "      <td>2021-02-27 15:21:33</td>\n",
       "      <td>40.691201</td>\n",
       "      <td>-73.984197</td>\n",
       "      <td>40.691201</td>\n",
       "      <td>-73.984197</td>\n",
       "      <td>1.550000</td>\n",
       "      <td>10.490000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-05-01 06:12:14</td>\n",
       "      <td>2022-05-01 06:17:35</td>\n",
       "      <td>40.736824</td>\n",
       "      <td>-73.948788</td>\n",
       "      <td>40.737698</td>\n",
       "      <td>-73.948136</td>\n",
       "      <td>2.830000</td>\n",
       "      <td>16.680000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-07-02 21:51:33.500000</td>\n",
       "      <td>2023-07-02 22:17:46.500000</td>\n",
       "      <td>40.774376</td>\n",
       "      <td>-73.900317</td>\n",
       "      <td>40.774376</td>\n",
       "      <td>-73.899735</td>\n",
       "      <td>5.530000</td>\n",
       "      <td>26.350000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>2.350000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-08-31 21:37:05</td>\n",
       "      <td>2024-08-31 21:57:29</td>\n",
       "      <td>40.899528</td>\n",
       "      <td>-73.726655</td>\n",
       "      <td>40.899528</td>\n",
       "      <td>-73.726655</td>\n",
       "      <td>33.170000</td>\n",
       "      <td>188.580000</td>\n",
       "      <td>43.310000</td>\n",
       "      <td>5.450000</td>\n",
       "      <td>17.570000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.068382</td>\n",
       "      <td>0.064742</td>\n",
       "      <td>0.068795</td>\n",
       "      <td>0.068168</td>\n",
       "      <td>4.263149</td>\n",
       "      <td>15.197834</td>\n",
       "      <td>2.713401</td>\n",
       "      <td>0.493748</td>\n",
       "      <td>1.412463</td>\n",
       "      <td>2.41649</td>\n",
       "      <td>0.571035</td>\n",
       "      <td>1.333243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     pickup_datetime               dropoff_datetime  \\\n",
       "count                          14383                          14383   \n",
       "mean   2022-05-03 15:37:43.487311360  2022-05-03 15:55:31.077104896   \n",
       "min              2020-01-01 03:27:51            2020-01-01 03:30:21   \n",
       "25%              2021-02-27 15:08:56            2021-02-27 15:21:33   \n",
       "50%              2022-05-01 06:12:14            2022-05-01 06:17:35   \n",
       "75%       2023-07-02 21:51:33.500000     2023-07-02 22:17:46.500000   \n",
       "max              2024-08-31 21:37:05            2024-08-31 21:57:29   \n",
       "std                              NaN                            NaN   \n",
       "\n",
       "             pu_lat        pu_lon        do_lat        do_lon    trip_miles  \\\n",
       "count  14383.000000  14383.000000  14383.000000  14383.000000  14383.000000   \n",
       "mean      40.737349    -73.935103     40.737331    -73.935269      4.372007   \n",
       "min       40.561994    -74.170885     40.561994    -74.174002      0.010000   \n",
       "25%       40.691201    -73.984197     40.691201    -73.984197      1.550000   \n",
       "50%       40.736824    -73.948788     40.737698    -73.948136      2.830000   \n",
       "75%       40.774376    -73.900317     40.774376    -73.899735      5.530000   \n",
       "max       40.899528    -73.726655     40.899528    -73.726655     33.170000   \n",
       "std        0.068382      0.064742      0.068795      0.068168      4.263149   \n",
       "\n",
       "        fare_amount  tolls_amount           bcf     sales_tax   tip_amount  \\\n",
       "count  14383.000000  14383.000000  14383.000000  14383.000000  14383.00000   \n",
       "mean      21.021585      0.678372      0.616084      1.867329      0.79651   \n",
       "min        0.610000      0.000000      0.000000      0.000000      0.00000   \n",
       "25%       10.490000      0.000000      0.290000      0.910000      0.00000   \n",
       "50%       16.680000      0.000000      0.470000      1.450000      0.00000   \n",
       "75%       26.350000      0.000000      0.770000      2.350000      0.00000   \n",
       "max      188.580000     43.310000      5.450000     17.570000     50.00000   \n",
       "std       15.197834      2.713401      0.493748      1.412463      2.41649   \n",
       "\n",
       "        airport_fee  congestion_surcharge  \n",
       "count  14383.000000          14383.000000  \n",
       "mean       0.136533              1.056525  \n",
       "min        0.000000              0.000000  \n",
       "25%        0.000000              0.000000  \n",
       "50%        0.000000              0.000000  \n",
       "75%        0.000000              2.750000  \n",
       "max        5.000000              2.750000  \n",
       "std        0.571035              1.333243  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "33bf45d4-dbc2-4928-b45b-fe29e4d20683",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"uber_data_cleaned.csv\"\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "uber_data.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "## Processing Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794fc0cd-a233-4f5e-9327-a5e3e71bda1b",
   "metadata": {},
   "source": [
    "This section processes raw weather data from CSV files into clean, ready-to-use datasets for analysis. The weather data includes both hourly and daily records, which are cleaned and stored separately.\n",
    "\n",
    "### Steps Involved\n",
    "\n",
    "#### 1. Fetch Weather CSV Files\n",
    "##### **Function: `get_all_weather_csvs`**\n",
    "- **Purpose**: Collects all CSV files from a specific directory containing weather data.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Clean Hourly Weather Data\n",
    "##### **Function: `clean_month_weather_data_hourly`**\n",
    "- **Purpose**: Processes and cleans hourly weather data from a single CSV file.\n",
    "- **Steps**:\n",
    "  - Reads the file and extracts relevant columns: \n",
    "    - `date` (timestamp)\n",
    "    - `precipitation` (amount of rain/snow)\n",
    "    - `wind_speed` (wind velocity)\n",
    "  - Converts columns to numeric types for analysis.\n",
    "  - Handles missing or invalid values by replacing them with `NaN` (Not a Number).\n",
    "  - Normalizes column names for consistency.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Clean Daily Weather Data\n",
    "##### **Function: `clean_month_weather_data_daily`**\n",
    "- **Purpose**: Processes and cleans daily weather data from a single CSV file.\n",
    "- **Steps**:\n",
    "  - Extracts and cleans columns for:\n",
    "    - `date`\n",
    "    - `precipitation` (total rainfall/snowfall)\n",
    "    - `average_wind_speed` (daily average wind velocity)\n",
    "    - `snowfall` (total snowfall)\n",
    "    - `sunrise` (sunrise time)\n",
    "    - `sunset` (sunset time)\n",
    "  - Converts columns to numeric types and drops rows with missing values in all key fields.\n",
    "  - Normalizes column names for consistency.\n",
    "- **Output**: A clean DataFrame with daily weather data.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Combine Weather Data\n",
    "##### **Function: `load_and_clean_weather_data`**\n",
    "- **Purpose**: Processes all weather CSV files and combines them into unified datasets.\n",
    "- **Steps**:\n",
    "  - Iterates over all weather files.\n",
    "  - Cleans hourly and daily weather data for each file.\n",
    "  - Combines all hourly data into a single DataFrame (`hourly_data`).\n",
    "  - Combines all daily data into another DataFrame (`daily_data`).\n",
    "  - Replaces any remaining missing values with `0` to ensure data completeness.\n",
    "- **Output**: Two comprehensive DataFrames:\n",
    "  - `hourly_data`: Hourly weather data for all files.\n",
    "  - `daily_data`: Daily weather data for all files.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Save Processed Data\n",
    "- **Outputs**:\n",
    "  - Hourly weather data is saved to `cleaned_hourly_weather_data.csv`.\n",
    "  - Daily weather data is saved to `cleaned_daily_weather_data.csv`.\n",
    "---\n",
    "\n",
    "### Outcome\n",
    "The cleaned weather data is stored in two CSV files, ready for integration with ride data or for standalone analysis:\n",
    "- **Hourly Weather Data**: Provides detailed hourly precipitation and wind speed.\n",
    "- **Daily Weather Data**: Offers daily summaries of precipitation, wind speed, and snowfall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "eece1cb5-c613-43a4-8e71-ae9ba348c624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieve all CSV file paths from a specified directory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing CSV files.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of file paths to all CSV files in the directory.\n",
    "    \"\"\"\n",
    "    csv_files: List[str] = [\n",
    "        os.path.join(directory, file) for file in os.listdir(directory) if file.endswith(\".csv\")\n",
    "    ]\n",
    "    return csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean and process hourly weather data from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file containing weather data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing cleaned and processed hourly weather data.\n",
    "                      Columns include 'date', 'precipitation', and 'wind_speed'.\n",
    "                      Returns an empty DataFrame if the processing fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df: pd.DataFrame = pd.read_csv(csv_file, low_memory=False)\n",
    "        df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "\n",
    "        # Convert 'DATE' column to datetime\n",
    "        df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], errors=\"coerce\")\n",
    "\n",
    "        # Select and clean relevant columns\n",
    "        hourly_data: pd.DataFrame = df[[\"DATE\", \"HourlyPrecipitation\", \"HourlyWindSpeed\"]].copy()\n",
    "        hourly_data.replace(\"T\", 0, inplace=True)\n",
    "        hourly_data.replace(regex=r\"[^\\d.]\", value=np.nan, inplace=True)\n",
    "\n",
    "        # Rename columns for consistency\n",
    "        hourly_data.columns = [\"date\", \"precipitation\", \"wind_speed\"]\n",
    "\n",
    "        # Convert columns to numeric\n",
    "        hourly_data[\"precipitation\"] = pd.to_numeric(hourly_data[\"precipitation\"], errors=\"coerce\")\n",
    "        hourly_data[\"wind_speed\"] = pd.to_numeric(hourly_data[\"wind_speed\"], errors=\"coerce\")\n",
    "\n",
    "        return hourly_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {csv_file}: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean and process daily weather data from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file containing weather data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing cleaned and processed daily weather data.\n",
    "                      Columns include 'date', 'precipitation', 'average_wind_speed', 'snowfall',\n",
    "                      'sunrise', and 'sunset'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load data\n",
    "        df: pd.DataFrame = pd.read_csv(csv_file, low_memory=False)\n",
    "        df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "\n",
    "        # Convert 'DATE' column to datetime\n",
    "        df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], errors=\"coerce\")\n",
    "\n",
    "        # Select and clean relevant columns\n",
    "        daily_data: pd.DataFrame = df[\n",
    "            [\"DATE\", \"DailyPrecipitation\", \"DailyAverageWindSpeed\", \"DailySnowfall\", \"Sunrise\", \"Sunset\"]\n",
    "        ].copy()\n",
    "\n",
    "        # Replace placeholder and non-numeric values\n",
    "        daily_data.replace(\"T\", 0, inplace=True)\n",
    "        daily_data.replace(regex=r\"[^\\d.]\", value=np.nan, inplace=True)\n",
    "\n",
    "        # Rename columns for consistency\n",
    "        daily_data.columns = [\"date\", \"precipitation\", \"average_wind_speed\", \"snowfall\", \"sunrise\", \"sunset\"]\n",
    "\n",
    "        # Convert columns to numeric\n",
    "        daily_data[\"precipitation\"] = pd.to_numeric(daily_data[\"precipitation\"], errors=\"coerce\")\n",
    "        daily_data[\"average_wind_speed\"] = pd.to_numeric(daily_data[\"average_wind_speed\"], errors=\"coerce\")\n",
    "        daily_data[\"snowfall\"] = pd.to_numeric(daily_data[\"snowfall\"], errors=\"coerce\")\n",
    "\n",
    "        # Drop rows where the columns values are NaN\n",
    "        daily_data = daily_data.dropna(subset=[\"precipitation\", \"average_wind_speed\", \"snowfall\", \"sunrise\", \"sunset\"], how=\"all\")\n",
    "\n",
    "        # Format 'sunrise' and 'sunset' columns\n",
    "        daily_data['sunrise'] = daily_data['sunrise'].astype(float).astype(int).astype(str).str.zfill(4)\n",
    "        daily_data['sunset'] = daily_data['sunset'].astype(float).astype(int).astype(str).str.zfill(4)\n",
    "\n",
    "        daily_data['sunrise'] = pd.to_datetime(daily_data['sunrise'], format='%H%M', errors='coerce').dt.time\n",
    "        daily_data['sunset'] = pd.to_datetime(daily_data['sunset'], format='%H%M', errors='coerce').dt.time\n",
    "\n",
    "        # Convert 'date' column to date\n",
    "        daily_data[\"date\"] = daily_data[\"date\"].dt.date\n",
    "\n",
    "        return daily_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {csv_file}: {e}\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data(directory: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load and clean weather data from all CSV files in the specified directory.\n",
    "\n",
    "    This function processes each file to extract and clean both hourly and daily weather data.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing CSV files.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Two DataFrames:\n",
    "            - hourly_data (pd.DataFrame): Combined cleaned hourly weather data.\n",
    "            - daily_data (pd.DataFrame): Combined cleaned daily weather data.\n",
    "    \"\"\"\n",
    "    weather_csv_files = get_all_weather_csvs(directory)\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "\n",
    "        # Append cleaned data to the respective lists\n",
    "        if not hourly_dataframe.empty:\n",
    "            hourly_dataframes.append(hourly_dataframe)\n",
    "        if not daily_dataframe.empty:\n",
    "            daily_dataframes.append(daily_dataframe)\n",
    "\n",
    "    hourly_data = pd.concat(hourly_dataframes, ignore_index=True) if hourly_dataframes else pd.DataFrame()\n",
    "    daily_data = pd.concat(daily_dataframes, ignore_index=True) if daily_dataframes else pd.DataFrame()\n",
    "    \n",
    "    # Replace NaN values with 0\n",
    "    hourly_data.fillna(0, inplace=True)\n",
    "    daily_data.fillna(0, inplace=True)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dcf3c2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    WEATHER_CSV_DIR = \"weather/\"  # Path to your weather data directory\n",
    "    hourly_weather_data, daily_weather_data = load_and_clean_weather_data(WEATHER_CSV_DIR)\n",
    "\n",
    "    hourly_weather_data.to_csv(\"cleaned_hourly_weather_data.csv\", index=False)\n",
    "    daily_weather_data.to_csv(\"cleaned_daily_weather_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "48216557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 00:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01 01:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01 02:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01 03:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01 04:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date  precipitation  wind_speed\n",
       "0 2020-01-01 00:51:00            0.0         8.0\n",
       "1 2020-01-01 01:51:00            0.0         8.0\n",
       "2 2020-01-01 02:51:00            0.0        14.0\n",
       "3 2020-01-01 03:51:00            0.0        11.0\n",
       "4 2020-01-01 04:51:00            0.0         6.0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "935261b7-ae23-427c-97ff-ea31aa4e44c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 56098 entries, 0 to 56097\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   date           56098 non-null  datetime64[ns]\n",
      " 1   precipitation  56098 non-null  float64       \n",
      " 2   wind_speed     56098 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(2)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "hourly_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a7dcb502-d1d1-447d-aa68-11bff0dc53b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>56098</td>\n",
       "      <td>56098.000000</td>\n",
       "      <td>56098.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-05-29 21:14:19.618881024</td>\n",
       "      <td>0.010288</td>\n",
       "      <td>4.537238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-01-01 00:51:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2021-03-18 19:01:45</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-05-28 01:21:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-08-15 05:39:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-10-22 18:51:00</td>\n",
       "      <td>3.470000</td>\n",
       "      <td>2237.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.056033</td>\n",
       "      <td>13.883208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                date  precipitation    wind_speed\n",
       "count                          56098   56098.000000  56098.000000\n",
       "mean   2022-05-29 21:14:19.618881024       0.010288      4.537238\n",
       "min              2020-01-01 00:51:00       0.000000      0.000000\n",
       "25%              2021-03-18 19:01:45       0.000000      0.000000\n",
       "50%              2022-05-28 01:21:00       0.000000      5.000000\n",
       "75%              2023-08-15 05:39:00       0.000000      7.000000\n",
       "max              2024-10-22 18:51:00       3.470000   2237.000000\n",
       "std                              NaN       0.056033     13.883208"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_weather_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>average_wind_speed</th>\n",
       "      <th>snowfall</th>\n",
       "      <th>sunrise</th>\n",
       "      <th>sunset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>07:20:00</td>\n",
       "      <td>16:39:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>07:20:00</td>\n",
       "      <td>16:40:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>07:20:00</td>\n",
       "      <td>16:41:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>0.27</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>07:20:00</td>\n",
       "      <td>16:42:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>07:20:00</td>\n",
       "      <td>16:43:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  precipitation  average_wind_speed  snowfall   sunrise    sunset\n",
       "0  2020-01-01           0.00                 8.6       0.0  07:20:00  16:39:00\n",
       "1  2020-01-02           0.00                 5.4       0.0  07:20:00  16:40:00\n",
       "2  2020-01-03           0.15                 3.4       0.0  07:20:00  16:41:00\n",
       "3  2020-01-04           0.27                 4.4       0.0  07:20:00  16:42:00\n",
       "4  2020-01-05           0.00                11.3       0.0  07:20:00  16:43:00"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f090eb94-a5b0-4d93-bf82-a596d2521b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1755 entries, 0 to 1754\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   date                1755 non-null   object \n",
      " 1   precipitation       1755 non-null   float64\n",
      " 2   average_wind_speed  1755 non-null   float64\n",
      " 3   snowfall            1755 non-null   float64\n",
      " 4   sunrise             1755 non-null   object \n",
      " 5   sunset              1755 non-null   object \n",
      "dtypes: float64(3), object(3)\n",
      "memory usage: 82.4+ KB\n"
     ]
    }
   ],
   "source": [
    "daily_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8c074aa3-a5f2-4586-8748-411e1e6c11da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precipitation</th>\n",
       "      <th>average_wind_speed</th>\n",
       "      <th>snowfall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1755.000000</td>\n",
       "      <td>1755.000000</td>\n",
       "      <td>1755.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.141966</td>\n",
       "      <td>4.835499</td>\n",
       "      <td>0.039088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.414574</td>\n",
       "      <td>2.467952</td>\n",
       "      <td>0.493457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.060000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.130000</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>14.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       precipitation  average_wind_speed     snowfall\n",
       "count    1755.000000         1755.000000  1755.000000\n",
       "mean        0.141966            4.835499     0.039088\n",
       "std         0.414574            2.467952     0.493457\n",
       "min         0.000000            0.000000     0.000000\n",
       "25%         0.000000            3.000000     0.000000\n",
       "50%         0.000000            4.500000     0.000000\n",
       "75%         0.060000            6.200000     0.000000\n",
       "max         7.130000           14.200000    14.800000"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "# Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a8a5c2-ee59-4ada-8e69-87ad6de1bd26",
   "metadata": {},
   "source": [
    "In this part, we store the cleaned datasets into a SQLite database by creating appropriate schemas for each dataset and loading the data into the database. This ensures our data is structured and easily queryable for subsequent analyses.\n",
    "\n",
    "#### 1. Load Cleaned Datasets\n",
    "- **Datasets loaded**:\n",
    "  - `daily_data`: Daily weather data.\n",
    "  - `hourly_data`: Hourly weather data.\n",
    "  - `taxi_data`: Cleaned Yellow Taxi trip data.\n",
    "  - `uber_data`: Cleaned Uber HVHF trip data.\n",
    "- These datasets are read from their respective cleaned CSV files.\n",
    "\n",
    "#### 2. Define Database Schemas\n",
    "- **Schema definitions**:\n",
    "  - `hourly_weather`: Contains hourly weather data including datetime, temperature, precipitation, and wind speed.\n",
    "  - `daily_weather`: Contains daily weather data including date, average precipitation, average wind speed, total snowfall, sunset and sunrise time.\n",
    "  - `taxi_trips`: Contains Yellow Taxi trip data including pickup/dropoff datetimes, coordinates, trip miles, and total amount.\n",
    "  - `uber_trips`: Contains Uber trip data with similar columns as `taxi_trips`.\n",
    "\n",
    "#### 3. Save Schema to `schema.sql`\n",
    "- All schema definitions are written to a file named `schema.sql`.\n",
    "- This file serves as a record of the database structure and can be reused to recreate the tables in another database.\n",
    "\n",
    "#### 4. Create Tables\n",
    "- The `schema.sql` file is executed to create the database tables.\n",
    "- **Execution steps**:\n",
    "  1. Open a connection to the SQLite database.\n",
    "  2. Read the `schema.sql` file.\n",
    "  3. Execute each schema creation statement.\n",
    "\n",
    "By the end of this part, the SQLite database is populated with structured tables ready for queries and further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2df280ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned datasets\n",
    "daily_data = pd.read_csv(\"cleaned_daily_weather_data.csv\")\n",
    "hourly_data = pd.read_csv(\"cleaned_hourly_weather_data.csv\")\n",
    "taxi_data = pd.read_csv(\"taxi_data_cleaned.csv\")\n",
    "uber_data = pd.read_csv(\"uber_data_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    date DATETIME NOT NULL,\n",
    "    precipitation FLOAT,\n",
    "    wind_speed FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    date DATE NOT NULL,\n",
    "    precipitation FLOAT,\n",
    "    average_wind_speed FLOAT,\n",
    "    snowfall FLOAT,\n",
    "    sunrise TIME,\n",
    "    sunset TIME\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    pickup_datetime DATETIME NOT NULL,\n",
    "    dropoff_datetime DATETIME NOT NULL,\n",
    "    pu_lat FLOAT,\n",
    "    pu_lon FLOAT,\n",
    "    do_lat FLOAT,\n",
    "    do_lon FLOAT,\n",
    "    trip_miles FLOAT,\n",
    "    total_amount FLOAT,\n",
    "    fare_amount FLOAT,\n",
    "    extra FLOAT,\n",
    "    mta_tax FLOAT,\n",
    "    tip_amount FLOAT,\n",
    "    tolls_amount FLOAT,\n",
    "    improvement_surcharge FLOAT,\n",
    "    congestion_surcharge FLOAT,\n",
    "    airport_fee FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    pickup_datetime DATETIME NOT NULL,\n",
    "    dropoff_datetime DATETIME NOT NULL,\n",
    "    pu_lat FLOAT,\n",
    "    pu_lon FLOAT,\n",
    "    do_lat FLOAT,\n",
    "    do_lon FLOAT,\n",
    "    trip_miles FLOAT,\n",
    "    fare_amount FLOAT,\n",
    "    tolls_amount FLOAT,\n",
    "    bcf FLOAT,\n",
    "    sales_tax FLOAT,\n",
    "    tip_amount FLOAT,\n",
    "    airport_fee FLOAT,\n",
    "    congestion_surcharge FLOAT\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    with open(DATABASE_SCHEMA_FILE, \"r\") as schema_file:\n",
    "        schema_script = schema_file.read()\n",
    "        statements = schema_script.split(\";\")\n",
    "        for statement in statements:\n",
    "            statement = statement.strip()\n",
    "            if statement:\n",
    "                connection.execute(db.text(statement))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    \"\"\"\n",
    "    Writes dataframes to the corresponding SQL tables without replacing the tables.\n",
    "\n",
    "    Args:\n",
    "        table_to_df_dict (dict): A dictionary where keys are table names\n",
    "                                 and values are the respective DataFrames.\n",
    "    \"\"\"\n",
    "    for table_name, df in table_to_df_dict.items():\n",
    "        # Use 'append' mode to insert data without replacing the table\n",
    "        df.to_sql(table_name, con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query either via sqlalchemy\n",
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_1)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "pd.read_sql(QUERY_1, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
